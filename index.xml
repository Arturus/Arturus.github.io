<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Артур Суилин on Артур Суилин</title>
    <link>https://suilin.ru/</link>
    <description>Recent content in Артур Суилин on Артур Суилин</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ru</language>
    <copyright>&amp;copy; 2018 &amp;middot; Артур Суилин</copyright>
    <lastBuildDate>Tue, 30 Apr 2019 00:00:00 +0300</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ускоряем драйвер ClickHouse</title>
      <link>https://suilin.ru/post/clickhouse_driver/</link>
      <pubDate>Tue, 30 Apr 2019 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/clickhouse_driver/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://clickhouse.yandex&#34; target=&#34;_blank&#34;&gt;ClickHouse&lt;/a&gt; &amp;ndash; самая быстрая в мире аналитическая СУБД. Для тех, кто с ним ещё не знаком,
очень рекомендую попробовать, пересаживаться обратно на MySQL или Postgress потом не захочется.&lt;/p&gt;

&lt;p&gt;Обычно данные хранятся в ClickHouse в сыром, неагрегированном виде, и агрегируются
на лету при выполнении SQL запросов. Но при решении data science задач часто возникает необходимость
выгрузки именно сырых данных, для дальнейшей их обработки в памяти (например, для
обучения модели по этим данным). Если выгружать данные в текстовый файл
с помощью родного клиента ClickHouse, всё происходит достаточно шустро &amp;ndash;
&amp;ldquo;ClickHouse не тормозит&amp;rdquo;™.
Но если пользоваться &lt;a href=&#34;https://github.com/mymarilyn/clickhouse-driver&#34; target=&#34;_blank&#34;&gt;драйвером для Python&lt;/a&gt;, то процесс выгрузки затягивается
надолго. Почему?&lt;/p&gt;

&lt;p&gt;Дело не в том, что драйвер плохо написан &amp;ndash; драйвер как раз отличный. Проблема
лежит глубже. ClickHouse отдаёт данные в виде бинарного потока, каждый
элемент которого соответствует машинному представлению числа на x86 процессоре.
Если работать с этими данными на низкоуровневом языке, таком как С++ (как
в родном клиенте), проблем с быстродействием не будет. Если колонка, например,
имеет тип &lt;code&gt;Int32&lt;/code&gt;, то на клиента приедет фактически готовый к использованию
массив чисел с типом &lt;code&gt;int32_t&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Но Python представляет все числа, как объекты. Это означает, что драйвер
проходит по загруженным данным, преобразует каждое число в объект,
и потом уже из этих объектов собирает питоновский массив (состоящий из указателей). Такая операция
называется &lt;em&gt;boxing&lt;/em&gt;, и при больших объемах данных она отнимает значительное время.
Собственно, в ходе загрузки данных через Python-драйвер основное занятие CPU это переупаковка
чисел из машинного представления в объекты.&lt;/p&gt;

&lt;p&gt;В то же время в data science
принято работать c &lt;a href=&#34;https://www.numpy.org/&#34; target=&#34;_blank&#34;&gt;numpy&lt;/a&gt; массивами (pandas тоже работает через numpy), которые содержат числа
в машинном представлении, как в С. То есть, сначала мы долго упаковывали числа в объекты, а потом,
при конвертировании из Python массива в numpy массив будем распаковывать объекты обратно в числа (&lt;em&gt;unboxing&lt;/em&gt;).
Очевидно, что промежуточное объектное представление здесь только мешает,
и если бы драйвер умел выгружать данные сразу в numpy массивы, процесс
пошёл бы намного бодрее. Но драйвер этого не умеет, поэтому я его немного доработал,
чтобы такая возможность появилась.&lt;/p&gt;

&lt;h2 id=&#34;инсталляция&#34;&gt;Инсталляция&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Если уже установлен пакет &lt;em&gt;clickhouse-driver&lt;/em&gt;, удалить его: &lt;code&gt;pip uninstall clickhouse-driver&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Инсталлировать из github версию с ускоренным чтением:
&lt;code&gt;pip install git+https://github.com/Arturus/clickhouse-driver.git&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;использование&#34;&gt;Использование&lt;/h2&gt;

&lt;p&gt;При создании объекта &lt;code&gt;Client&lt;/code&gt; надо включить новую опцию &lt;code&gt;numpy_columns=True&lt;/code&gt;,
а при выполнении запросов включать опцию &lt;code&gt;columnar=True&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;client = Client(&#39;localhost&#39;, database=&#39;db&#39;, settings=dict(numpy_columns=True))
data = client.execute(query, columnar=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;В &lt;em&gt;data&lt;/em&gt; будет содержаться набор колонок. Колонки, представляющие собой
числа или timestamp, будут numpy-массивами, остальные колонки (например, строки) будут стандартными
Python массивами. В numpy формат конвертируются следующие типы Clickhouse:
Int8/16/32/64, UInt8/16/32/64, DateTime.&lt;/p&gt;

&lt;p&gt;Полученные данные часто преобразуются в pandas DataFrame с именами колонок,
соответствующими именам колонок в БД. Чтобы не делать это каждый раз вручную,
в класс Client добавлен метод &lt;code&gt;query_dataframe()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = client.query_dataframe(&#39;SELECT a,b FROM table&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Результатом будет DataFrame с двумя колонками, &lt;strong&gt;a&lt;/strong&gt; и &lt;strong&gt;b&lt;/strong&gt;.
&lt;style&gt;th {text-align: center;}&lt;/style&gt;&lt;/p&gt;

&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;

&lt;p&gt;Замерялась скорость выполнения запроса &lt;code&gt;SELECT x1,x2,...,xn FROM table&lt;/code&gt; на таблице со
100 млн. записей (реальные данные из Logs API Яндекс.Метрики), engine=MergeTree. Запросы выполнялись на локальном
ClickHouse c дефолтными настройками драйвера.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Запрос&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Время, numpy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Время, standard&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Ускорение&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Memory, numpy&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Memory, standard&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4 колонки Int8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.34 s&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5.8 s&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;×&lt;strong&gt;17&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.82 Gb&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3.3 Gb&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2 колонки Int64&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.38 s&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12 s&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;×&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2.61 Gb&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;9.7 Gb&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1 колонка DateTime&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12.1 s&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;7.1 m&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;×&lt;strong&gt;35&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1.16 Gb&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4.8 Gb&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Использование numpy ускоряет чтение на порядок. Особенно заметно ускорение на
типе DateTime, потому что работа c временем на уровне Питоновских datetime-объектов
происходит очень медленно. Фактически, без использования numpy время выполнения
запроса, включающего колонку со временем, выходит за рамки разумного.&lt;/p&gt;

&lt;p&gt;В последних двух колонках &amp;ndash; объём памяти, занимаемый процессом после
выполнения запроса. Видно, что использование numpy не только ускоряет
загрузку данных, но и уменьшает объём требуемой памяти примерно в 4 раза.&lt;/p&gt;

&lt;h2 id=&#34;ограничения&#34;&gt;Ограничения&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Поддерживается только чтение в numpy массивы. Запись возможна только в режиме &lt;code&gt;numpy_columns=False&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;numpy массивы не используются при чтении nullable колонок и колонок-массивов. Впрочем, код чтения массивов тоже немного оптимизирован и теперь работает быстрее, чем в обычном драйвере.&lt;/li&gt;
&lt;li&gt;Также numpy не используется при чтении enums, decimal и
прочих продвинутых типов (поддержка может быть добавлена в будущем).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ограничения на чтение никак не мешают функционированию драйвера, просто
 для некоторых типов данных чтение ускоряется, а для некоторых &amp;ndash; работает, как обычно.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science VI. Клиент скорее жив, чем мёртв?</title>
      <link>https://suilin.ru/post/user_churn/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/user_churn/</guid>
      <description>

&lt;p&gt;Некоторые сайты, например новостные, не имеют явных целевых действий, таких как заказ или подписка.
Для них важно не выполнение посетителем каких либо действий, а
само присутствие посетителя на сайте, желательно регулярное.
Но и для e-commerce сайтов тоже очень важны &lt;em&gt;возвраты&lt;/em&gt; посетителей, так как
вернувшийся посетитель обходится намного дешевле, чем новый. Каждый
возврат это шанс что нибудь продать, при этом чем меньше интервал между возвратами,
тем больше шансов на осуществление продажи.&lt;/p&gt;

&lt;p&gt;Задача определения того, продолжит ли посетитель/клиент пользоваться сервисом,
или ушёл и никогда больше не вернётся, называется &lt;em&gt;churn prediction&lt;/em&gt; (прогнозирование оттока).
Лобовой подход к её решению &amp;ndash; взять определенный промежуток времени, например месяц
или полгода, и спрогнозировать вероятность возврата клиента в этом промежутке.
Обычно data scientist-ы так и поступают. Но такой подход не очень хорош с точки зрения
качества прогноза.&lt;/p&gt;

&lt;p&gt;Во-первых, хорошо бы использовать прогнозное значение,
как метрику вовлечённости клиента (т.е. насколько он &amp;ldquo;churned&amp;rdquo; или &amp;ldquo;not churned&amp;rdquo;).
Клиент, который вернётся на сайт завтра, очевидно,
более вовлечён, чем клиент, который вернётся через полгода. Но для модели, определяющей
просто вероятность возврата в течение полугода, оба клиента будут совершенно равнозначны.&lt;/p&gt;

&lt;p&gt;Во-вторых, возникает вопрос выбора размера промежутка для прогноза. Каким он должен быть? Месяц?
Квартал? Год? Интуитивно кажется, что чем больший период жизни клиента мы охватываем, тем лучше.
Но если взять промежуток в год,
то обучаться можно будет только на устаревших данных (старше года), так как данные за последний
год будут использоваться только для формирования целевых значений. За год
данные могут и &amp;ldquo;протухнуть&amp;rdquo;: изменится сам сайт, изменятся источники трафика и свойства аудитории.
Чем свежее обучающие данные, тем качественнее будет прогноз.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;exit.png&#34; width=&#34;422&#34; height=&#34;211&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Более правильным кажется предсказывать не
вероятность возврата внутри фиксированного промежутка, а интервал времени до следующего посещения сайта,
лежащий в диапазоне $[t_s, \infty]$, где $t_s$ это таймаут, которым разделяются сессии
(8 часов). Чем
меньше этот интервал, тем более вовлечён клиент. Если предсказанный интервал близок к бесконечности,
то клиент возможно не вернётся на сайт никогда, и можно считать, что он &lt;em&gt;churned&lt;/em&gt;.
Порог, после которого клиент становится &lt;em&gt;churned&lt;/em&gt;, можно выбирать и изменять
уже после обучения модели, это ещё один плюс.&lt;/p&gt;

&lt;p&gt;Но тогда возникает другая проблема: чему равен интервал для клиентов, которые ушли с сайта и пока не вернулись?
Ведь верхняя его граница находится где то в будущем (возможно, бесконечно далёком).
Точно известно только то, что этот интервал больше, чем &lt;code&gt;now_time - last_event_time&lt;/code&gt;.
Как обучать модель, если целевая переменная определена только нижней границей?&lt;/p&gt;

&lt;p&gt;Такое обучение возможно при использовании регрессионной модели,
которая называется &lt;em&gt;Cox Proportional Hazards&lt;/em&gt;. Чтобы понять, про какие
&amp;ldquo;пропорциональные опасности&amp;rdquo; идет речь,
надо немного познакомиться с &lt;em&gt;&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%B2%D1%8B%D0%B6%D0%B8%D0%B2%D0%B0%D0%B5%D0%BC%D0%BE%D1%81%D1%82%D0%B8&#34; target=&#34;_blank&#34;&gt;анализом выживаемости&lt;/a&gt;&lt;/em&gt;
(&lt;a href=&#34;https://en.wikipedia.org/wiki/Survival_analysis&#34; target=&#34;_blank&#34;&gt;Survival analysis&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&#34;survival-analysis&#34;&gt;Survival analysis&lt;/h2&gt;

&lt;p&gt;Задача прогноза времени до наступления события (в нашем случае - возврата на сайт)
встречается в жизни довольно часто, но теория анализа выживаемости и соответствующие методики прогнозирования
сначала были разработаны для медицинских исследований. Событием, наступление которого анализировалось,
являлась смерть пациента, отсюда и название &amp;ldquo;анализ выживаемости&amp;rdquo;. Такой анализ часто применяется
при сравнении групп пациентов, получивших разные способы лечения: более эффективен тот способ,
где пациенты прожили дольше. В нашем случае логика инвертируется: хороший клиент это
тот, кто вернулся на сайт (&amp;ldquo;умер&amp;rdquo; с точки зрения survival analysis) как можно раньше.&lt;/p&gt;

&lt;p&gt;Естественно, врачи не хотели дожидаться естественной смерти всех пациентов, чтобы сделать выводы
об эффективности лечения. Поэтому survival analysis умеет работать с &lt;em&gt;цензурированными&lt;/em&gt;
(&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Censoring_(statistics)&#34; target=&#34;_blank&#34;&gt;censored&lt;/a&gt;&lt;/em&gt;) данными.
Цензурирование возникает, когда точное время наступления события неизвестно
(находится где то в будущем, пациент все еще жив), известно только, что событие не наступило
до определенного момента времени (обычно до текущего момента).&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;censoring1.png&#34; alt=&#34;Времена до возврата на сайт. Бордовым показаны посетители, у которых время возврата уже известно, голубым - которые еще не вернулись (вернутся в будущем), у них цензурированное время наступления события.&#34; width=&#34;710&#34; height=&#34;344&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Времена до возврата на сайт. Бордовым показаны посетители, у которых время возврата уже известно, голубым - которые еще не вернулись (вернутся в будущем), у них цензурированное время наступления события.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Базовое понятие анализа выживаемости это &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Survival_function&#34; target=&#34;_blank&#34;&gt;Survival function&lt;/a&gt;&lt;/em&gt; (функция выживания):
$$ S(t) = \Pr(T&amp;gt;t) $$
где $T$ &amp;ndash; время жизни члена популяции, $t$ &amp;ndash; произвольный интервал времени.
Смысл функции &amp;ndash; вероятность того, что член популяции проживёт дольше, чем $t$.
Функция является строго убывающей, в момент времени $t=0$ вероятность
равна 100% (все только что родились), по мере возрастания $t$ вероятность
убывает (постепенно умирают). Для пациентов вероятность станет практически нулевой
после 100 лет, но в общем случае возможна &amp;ldquo;вечная жизнь&amp;rdquo;,
когда часть популяции выживает в течение всего обозримого времени
(в нашем случае &amp;ndash; клиент никогда не возвращается на сайт).&lt;/p&gt;

&lt;p&gt;Давайте визуализируем функцию выживания разных когорт посетителей
на сайте shop.ml. Когорты сформируем из тех, у кого была только одна (первая) сессия,
две и три сессии, и посмотрим на динамику возвратов:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;455&#34; height=&#34;279&#34; class=&#34;lazy&#34; src=&#34;shop_survival_lr.png&#34;
      data-src=&#34;shop_survival.png&#34; data-srcset=&#34;shop_survival@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;График функции выживаемости это по сути перевёрнутый график user retention. На старте
есть 100% юзеров, постепенно часть из них возвращается, а часть теряется навсегда
(графики заканчиваются выше нулевой точки). Посетители с единственной сессией
возвращаются на сайт очень неохотно: за две недели возвращается только 10%,
а в течение года количество вернувшихся не доходит даже до 20%. Посетители,
у которых было уже две или три сессии намного более вовлечены: в течение недели
возвращается 20% посетителей с двумя сессиями, а в течение двух месяцев &amp;ndash;
половина посетителей, у которых было три сессии.&lt;/p&gt;

&lt;h2 id=&#34;hazard-function&#34;&gt;Hazard function&lt;/h2&gt;

&lt;p&gt;Survival function показывает, какая доля популяции выживает (какая доля посетителей не возвращается)
к моменту времени $t$. Комплементарная ей функция, &lt;em&gt;lifetime distribution
function&lt;/em&gt;, показывает, какая доля &amp;ldquo;умирает&amp;rdquo; к моменту $t$:
$$F(t) = 1 - S(t)$$
Если взять производную от lifetime distribution function, получим плотность событий
 (количество смертей пациентов или возвратов посетителей) в единицу времени:
$$f(t) = F&amp;rsquo;(t) = \frac{d}{dt}F(t)$$
Плотность событий не очень удобна для использования,
т.к. по мере вымирания популяции общее количество событий быстро уменьшается.
Поэтому обычно используют другую функцию, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Survival_analysis#Hazard_function_and_cumulative_hazard_function&#34; target=&#34;_blank&#34;&gt;hazard function&lt;/a&gt;&lt;/em&gt;
(функция риска),
в которой количество смертей в единицу времени $f(t)$ нормируется на долю
популяции, выжившей к этому моменту времени:
$$\lambda(t) = \frac{f(t)}{S(t)} = -\frac{S&amp;rsquo;(t)}{S(t)}$$
Эта функция фактически показывает риск умереть в момент времени $t$ для
тех, кто дожил до этого момента, отсюда название &lt;em&gt;hazard function&lt;/em&gt;.
Визуализируем функцию риска для наших когорт посетителей:&lt;/p&gt;

&lt;p&gt;&lt;figure &gt;
&lt;img width=&#34;467&#34; height=&#34;279&#34; class=&#34;lazy&#34; src=&#34;shop_hazard_lr.png&#34;
      data-src=&#34;shop_hazard.png&#34; data-srcset=&#34;shop_hazard@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
Лёгкая волнистость, которая наблюдалась на графике функции выживания, превратилась
в острые пики. Причина этих пиков &amp;ndash; суточная сезонность. Посетители чаще всего
возвращаются на сайт в то же время дня, в которое они посещали его в предыдущий раз.
Заметно, что интенсивность событий остается примерно постоянной до 30 дней, а
затем идёт на спад, то есть через месяц посетители начинают забывать о сайте.
Также обратим внимание: форма всех трёх графиков очень похожа,
они отличаются в основном масштабом по оси $y$.&lt;/p&gt;

&lt;p&gt;Используется также &lt;em&gt;cumulative hazard function&lt;/em&gt; (кумулятивная функция риска),
она выражает суммарный риск, накопившийся к моменту времени $t$:
$$\Lambda(t)=\int_0^t\lambda(u) du$$
Cumulative hazard function и survival function связаны следующим выражением:
$$S(t)=\exp(-\Lambda(t))$$&lt;/p&gt;

&lt;h2 id=&#34;cox-proportional-hazards&#34;&gt;Cox proportional hazards&lt;/h2&gt;

&lt;p&gt;Одна из часто используемых в survival analysis моделей это
&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Proportional_hazards_model&#34; target=&#34;_blank&#34;&gt;Proportional hazards&lt;/a&gt;&lt;/em&gt;. В этой модели делается допущение, что
функции риска для каждого индивидуального члена популяции имеют примерно одинаковую
форму, задаваемую через общую для всех &lt;em&gt;baseline hazard&lt;/em&gt; функцию $\lambda_0(t)$, и отличаются
только на положительный коэффициент пропорциональности $k_i$, индивидуальный для каждого члена популяции:
$$\lambda_i(t) = \lambda_0(t) k_i,\;i \in 1 \mathrel{{.}\,{.}} N $$
В классической модели, предложенной статистиком Дэвидом Коксом,
коэффициент вычисляется с помощью линейной регрессии:
$$k_i = \exp(logits_i)$$
$$logits_i = \mathbf{x_i}^\top\boldsymbol{\beta}$$
где $\mathbf{x_i}$ &amp;ndash; вектор признаков i-го члена популяции, $\boldsymbol{\beta}$ &amp;ndash; вектор коэффициентов
линейной модели. Но ничего не мешает использовать для вычисления $logits$ любую
другую модель машинного обучения, например нейросеть или деревья решений.&lt;/p&gt;

&lt;p&gt;Изящество этой модели в том, её можно обучать через максимизацию &lt;em&gt;частичного правдоподобия&lt;/em&gt;,
не задавая в явном виде baseline функцию $\lambda_0(t)$, форма которой,
вообще говоря, неизвестна до окончания обучения.&lt;/p&gt;

&lt;p&gt;Допущение, что функции риска всех членов популяции отличаются только на пропорциональный коэффициент,
конечно, не всегда соответствует реальности. В порядке эксперимента, попробуем
привести функции риска для наших когорт посетителей к одному масштабу, умножая каждый
график на эмпирически подобранный коэффициент. В идеале графики должны совпасть:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;460&#34; height=&#34;279&#34; class=&#34;lazy&#34; src=&#34;shop_hazard_prop_lr.png&#34;
      data-src=&#34;shop_hazard_prop.png&#34; data-srcset=&#34;shop_hazard_prop@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Графики в значительной степени совпадают, но не на 100%. Сильнее всего
различается поведение после 30 дней.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;В более продвинутых моделях
каждый член популяции может иметь индивидуальную функцию риска произвольной формы.
Но с другой стороны, чтобы
смоделировать индивидуальные функции, требуется очень много обучающих данных,
а proportional hazards модели хорошо работают даже на небольших обучающих выборках.&lt;/p&gt;

&lt;h2 id=&#34;моделирование&#34;&gt;Моделирование&lt;/h2&gt;

&lt;p&gt;Пациент может умереть только единожды, поэтому survival analysis работает исключительно с
с &lt;em&gt;терминальными&lt;/em&gt; событиями, т.е. событие должно быть последней точкой в жизни члена популяции.
В отличие от пациента, посетитель сайта может возвращаться неограниченное количество раз, поэтому
мы немного схитрим, и представим, что каждая сессия происходит от
лица виртуального нового посетителя. В этом случае модель может переобучиться на посетителях,
которые возвращались много раз, т.к. их признаки будут учтены многократно.
Чтобы этого не случилось, дадим каждому виртуальному посетителю вес $w_i = s_i^{-1}$, обратно пропорциональный
количеству его сессий $s_i$.&lt;/p&gt;

&lt;p&gt;В качестве метрики качества будем использовать уже знакомый по предыдущим
статьям &lt;em&gt;Concordance Index&lt;/em&gt; &amp;ndash; обобщение метрики AUC для регрессионных
задач. Важно, что Concordance Index корректно работает с цензурированными данными,
ведь основной процент посетителей никогда не возвращается после первой сессии.&lt;/p&gt;

&lt;p&gt;Модель Proportional Hazards выдаёт значение коэффициента $k_i$. С помощью
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/17768681&#34; target=&#34;_blank&#34;&gt;Breslow estimator&lt;/a&gt; можно найти базовую кумулятивную функцию $\Lambda_0(t)$,
и перейти от неё к функции выживания для i-го посетителя:
$$S_i(t) = \exp\left(-\int_0^t k_i d\Lambda_0(u)\right)$$
Таким образом, результатом является не точечная оценка
вероятного интервала между сессиями (как было бы в традиционных моделях), а целая вероятностная
функция, параметризованная коэффициентом $k_i$.&lt;/p&gt;

&lt;p&gt;Для скоринга посетителей по степени того, насколько они &lt;em&gt;churned&lt;/em&gt;,
абсолютные значения интервалов, и тем более вероятностные функции
сложной формы не нужны. Достаточно значения коэффициента $k_i$: чем оно больше, тем
более вовлечён (менее &lt;em&gt;churned&lt;/em&gt;) посетитель. На практике удобнее использовать даже
не сам коэффициент, а его логарифм (соответствующий $logits$ из модели),
т.к. у логарифмического значения близкое к нормальному распределение,
такие значения проще воспринимать.&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;722&#34; height=&#34;293&#34; class=&#34;lazy&#34; src=&#34;shop_score_distributions_lr.png&#34;
      data-src=&#34;shop_score_distributions.png&#34; data-srcset=&#34;shop_score_distributions@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Слева распределение прогнозных значений коэффициента $k_i$, справа &amp;ndash;
распределение значений его логарифма. Правое распределение ближе к нормальному,
левое &amp;ndash; к экспоненциальному. Обратите внимание на логарифмическую шкалу частот (ось Y).&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;Shapely values рассчитываются тоже в этой логарифмической шкале. Одной единице шкалы
соответствует двукратная разница в интенсивности событий &amp;ldquo;возврат на сайт&amp;rdquo;,
т.е. за одинаковый промежуток времени на сайт вернётся в два больше раза посетителей
из когорты, где $score=1$ по сравнению с когортой, где $score=0$.&lt;/p&gt;

&lt;p&gt;Также обратим внимание, что распределение скоринговых значений зависит от того,
считается оно по уникальным посетителям или по уникальным сессиями (после каждой сессии
скоринговое значение обновляется):
&lt;figure &gt;
&lt;img width=&#34;385&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;shop_user_session_diff_lr.png&#34;
      data-src=&#34;shop_user_session_diff.png&#34; data-srcset=&#34;shop_user_session_diff@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Если смотреть распределение по посетителям, кажется, что скоринговое значение $&amp;gt;2$ практически не встречается.
Но в распределении по сессиям таких значений много, потому что активные
посетители, у которых маленький промежуток между сессиями, генерируют в целом
намного больше сессий, чем неактивные.&lt;/p&gt;

&lt;h2 id=&#34;результаты-shop-ml&#34;&gt;Результаты, shop.ml&lt;/h2&gt;

&lt;p&gt;Для начала, визуализируем получившуюся baseline hazard функцию, относительно
которой рассчитываются скоринговые коэффициенты:
&lt;figure &gt;
&lt;img width=&#34;405&#34; height=&#34;279&#34; class=&#34;lazy&#34; src=&#34;shop_baseline_hazard_lr.png&#34;
      data-src=&#34;shop_baseline_hazard.png&#34; data-srcset=&#34;shop_baseline_hazard@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Сюрпризов здесь нет, baseline повторяет форму функции риска,
рассчитанную по исходным данным. Посмотрим, как будут отличаться функции
выживания для посетителей с разными скоринговыми коэффициентами:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;455&#34; height=&#34;279&#34; class=&#34;lazy&#34; src=&#34;shop_survival_by_score_lr.png&#34;
      data-src=&#34;shop_survival_by_score.png&#34; data-srcset=&#34;shop_survival_by_score@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Половина посетителей с коэффициентом +3 вернётся на сайт уже на следующий день,
а в течение года вернётся почти 100%.  В то же время среди посетителей с коэффициентом -2.4
(самое распространённое значение в нашей выборке) даже через год
на сайт возвращается только ~13%.&lt;/p&gt;

&lt;p&gt;Concordance Index этой модели на тестовой выборке ~77%. С точки зрения
классического machine learning это кажется довольно скромным результатом,
но для survival analysis это хорошая точность. Ведь мы предсказываем
по большому счёту случайную величину, зависящую от множества внешних факторов,
которые невозможно учесть в модели.&lt;/p&gt;

&lt;p&gt;Посмотрим, какие признаки играют определяющую роль в том, насколько
быстро посетитель вернётся на сайт:
&lt;figure &gt;
&lt;img width=&#34;587&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;shop_shap_summary_lr.png&#34;
      data-src=&#34;shop_shap_summary.png&#34; data-srcset=&#34;shop_shap_summary@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;На первом месте &amp;ndash; &amp;ldquo;возраст&amp;rdquo; &lt;strong&gt;user_age&lt;/strong&gt;, т.е. насколько давно посетитель впервые появился на сайте.
&lt;figure &gt;
&lt;img width=&#34;454&#34; height=&#34;316&#34; class=&#34;lazy&#34; src=&#34;shop_user_age_lr.png&#34;
      data-src=&#34;shop_user_age.png&#34; data-srcset=&#34;shop_user_age@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
Если присмотреться, становится понятным, что на самом деле роль здесь играет
не столько возраст посетителя, сколько длительность его первой сессии (если была только
одна сессия, то возраст совпадает с её длительностью). Точка &amp;ldquo;перелома&amp;rdquo;
находится в районе 10 минут: если сессия продлилась дольше, вероятность
возврата на сайт быстро возрастает.&lt;/p&gt;

&lt;p&gt;На втором месте &amp;ndash; &lt;strong&gt;session_count&lt;/strong&gt;, количество сессий:
&lt;figure &gt;
&lt;img width=&#34;488&#34; height=&#34;319&#34; class=&#34;lazy&#34; src=&#34;shop_session_count_lr.png&#34;
      data-src=&#34;shop_session_count.png&#34; data-srcset=&#34;shop_session_count@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
&amp;ldquo;Возвращаемость&amp;rdquo; посетителя начинает расти с третьей сессии. Разница между
теми, у которых было 1-2 сессии и теми у кого было много сессий, достигает 3 единицы,
это 8-кратное увеличение &amp;ldquo;возвращаемости&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;session_interval&lt;/strong&gt; &amp;ndash; средний промежуток времени между сессиями. Это
примерно то, что мы пытаемся прогнозировать, поэтому неудивительно, что
этот признак оказывает значительное влияние на результат:
&lt;figure &gt;
&lt;img width=&#34;488&#34; height=&#34;316&#34; class=&#34;lazy&#34; src=&#34;shop_shap_session_interval_lr.png&#34;
      data-src=&#34;shop_shap_session_interval.png&#34; data-srcset=&#34;shop_shap_session_interval@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
Нулевое значение соответствует единственной сессии; минимальный интервал
между сессиями 8 часов, поэтому на диаграмме такой разрыв между &lt;strong&gt;0&lt;/strong&gt; и &lt;strong&gt;8h&lt;/strong&gt;.
C увеличением исторического среднего интервала между сессиями растёт и прогнозируемый
интервал (уменьшается score), что вполне логично.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;pageview_count&lt;/strong&gt; &amp;ndash; количество просмотренных страниц, также один из
главных показателей вовлечённости посетителя.
&lt;figure &gt;
&lt;img width=&#34;458&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;shop_pageview_count_lr.png&#34;
      data-src=&#34;shop_pageview_count.png&#34; data-srcset=&#34;shop_pageview_count@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
Здесь любопытно взаимодействие с признаком &lt;strong&gt;session_count&lt;/strong&gt;: если было
много сессий, то положительная роль большого количества просмотренных страниц
ослабляется. Если же просмотрено в менее 10 страниц, то большое количество сессий
становится негативным фактором.&lt;/p&gt;

&lt;h2 id=&#34;результаты-другие-сайты&#34;&gt;Результаты, другие сайты&lt;/h2&gt;

&lt;p&gt;Результаты работы модели на других сайтах весьма похожи на результаты
shop.ml, и отличаются только нюансами, связанными с разной структурой сайтов
и разной структурой входящего трафика. Поэтому не буду приводить подробные
результаты для каждого сайта, в качестве примера &amp;ndash; summary для
courses.ml:
&lt;figure &gt;
&lt;img width=&#34;626&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;courses_shap_summary_lr.png&#34;
      data-src=&#34;courses_shap_summary.png&#34; data-srcset=&#34;courses_shap_summary@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Точность по Concordance Index для всех сайтов находится в районе 75%.
Форма baseline hazard function тоже довольно схожа у всех сайтов.&lt;/p&gt;

&lt;h2 id=&#34;индивидуальные-прогнозы&#34;&gt;Индивидуальные прогнозы&lt;/h2&gt;

&lt;p&gt;Основное применение этой модели &amp;ndash; прогнозирование вовлеченности
отдельных посетителей, чтобы работать с ними на индивидуальном уровне.
Поэтому здесь особенно важны scorecards, позволяющие &amp;ldquo;заглянуть в душу&amp;rdquo;
любого посетителя и получить представление о его планах на будущее.
&lt;figure &gt;
&lt;img width=&#34;550&#34; height=&#34;612&#34; class=&#34;lazy&#34; src=&#34;shop_scorecards_lr.png&#34;
      data-src=&#34;shop_scorecards.png&#34; data-srcset=&#34;shop_scorecards@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
На диаграмме представлены образцы scorecards трёх посетителей:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A&lt;/strong&gt; &amp;ndash; посетитель, который по мнению модели точно вернётся на сайт в ближайшем будущем.
Характерные особенности: много предыдущих сессий и просмотров страниц; относительно короткий (полтора дня)
интервал между сессиями; появился на сайте два месяца назад&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;С&lt;/strong&gt; &amp;ndash; посетитель, который вряд ли вернётся на сайт. Особенности:
единственная короткая сессия и единственный просмотр страницы, длившийся секунды.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt; &amp;ndash; посетитель, вероятность возврата которого близка к
средней по сайту (а в среднем посетители возвращаются на сайт не очень охотно).
Тоже единственная сессия, с единственным просмотром страницы, длившаяся 18 секунд.
Но есть и положительные факторы: использование планшета, переход на сайт
из поисковика (по видимому Google + AdWords), переход на &amp;ldquo;мейнстримовую&amp;rdquo;
страницу (&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=other: 0&lt;/span&gt;).&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;заключение&#34;&gt;Заключение&lt;/h2&gt;

&lt;p&gt;Proportional hazards это не единственный возможный способ моделирования
потенциального возврата посетителя на сайт. У этой модели есть свои
недостатки, как математические (не всегда выполняющееся предположение о пропорциональности
функций риска у разных членов популяции), так и технические: при обучении необходимо,
чтобы весь dataset был в памяти, так как loss вычисляется по всему набору данных.
По последней причине с этой моделью невозможно использовать минибатчи, и не получится
обучиться на данных крупных сайтов, которые просто не влезают в память.&lt;/p&gt;

&lt;p&gt;Но для сайтов небольшого и среднего размера модель вполне хороша, её главный
плюс это очень экономичное использование данных: во-первых, для обучения используются &lt;strong&gt;все&lt;/strong&gt; данные,
включая сессии, закончившиеся недавно. Традиционные модели игнорируют
свежие (самые ценные) данные, используя их только для формирования целевой переменной.
Во-вторых, с помощью baseline hazard автоматически моделируется динамика выживаемости любой произвольной сложности.
Например, чтобы обучиться затухающей суточной сезонности, которую
мы видели на графике функции риска, традиционным методикам
машинного обучения потребовалось бы очень немаленькое количество данных.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science V. А сколько корова даёт молока?</title>
      <link>https://suilin.ru/post/clv/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/clv/</guid>
      <description>

&lt;p&gt;В идеальном мире маркетолог или владелец бизнеса не только работает с
&amp;ldquo;конверсиями&amp;rdquo;, но использует для оценки посетителей сайта
Customer Lifetime Value (сокращённо CLV) &amp;ndash; грубо говоря, это сумма денег, которую
принесёт посетитель за всё время, пока будет пользоваться сервисом.
Оценка клиентов по CLV является предпочтительной и для мобильных
приложений, и для игр, и для любых онлайн сервисов.&lt;/p&gt;

&lt;p&gt;Но в реальной жизни всё, конечно, не так, как в идеальном мире. Чтобы рассчитать CLV клиента,
нужны исходные данные: средний чек, частота покупок, и время &amp;ldquo;удержания&amp;rdquo;, т.е.
как долго клиент будет пользоваться сервисом.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;clv.png&#34; width=&#34;347&#34; height=&#34;84&#34; /&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Все эти параметры, во-первых, вообще неизвестны для нового посетителя, который
только что пришёл на сайт.&lt;/li&gt;
&lt;li&gt;Во-вторых, параметры &amp;ldquo;средний чек&amp;rdquo; и &amp;ldquo;частота покупок&amp;rdquo;
подразумевают достаточный объем накопленной статистики по каждому клиенту, как минимум
3-5 покупок. В реальной жизни такая статистика накапливается у очень небольшого
процента посетителей.&lt;/li&gt;
&lt;li&gt;В третьих, время удержания конкретного клиента заранее неизвестно,
а среднее время удержания, рассчитанное по сегменту посетителей, имеет огромную дисперсию:
один посетитель перестал заходить на сайт сразу после первого визита (время удержания ноль),
второй пользуется в течение двух лет. И каким, например, надо считать время удержания
посетителя, который появился на сайте только вчера? (кстати, это интересная тема для
&lt;a href=&#34;../user_churn/&#34;&gt;следующей статьи&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Всё это делает расчёт CLV для индивидуального посетителя вообще невозможным,
а расчёт для сегмента посетителей &amp;ndash; крайне ненадёжным, особенно для небольших
сегментов, и новых посетителей.&lt;/p&gt;

&lt;p&gt;В то же время было бы очень заманчивым использовать CLV, например, для
оценки трафика, генерируемого рекламными кампаниями и даже отдельными
рекламными объявлениями. Как это сделать?&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;looking-to-the-future.jpg&#34; width=&#34;512&#34; height=&#34;342&#34; /&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;как-заглянуть-в-будущее&#34;&gt;Как заглянуть в будущее&lt;/h2&gt;

&lt;p&gt;В &lt;a href=&#34;../conversion_factors&#34;&gt;предыдущих&lt;/a&gt; &lt;a href=&#34;../conversion_history&#34;&gt;статьях&lt;/a&gt;
мы прогнозировали конверсию, которая произойдёт
или не произойдет после перехода на сайт. Но ничего не мешает
расширить горизонт прогноза, и в буквальном смысле заглядывать в будущее &amp;ndash;
предсказывать, сколько конверсий
будет у посетителя в течение определённого интервала времени, например
следующих двух месяцев. Это будет не совсем классический CLV, поскольку
прогноз делается для конечного интервала, а не &amp;ldquo;на всю жизнь&amp;rdquo;, тем
не менее такой прогноз вполне достаточен для решения многих
маркетинговых задач. Более того, вместо конверсий можно предсказывать
сумму заказов &amp;ndash; это уже почти CLV.&lt;/p&gt;

&lt;p&gt;В отличие от классического расчёта CLV через предрассчитанные параметры клиентов (удержание,
средний чек, частота), прогноз CLV моделью машинного обучения не требует
никаких предварительных расчётов и
работает на &lt;strong&gt;индивидуальном&lt;/strong&gt; для каждого клиента уровне, что позволяет
легко сравнивать клиентов друг с другом и группировать в сегменты по
ожидаемой выручке.&lt;/p&gt;

&lt;h3 id=&#34;что-прогнозируем&#34;&gt;Что прогнозируем&lt;/h3&gt;

&lt;p&gt;Будем считать, что количество конверсий $k$ берётся из &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%9F%D1%83%D0%B0%D1%81%D1%81%D0%BE%D0%BD%D0%B0&#34; target=&#34;_blank&#34;&gt;распределения Пуассона&lt;/a&gt;:
$$\Pr(k)=\frac{\lambda^k}{k!}e^{-\lambda}$$
тогда для каждого посетителя надо вычислить его индивидуальный коэффициент $\lambda$,
обозначающий ожидаемое количество конверсий за единицу времени (в нашем случае
60 дней).
Эта задача известна, как &lt;a href=&#34;https://en.wikipedia.org/wiki/Poisson_regression&#34; target=&#34;_blank&#34;&gt;пуассоновская регрессия&lt;/a&gt;. В классическом
варианте она решается через линейную модель, но можно использовать
любые продвинутые алгоритмы: деревья решений, нейросети и т.п.&lt;/p&gt;

&lt;p&gt;Построение моделей и оценку качества будем делать на тех же &lt;a href=&#34;../conversion_factors/#%D1%81%D0%B0%D0%B9%D1%82%D1%8B&#34;&gt;сайтах&lt;/a&gt;,
что и в предыдущих статьях.&lt;/p&gt;

&lt;h3 id=&#34;оценка-качества&#34;&gt;Оценка качества&lt;/h3&gt;

&lt;p&gt;Качество регрессии традиционно оценивается или через абсолютную ошибку
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_squared_error&#34; target=&#34;_blank&#34;&gt;MSE&lt;/a&gt; (среднеквадратичная ошибка)
или
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_absolute_error&#34; target=&#34;_blank&#34;&gt;MAE&lt;/a&gt; (средняя абсолютная ошибка),
или через через относительную ошибку, например &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean_absolute_percentage_error&#34; target=&#34;_blank&#34;&gt;MAPE&lt;/a&gt;
 (Mean Absolute Percentage Error).
К сожалению, все эти способы подразумевают, что целевая переменная
имеет нормальное или близкое к нему распределение, и для нашей задачи
не подходят. Поясню на примерах:&lt;/p&gt;

&lt;p&gt;Если использовать абсолютную ошибку, то разница
между 10 конверсиями и 11 конверсиями будет такой же, как разница между нулём конверсий
и одной конверсией. Но с точки зрения бизнеса, ноль конверсий и одна конверсия
это большая разница, а 10 и 11 &amp;ndash; незначительная.&lt;/p&gt;

&lt;p&gt;Если использовать относительную ошибку, то в случае, когда предсказана
одна конверсия, а в реальности было ноль конверсий, ошибка станет бесконечной &amp;ndash; так тоже не годится.
Можно добавлять к истинному значению сглаживающую константу $\epsilon$, чтобы
избежать деления на 0, но такая ошибка станет просто абстрактным числом,
не имеющим внятной интерпретации.&lt;/p&gt;

&lt;p&gt;Более приемлемым вариантом оказалось использование метрики &lt;a href=&#34;https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve&#34; target=&#34;_blank&#34;&gt;AUC&lt;/a&gt;,
только для регрессионных задач она называется Concordance index (c-index, c-statistic), и интерпретируется
не как площадь под кривой, а как вероятность того, что в
парах &amp;ldquo;прогноз &amp;ndash; истинное значение&amp;rdquo;, относящимся к двум случайно выбранным примерам,
прогнозы будут правильно отранжированы.
Т.е. если истинное значение в первой паре больше значения во второй паре, то
и прогноз из первой пары будет больше прогноза из второй пары.&lt;/p&gt;

&lt;p&gt;Concordance index не чувствителен к калибровке прогноза, это тоже важно
(вопроса калибровки коснёмся немного позже).&lt;/p&gt;

&lt;h2 id=&#34;входные-данные&#34;&gt;Входные данные&lt;/h2&gt;

&lt;p&gt;Будем использовать такой же набор признаков, как и в &lt;a href=&#34;../conversion_history/#%D0%BA%D0%BE%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%B8%D1%80%D1%83%D0%B5%D0%BC-%D0%BF%D1%80%D0%B8%D0%B7%D0%BD%D0%B0%D0%BA%D0%B8&#34;&gt;предыдущей модели&lt;/a&gt;,
за исключением
признаков, относящихся к текущему переходу на сайт, т.к. в новой модели
нет &amp;ldquo;текущего&amp;rdquo; перехода. Т.е. оставим только признаки, относящиеся к истории переходов, к истории сессий, и
к посетителю в целом (браузер, гео, устройство и т.п.).&lt;/p&gt;

&lt;p&gt;Кроме этого, нужен ещё один признак, играющий в модели важную роль: время
с момента последней активности посетителя.&lt;/p&gt;

&lt;p&gt;В предыдущих моделях за текущий момент времени принимался момент перехода на сайт.
Это было удобным для изучения факторов, влияющих на вероятность конверсии, но прогноз,
который выдавала модель, сам по себе не имел практической ценности. В самом деле,
какой смысл прогнозировать результат сессии, если можно просто немного подождать,
сессия закончится сама собой, и результат будет виден и так?&lt;/p&gt;

&lt;p&gt;В новой модели мы прогнозируем количество конверсий для произвольной выборки посетителей, которые
были на сайте &lt;em&gt;когда то раньше&lt;/em&gt;. Очевидно, что ожидаемое количество конверсий
для посетителя, который последний раз зашёл на сайт год назад, и c тех пор не появлялся, будет
близко к нулю. В то же время посетитель, который заходил на сайт вчера, имеет гораздо больше шансов
на появление новых конверсий, чем предыдущий. Поэтому время с момента
последней активности &lt;strong&gt;time_since_last&lt;/strong&gt; обязательно должно быть включено в модель.&lt;/p&gt;

&lt;p&gt;Но в истории действий посетителей, из которой формируются данные для обучения, нет понятия
&amp;ldquo;текущее время&amp;rdquo;. В принципе любой момент после первого появления посетителя на сайте может быть выбран, как текущий.
Тогда то, что было то этого, принимается за историю, а на основе того, что было после, рассчитывается
целевая переменная, т.е. количество конверсий или сумма заказов.
&lt;figure &gt;
&lt;img width=&#34;791&#34; height=&#34;88&#34; class=&#34;lazy&#34; src=&#34;history_lr.gif&#34;
      data-src=&#34;history.gif&#34; data-srcset=&#34;history@2x.gif 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;$t_0$ &amp;ndash; время первого появления посетителя на сайте; $t_{end}$ &amp;ndash;
максимальное время в данных; $t_1,\dots,t_9$ &amp;ndash; случайные моменты времени, используемые для обучения.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Итак, для каждого посетителя мы будем случайным образом выбирать
&amp;ldquo;текущий&amp;rdquo; момент времени, причём для одного и того же
посетителя может быть выбрано несколько разных моментов, относящихся
к разным периодам его жизни. Заодно мы таким образом проводим
&lt;a href=&#34;https://www.quora.com/What-does-the-term-data-augmentation-mean-in-the-context-of-Machine-Learning&#34; target=&#34;_blank&#34;&gt;data augmentation&lt;/a&gt;:
из одного физического посетителя генерируем несколько &amp;ldquo;виртуальных&amp;rdquo;.
От каждого текущего момента отсчитывается 60 дней в будущее и берётся суммарное
количество конверсий за этот период в качестве целевой переменной.&lt;/p&gt;

&lt;p&gt;У посетителей, которые очень давно не были на сайте, количество ожидаемых будущих конверсий стремится к нулю
(они скорее всего уже никогда не вернутся). Поэтому имеет смысл ограничить
максимальный период &amp;ldquo;бездействия&amp;rdquo; сроком, например, в 180 дней. Т.е. будем включать
в обучающую и тестовую выборки только таких посетителей, у которых последнее событие было не позже 180 дней
от текущего момента времени. У остальных посетителей ожидаемое количество конверсий
можно считать нулевым.&lt;/p&gt;

&lt;h2 id=&#34;результаты-shop-ml&#34;&gt;Результаты, shop.ml&lt;/h2&gt;

&lt;p&gt;На тестовой выборке получен Concordance Index ~94%, это очень хороший показатель.
В качестве тестовой выборки использовались данные за последние два месяца,
 которые модель не видела при обучении и настройке (из двенадцати имеющихся месяцев) &amp;ndash; всё, как в реальной жизни.&lt;/p&gt;

&lt;p&gt;Чтобы эксперимент был честным, из данных были удалены сессии посетителей,
которые посещали внутренние домены, недоступные из внешнего мира
(т.е. сессии персонала магазина и разработчиков). Если не удалять эти данные,
то модель легко обучается распознавать такие нетипичные сессии, и Concordance
Index вырастает до 95-96%.&lt;/p&gt;

&lt;p&gt;Посмотрим на влияние признаков (методика интерпретации диаграмм
влияния описана в &lt;a href=&#34;../conversion_history/#%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C-%D0%B4%D0%BB%D1%8F-luxshop-ml-%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%D1%8B&#34;&gt;предыдущей статье&lt;/a&gt;):&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;590&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;shop_summary_lr.png&#34;
      data-src=&#34;shop_summary.png&#34; data-srcset=&#34;shop_summary@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Признаки с наибольшим абсолютным влиянием, сайт shop.ml&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;Решающую роль играет признак &lt;strong&gt;time_since_last&lt;/strong&gt;, не зря мы
включили его! Также выраженное положительное влияние у &lt;strong&gt;pageview_count&lt;/strong&gt;
и &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$order_total&lt;/span&gt;&lt;/strong&gt; (суммарная стоимость всех заказов посетителя).
Физический смысл SHAP values: изменение SHAP value на одну единицу
изменяет ожидаемое количество конверсий в два раза. Т.е. например
value=2 даёт 4x прирост количества конверсий относительно среднего по сайту
(в среднем на одного посетителя shop.ml приходится 0.00086 будущих конверсий)&lt;/p&gt;

&lt;p&gt;Посмотрим
на детальные диаграммы:
&lt;figure &gt;
&lt;img width=&#34;451&#34; height=&#34;317&#34; class=&#34;lazy&#34; src=&#34;shop_time_since_last_lr.png&#34;
      data-src=&#34;shop_time_since_last.png&#34; data-srcset=&#34;shop_time_since_last@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;С течением времени SHAP value изменяется от +2.5 до -2.5, т.е. ожидаемое
количество конверсий падает в ~32 раза за 180 дней.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;453&#34; height=&#34;329&#34; class=&#34;lazy&#34; src=&#34;shop_pageview_count_lr.png&#34;
      data-src=&#34;shop_pageview_count.png&#34; data-srcset=&#34;shop_pageview_count@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Количество будущих конверсий начинает расти после просмотра 4-x страниц. Просмотр
более 30 страниц вызывает рост кол-ва конверсий примерно в 4 раза.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;figure &gt;
&lt;img width=&#34;461&#34; height=&#34;328&#34; class=&#34;lazy&#34; src=&#34;shop_order_total_lr.png&#34;
      data-src=&#34;shop_order_total.png&#34; data-srcset=&#34;shop_order_total@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Клиенты, которые заказывали много, продолжат делать заказы &amp;ndash; видна выраженная
зависимость ожидаемого кол-ва конверсий от суммы заказов.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;В этой модели нам интересны не столько зависимости от признаков, сколько результаты
прогноза. Чтобы лучше понять, что предсказывает модель, разобьём
тестовую выборку на несколько групп, в соответствии со значением целевой переменной:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;0 конверсий&lt;/li&gt;
&lt;li&gt;1 конверсия&lt;/li&gt;
&lt;li&gt;2 конверсии&lt;/li&gt;
&lt;li&gt;3 конверсии&lt;/li&gt;
&lt;li&gt;от 4 до 5 конверсий&lt;/li&gt;
&lt;li&gt;более 5 конверсий&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;и будем смотреть, насколько точным получается прогноз для каждой группы.&lt;/p&gt;

&lt;p&gt;&lt;figure &gt;
&lt;img width=&#34;426&#34; height=&#34;267&#34; class=&#34;lazy&#34; src=&#34;shop_num_examples_lr.png&#34;
      data-src=&#34;shop_num_examples.png&#34; data-srcset=&#34;shop_num_examples@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Количество примеров в каждой из групп. Группа с нулевой конверсией содержит
более чем в 1000 раз больше примеров, чем остальные группы (обратите внимание на логарифмическую шкалу).&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;
Посчитаем среднее прогнозное значение в каждой группе. В идеале средний
прогноз должен совпасть со средним истинным значением, например для группы
&amp;ldquo;1 конверсия&amp;rdquo; средний прогноз должен быть равен единице.&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;375&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;shop_raw_forecast_lr.png&#34;
      data-src=&#34;shop_raw_forecast.png&#34; data-srcset=&#34;shop_raw_forecast@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Сравнение прогнозных и истинных средних значений в каждой группе.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;По графику видно, что чуда не произошло: прогноз и истина не совпадают.
Форма кривой прогнозных значений в принципе близка к форме кривой
истинных значений, и также монотонно растёт. Т.е. такие прогнозные значения
вполне можно использовать, как абстрактную оценку потенциала посетителя,
и для сравнения посетителей друг с другом. Но для каких либо
абсолютных оценок (например, сколько конверсий ожидается в конкретном сегменте посетителей?)
такой прогноз не годится. Впрочем, ситуация легко поправима.&lt;/p&gt;

&lt;h2 id=&#34;калибровка-прогноза&#34;&gt;Калибровка прогноза&lt;/h2&gt;

&lt;p&gt;Наша модель предполагает, что значения целевой переменной распределены
более-менее в соответствии с распределением Пуассона. Посмотрим,
сколько значений в каждой группе было бы, если бы значения были действительно
распределены таким образом, т.е. посчитаем вероятное кол-во примеров
в каждой группе для распределения Пуассона с $\lambda=0.00086$ (среднее кол-во
конверсий на посетителя):&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;423&#34; height=&#34;285&#34; class=&#34;lazy&#34; src=&#34;shop_num_examples_possion_lr.png&#34;
      data-src=&#34;shop_num_examples_possion.png&#34; data-srcset=&#34;shop_num_examples_possion@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Видно, что наличие 3-х и более конверсий &amp;ndash; крайне маловероятно,
ожидаемое количество примеров в этих группах приближается к нулю. То есть
распределение Пуассона не очень хорошо описывает то, что происходит в реальной жизни.
Количество примеров с нулевой конверсией в наших данных настолько велико, что они
&amp;ldquo;перетягивают одеяло на себя&amp;rdquo;, модель считает отличное от нуля значение маловероятным,
и занижает прогноз для примеров, в которых ожидается ненулевое количество конверсий. Это видно
и на распределении прогнозных значений:
&lt;figure &gt;
&lt;img width=&#34;387&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;shop_forecast_distribution_lr.png&#34;
      data-src=&#34;shop_forecast_distribution.png&#34; data-srcset=&#34;shop_forecast_distribution@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Сравнение распределений истинных и прогнозных значений. Видно, что прогнозные значения &amp;ldquo;прижаты&amp;rdquo; влево,
в область меньших величин.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;В принципе можно было бы подобрать вместо распределения Пуассона другое,
с более тяжелым &amp;ldquo;хвостом&amp;rdquo; справа (fat-tailed), например что нибудь из семейства
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%9F%D0%B0%D1%80%D0%B5%D1%82%D0%BE&#34; target=&#34;_blank&#34;&gt;распределений Парето&lt;/a&gt;.
Мы пойдем более простым путём, и просто применим к прогнозам преобразование,
которое приведёт их к правильному масштабу. Поиск и использование
такого преобразования называется &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Calibration_(statistics)#In_regression&#34; target=&#34;_blank&#34;&gt;калибровкой модели&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Существуют разнообразные методики калибровки различной степени сложности,
включая даже обучение отдельной модели, которая будет получать на вход
прогнозы исходной модели и выдавать откалиброванный прогноз. Но мы не
будем мудрить, у нас уже почти правильный прогноз, достаточно просто
домножить его на константу и немного скорректировать форму &amp;ldquo;загиба&amp;rdquo; в
конце кривой:
$$y^* = \hat{y}^{\beta}k $$
где $\hat{y}$ &amp;ndash; исходный прогноз; $y^*$ &amp;ndash; калиброванный прогноз; $k, \beta$ &amp;ndash;
коэффициенты, минимизирующие ошибку прогноза, и подбираемые с помощью кросс-валидации.&lt;/p&gt;

&lt;p&gt;После калибровки получается такой график:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;375&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;shop_calibrated_forecast_lr.png&#34;
      data-src=&#34;shop_calibrated_forecast.png&#34; data-srcset=&#34;shop_calibrated_forecast@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Стало намного лучше! Такой прогноз уже вполне можно использовать
для абсолютной оценки потенциала сегмента аудитории.&lt;/p&gt;

&lt;h3 id=&#34;индивидуальные-прогнозы&#34;&gt;Индивидуальные прогнозы&lt;/h3&gt;

&lt;p&gt;Мы рассматривали &lt;em&gt;средний&lt;/em&gt; прогноз для аудиторного сегмента, а что
с прогнозами на уровне отдельных посетителей, насколько они хороши? Чтобы
получить представление об этом, построим распределения прогнозных
значений отдельно для каждой группы:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;407&#34; height=&#34;352&#34; class=&#34;lazy&#34; src=&#34;shop_forecast_per_bucket_lr.png&#34;
      data-src=&#34;shop_forecast_per_bucket.png&#34; data-srcset=&#34;shop_forecast_per_bucket@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;В идеале все распределения должны быть узкими, вытянутыми по вертикали вдоль
истинного значения. В реальности распределения достаточно размыты:
для истинного значения &lt;strong&gt;1 конверсия&lt;/strong&gt; 90% прогнозов находится в интервале
от 0.02 до 3.9; для значения &lt;strong&gt;3 конверсии&lt;/strong&gt; &amp;ndash; в интервале от 0.4 до 8.6.&lt;/p&gt;

&lt;p&gt;Таким образом, совсем точного персонального прогноза не получается: для этого
потребовалась бы настоящая машина времени, которая заглянет в будущее,
и скажет, какое решение, покупать или не покупать, примет посетитель. Но если делать
групповой прогноз, то ошибки &amp;ldquo;в плюс&amp;rdquo; и &amp;ldquo;в минус&amp;rdquo; нейтрализуют
друг друга, и точность значительно вырастает.&lt;/p&gt;

&lt;h2 id=&#34;прогноз-суммы-заказов-shop-ml&#34;&gt;Прогноз суммы заказов, shop.ml&lt;/h2&gt;

&lt;p&gt;Используется точно такая же модель, как и для прогноза конверсий,
заменяется только целевая переменная. По своим характеристикам модель
получается очень похожей на предыдущую, остановимся только на отличиях:
&lt;figure &gt;
&lt;img width=&#34;387&#34; height=&#34;275&#34; class=&#34;lazy&#34; src=&#34;shop_forecast_distribution_price_lr.png&#34;
      data-src=&#34;shop_forecast_distribution_price.png&#34; data-srcset=&#34;shop_forecast_distribution_price@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Сравнение распределений истинных и прогнозных значений. Распределение истинных
значений принципиально отличается от прогнозных.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;
В магазине не бывает заказов стоимостью рубль, и не бывает заказов
стоимостью десять рублей, но ноль &amp;ndash; это допустимое значение,
соответствующее отсутствию заказов. В результате получается
бимодальное распределение, с одиночным пиком в районе нуля, и &amp;ldquo;куполом&amp;rdquo;
в районе 100-100000 руб. Естественно, распределение Пуассона не может
принять такую сложную форму, поэтому распределение прогнозов
сильно отличается от распределения истинных значений, и говорить о точном
прогнозе на индивидуальном уровне здесь уже нет смысла.&lt;/p&gt;

&lt;p&gt;Тем не менее, на групповом уровне после калибровки получается очень
приличный прогноз:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;381&#34; height=&#34;265&#34; class=&#34;lazy&#34; src=&#34;shop_calibrated_forecast_price_lr.png&#34;
      data-src=&#34;shop_calibrated_forecast_price.png&#34; data-srcset=&#34;shop_calibrated_forecast_price@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Прогноз с разбивкой на группы: 0 (нет заказов), 0-2000 руб., 2000-5000 руб.,
5000-10000 руб., &amp;gt; 10000 руб.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;Использование &lt;a href=&#34;https://en.wikipedia.org/wiki/Tweedie_distribution&#34; target=&#34;_blank&#34;&gt;распределения Твиди&lt;/a&gt;,
которое теоретически может быть мультимодальным, дало результат, практически
не отличающийся от распределения Пуассона. Видимо, чтобы получить
более правильные прогнозные значения, надо честно смоделировать процесс генерации данных,
и делать два прогноза, первый &amp;ndash; будут ли вообще заказы, и второй &amp;ndash; сумма
ожидаемых заказов.&lt;/p&gt;

&lt;h2 id=&#34;результаты-для-luxshop-ml&#34;&gt;Результаты для luxshop.ml&lt;/h2&gt;

&lt;p&gt;Тоже получилась очень похожая на предыдущую модель, с близким
Concordance Index и с аналогичным влиянием признаков на прогноз:
&lt;figure &gt;
&lt;img width=&#34;634&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;luxhop_summary_lr.png&#34;
      data-src=&#34;luxhop_summary.png&#34; data-srcset=&#34;luxhop_summary@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Признаки с наибольшим абсолютным влиянием, сайт luxshop.ml&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;380&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;luxhop_calibrated_forecast_lr.png&#34;
      data-src=&#34;luxhop_calibrated_forecast.png&#34; data-srcset=&#34;luxhop_calibrated_forecast@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Ошибка предсказания после калибровки получилась немного больше, чем у shop.ml, возможно
потому, что у luxshop.ml в целом меньше данных и меньше размеры
групп, например в группе &amp;ldquo;2 конверсии&amp;rdquo; всего 116 посетителей,
в отличие от 483 у shop.ml.&lt;/p&gt;

&lt;h2 id=&#34;результаты-для-courses-ml&#34;&gt;Результаты для courses.ml&lt;/h2&gt;

&lt;p&gt;На этом сайте нет целей типа &amp;ldquo;заказ&amp;rdquo;, поэтому будем предсказывать
не количество конверсий, а количество сессий за следующие 60 дней.
Concordance Index этой модели 90%, несколько хуже, чем у магазинов.&lt;/p&gt;

&lt;p&gt;&lt;figure &gt;
&lt;img width=&#34;626&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;courses_summary_lr.png&#34;
      data-src=&#34;courses_summary.png&#34; data-srcset=&#34;courses_summary@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Признаки с наибольшим абсолютным влиянием, сайт courses.ml&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;
Как и в других моделях, наибольшее влияние оказывает признак &lt;strong&gt;time_since_last&lt;/strong&gt;.
Но набор остальных признаков, оказывающих заметное влияние, несколько иной.&lt;/p&gt;

&lt;p&gt;&lt;figure &gt;
&lt;img width=&#34;382&#34; height=&#34;264&#34; class=&#34;lazy&#34; src=&#34;courses_calibrated_forecast_lr.png&#34;
      data-src=&#34;courses_calibrated_forecast.png&#34; data-srcset=&#34;courses_calibrated_forecast@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;
Эта модель чуть хуже поддаётся калибровке, чем модели магазинов, но результат
всё равно приемлемый.&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;407&#34; height=&#34;352&#34; class=&#34;lazy&#34; src=&#34;courses_forecast_per_bucket_lr.png&#34;
      data-src=&#34;courses_forecast_per_bucket.png&#34; data-srcset=&#34;courses_forecast_per_bucket@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Дисперсия прогнозов внутри каждой группы выше, чем при прогнозе конверсий. Например, для
группы &amp;ldquo;1 сессия&amp;rdquo; 90% прогнозов лежит в диапазоне [0.0015, 5.7], в то время
как для shop.ml аналогичный диапазон &amp;ldquo;1 конверсия&amp;rdquo; более узкий: [0.02, 3.9]. То есть предсказывать
количество будущих сессий сложнее, чем количество конверсий.&lt;/p&gt;

&lt;h2 id=&#34;scorecards&#34;&gt;Scorecards&lt;/h2&gt;

&lt;figure &gt;
&lt;img width=&#34;552&#34; height=&#34;612&#34; class=&#34;lazy&#34; src=&#34;shop_scorecards_lr.png&#34;
      data-src=&#34;shop_scorecards.png&#34; data-srcset=&#34;shop_scorecards@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Образцы scorecards для прогноза количества конверсий на сайте shop.ml.
Одному делению шкалы соответствует изменение количества конверсий в 2 раза.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;Одно из преимуществ применяемого нами подхода это то, что прогноз
не является чёрным ящиком, или предсказанием, волшебным образом
появившимся из недр модели. Ведь довольно трудно принимать бизнес-решения
на основе непонятной AI магии. А вдруг модель ошибается? Вдруг на вход
пришли такие данные, с которыми модель не обучена работать?&lt;/p&gt;

&lt;p&gt;Но благодаря наличию &lt;a href=&#34;../conversion_factors#shap&#34;&gt;SHAP values&lt;/a&gt;, работа модели становится абсолютно
прозрачной. Достаточно просто посмотреть на scorecard, чтобы понять,
&lt;em&gt;почему&lt;/em&gt; для данного посетителя выдан такой прогноз, и соотнести решение,
принятое моделью, с собственным опытом и здравым смыслом. Это не оставляет
места никакой магии: есть обоснование решений, можно или согласиться с ним,
или не согласиться и перестать использовать модель (или отправить
модель на переобучение).&lt;/p&gt;

&lt;p&gt;Кстати, не до конца решенная проблема в машинном обучении &amp;ndash; это проверка
и тестирование всего конвейера подготовки данных для модели. В коде
легко допустить ошибку, которая будет совершенно невидима извне, и
единственное её проявление будет в ухудшении качества работы модели. Но заметить
это ухудшение очень сложно, т.к. нет эталона качества.
SHAP values помогают и здесь: за время подготовки моделей для этих статей
я обнаружил с помощью SHAP-диаграмм несколько серьезных ошибок, которые
в противном случае надолго (возможно навсегда) остались бы незамеченными.&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;742&#34; height=&#34;464&#34; class=&#34;lazy&#34; src=&#34;howmuch_lr.png&#34;
      data-src=&#34;howmuch.png&#34; data-srcset=&#34;howmuch@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;h1 id=&#34;заключение&#34;&gt;Заключение&lt;/h1&gt;

&lt;p&gt;Мы убедились в том, что прогноз будущих конверсий и CLV для посетителей
сайта вполне возможен и даёт пригодные для практического использования
результаты даже на несложных моделях.&lt;/p&gt;

&lt;p&gt;Конечно, это модели еще не пригодны для применения в production, есть
ещё много вещей, которые можно сделать лучше.
В частности, мы использовали весьма простое конструирование признаков:
всего два уровня адресов страниц; игнорировали содержащиеся в URL
параметры, которые могут нести ценную информацию о действиях посетителя;
никак не разделяли действия, которые были совершены недавно и действия,
которые были в далёком прошлом;
использовали из данных о заказах только сумму заказа; не обращали внимания
на последовательность действий (брали только суммарное их количество).&lt;/p&gt;

&lt;p&gt;Также использовался ограниченный набор входных данных: у нас не было
ни соцдема, ни поисковых фраз, ни внутренней информации о посетителях,
которая содержится в CRM магазина, ни информации о взаимодействии с
сайтом на интерфейсном уровне, подобной той, которую собирает WebVisor.
Если задействовать больше данных, то прогнозы, естественно, станут
более точными.&lt;/p&gt;

&lt;p&gt;Структура данных о действиях посетителей в принципе очень близка
к структуре данных о действиях пользователей мобильных приложений, т.е.
аналогичные модели можно использовать и для работы с CLV в монетизируемых
приложениях и играх.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science IV. Кто владеет прошлым, тот контролирует будущее</title>
      <link>https://suilin.ru/post/conversion_history/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/conversion_history/</guid>
      <description>

&lt;p&gt;В &lt;a href=&#34;../conversion_factors&#34;&gt;предыдущей статье&lt;/a&gt; мы рассмотрели самую простую модель оценки
конверсионности посетителя по параметрам первого посещения.
Благодаря использованию &lt;a href=&#34;../conversion_factors#shap&#34;&gt;SHAP values&lt;/a&gt;
можно интерпретировать модели любой сложности, поэтому
усложним задачу, и оценим конверсионность посетителя,
у которого есть история предыдущего взаимодействия с сайтом.&lt;/p&gt;

&lt;p&gt;Наличие такой истории даст нам
много новой и полезной информации о намерениях посетителя. Но для
начала немного разберёмся с микроструктурой данных,
в частности с понятиями &amp;ldquo;визит/сессия&amp;rdquo; и &amp;ldquo;переход&amp;rdquo;,
которые в современной интернет-аналитике перемешаны друг с другом.
Работа полученных моделей будет оцениваться на тех же &lt;a href=&#34;../conversion_factors/#%D1%81%D0%B0%D0%B9%D1%82%D1%8B&#34;&gt;сайтах&lt;/a&gt; , что и в предыдущей статье.&lt;/p&gt;

&lt;h2 id=&#34;что-такое-сессия&#34;&gt;Что такое сессия?&lt;/h2&gt;

&lt;p&gt;Базовое понятие интернет-аналитики, вокруг которого выстраивается
большинство показателей, которые мы видим в отчётах, это &lt;em&gt;сессия&lt;/em&gt; (она же &lt;em&gt;визит&lt;/em&gt; в русскоязычной интерпретации).
Под сессией обычно подразумевается интервал времени, в котором
не было более чем получасовых пауз в активности посетителя. То есть
посетитель пришёл на сайт (сессия началась), что-то посмотрел, ушёл обедать,
через час вернулся, и его послеобеденная активность на сайте засчитается уже в новую сессию.
Хорошо ли это?&lt;/p&gt;

&lt;p&gt;На самом деле этот получасовой тайм-аут имеет происхождение
ещё из тех доисторических времён, когда пользователь натурально
&lt;em&gt;выходил в интернет&lt;/em&gt; с помощью модема, и по завершению сеанса работы (с поминутной оплатой)
отключался. Естественно, никто в здравом уме не ушёл бы на обед с
подключенным модемом и продолжающейся сессией. Получасовой тайм-аут
вполне соотносился с реальностью и соответствовал общепринятой практике
работы c интернетом.&lt;/p&gt;

&lt;p&gt;Но в современном мире человек находится в онлайне всегда, и сайт может
быть открыт во вкладке браузера днями, неделями и даже месяцами: пользователь
просто будет переключаться на вкладку с сайтом в моменты, когда в этом возникает необходимость.
Эти переключения никак не связаны с &amp;ldquo;сеансом работы в интернет&amp;rdquo;, и тайм-аут 30 минут в современных реалиях
оказывается просто взятым с потолка. Поэтому лучше определить
тайм-аут, исходя из реальной статистики поведения людей на сайтах.&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;839&#34; height=&#34;296&#34; class=&#34;lazy&#34; src=&#34;page_interval_lr.png&#34;
      data-src=&#34;page_interval.png&#34; data-srcset=&#34;page_interval@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Распределение интервалов между просмотрами
страниц каждым посетителем сайта shop.ml. Слева отображены интервалы
от 5 минут до 2 часов, справа от 5 минут до 24 часов. Такое же распределение,
с незначительными отличиями, наблюдается и на других сайтах.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;На левой диаграмме видно, что точка &amp;ldquo;30 минут&amp;rdquo; не обладает никакими
особенными свойствами: количество интервалов плавно
падает с возрастанием размера интервала, и до, и после этой точки. На правой диаграмме
более интересная картина: начиная примерно с 8 часов интервалы между просмотрами
выходят на &amp;ldquo;равнину&amp;rdquo;, а ближе к 24 часам количество интервалов даже растёт.
Рост объясняется суточной цикличностью, например человек посмотрел сайт перед уходом на работу,
количество времени было ограничено, поэтому &amp;ldquo;досматривает&amp;rdquo; его на следующий
день примерно в то же время. Но нас больше интересует момент выхода на минимальную
вероятность интервала: разумно предположить, что 8 часов это и есть наиболее естественный тайм-аут между
сессиями. Этот тайм-аут мы и будем использовать в дальнейшем во всех моделях.&lt;/p&gt;

&lt;h2 id=&#34;сессии-или-переходы&#34;&gt;Сессии или переходы?&lt;/h2&gt;

&lt;figure &gt;
&lt;img width=&#34;850&#34; height=&#34;348&#34; class=&#34;lazy&#34; src=&#34;user_timelines_lr.png&#34;
      data-src=&#34;user_timelines.png&#34; data-srcset=&#34;user_timelines@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;&amp;ldquo;Истории жизни&amp;rdquo; нескольких посетителей сайта.
Цветными кружками обозначены переходы на сайт, цветными треугольниками &amp;ndash;
достижения целей, тонкими вертикальными штрихами &amp;ndash; просмотры страниц,
черными точками &amp;ndash; моменты начала сессий.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;На диаграмме видно, что обычно переход
на сайт совпадает с началом сессии, но не всегда. Возможны сессии
без переходов, например у пользователей #11 и #17
 (вероятно, сайт висит во вкладке браузера и пользователь
время от времени переключается на него), и наоборот, возможны переходы
внутри сессий. На диаграмме переходы внутри сессий не очень хорошо различимы
 (несколько полупрозрачных кружков сливаются в один более насыщенный),
 поэтому рассмотрим подробную историю переходов одного из посетителей,
например #15:
&lt;style&gt;th {text-align: center;}&lt;/style&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Дата и время&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Тип перехода&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Номер сессии&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-12 00:03:59&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;organic&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-17 15:18:20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-17 15:19:46&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;direct&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-18 00:25:05&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-22 00:19:23&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;direct&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-22 00:20:25&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2016-12-22 00:24:11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2017-01-06 00:20:07&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2017-01-06 00:21:51&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;direct&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2017-02-13 19:13:00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;2017-02-13 19:13:27&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ad&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Видно, что в течение многих сессий совершается два или три перехода.
В среднем по всем посетителям количество переходов на 15-30% превышает количество визитов.&lt;/p&gt;

&lt;p&gt;Как с этим поступают современные системы интернет аналитики? Очень просто:
каждый новый переход на сайт считается началом нового визита.
Фактически переходы приравниваются к визитам, что приводит
к тому, что у посетителей, совершающих много переходов, визиты нарезаются в мелкую лапшу.
Для систем интернет-аналитики это нормальный компромиссный подход, в противном случае
пришлось бы ввести дополнительную сущность &amp;ldquo;переход&amp;rdquo; и все отчёты сильно бы усложнились.&lt;/p&gt;

&lt;p&gt;Но мы при построении моделей не станем упрощать действительность и честно учтём
визиты и переходы, как отдельные сущности. Границы визитов
будут определяться только таймаутом 8 часов, и ничем более.&lt;/p&gt;

&lt;h2 id=&#34;конструируем-признаки&#34;&gt;Конструируем признаки&lt;/h2&gt;

&lt;p&gt;Перейдем к собственно моделированию. Мы рассматриваем посетителей, которые
уже сделали хотя бы один переход на сайт, то есть у них есть история.
История содержит данные о сессиях, переходах, просмотрах страниц,
кликах на ведущие вовне ссылки, загрузках файлов и т.п. Как и в прошлой
модели, будем рассматривать посетителя в момент, когда он совершает новый
переход на сайт (только это будет уже не первый переход), и предсказывать,
произойдет ли конверсия до следующего перехода или до завершения сессии.&lt;/p&gt;

&lt;p&gt;К признакам, которые уже были в предыдущей модели, добавляются новые признаки,
сконструированные по историческим данным. Пример нескольких таких признаков:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;time_since_prev_landing&lt;/strong&gt; &amp;ndash; интервал времени с момента предыдущего перехода на сайт.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;time_since_sess_start&lt;/strong&gt; &amp;ndash; интервал времени от начала сессии до текущего перехода. Для переходов,
совпавших с началом сессии, равен нулю.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;time_since_prev_sess&lt;/strong&gt; &amp;ndash; интервал времени от последнего события в предыдущей сессии до
текущего перехода&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;landing_num&lt;/strong&gt; &amp;ndash; порядковый номер текущего перехода во всей истории переходов&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sess_interval&lt;/strong&gt; &amp;ndash; средний интервал времени между сессиями посетителя, по данным предыдущих сессий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;avg_sess_len&lt;/strong&gt; &amp;ndash; средняя продолжительность сессии посетителя, по данным предыдущих сессий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$order_step&lt;sub&gt;n&lt;/sub&gt;&lt;/span&gt;&lt;/strong&gt; &amp;ndash; количество достижений цели, являющейся n-ным шагом в
конверсионной воронке, т.е. одним из этапов совершения заказа, за всю историю до текущего момента.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;user_age&lt;/strong&gt; &amp;ndash; &amp;ldquo;возраст&amp;rdquo; посетителя, т.е. промежуток времени от момента,
когда посетитель впервые зашёл на сайт, до текущего перехода.
нет смысла засчитывать это время, как активное).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=X&lt;/span&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=X&lt;/span&gt;&lt;/strong&gt; &amp;ndash; суммарное количество просмотров страниц в истории, соответствующих
пути &lt;strong&gt;X&lt;/strong&gt; (берётся первый уровень пути для path&lt;sub&gt;1&lt;/sub&gt;, первый и второй уровни для path&lt;sub&gt;2&lt;/sub&gt;).
Это запись в &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%82%D0%B0%D1%86%D0%B8%D1%8F_%D0%90%D0%B9%D0%B2%D0%B5%D1%80%D1%81%D0%BE%D0%BD%D0%B0&#34; target=&#34;_blank&#34;&gt;нотации Айверсона&lt;/a&gt;
$\sum[path_i=X]$, у которой опущены для краткости квадратные скобки.
В обобщённом виде, без указания конкретного пути, эти признаки записываются как
&lt;strong&gt;path1_history&lt;/strong&gt; и &lt;strong&gt;path2_history&lt;/strong&gt;.

&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;все-ли-признаки-одинаково-полезны&#34;&gt;Все ли признаки одинаково полезны?&lt;/h2&gt;

&lt;p&gt;На основе истории посетителя можно сконструировать в общем то неограниченное
количество признаков, лимитом здесь является только фантазия автора модели.
Возникает логичный вопрос &amp;ndash; в какой момент надо остановиться? Проблема в том,
что несмотря на объем данных (миллионы посетителей), в них не так много положительных
примеров совершившихся конверсий (всего несколько тысяч). В то же время
количество признаков в моделях, учитывая что у нас в основном категориальные
признаки, которые переводятся в числовые через one-hot encoding, тоже
легко переваливает за тысячу и приближается к количеству позитивных примеров.
Это означает, что наши модели потенциально подвержены &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;переобучению&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Если подать на вход модели просто шум вместо реальных признаков, всё равно в нём найдутся случайные
корреляции с целевой переменной (конверсией), модель сделает вид, что обучилась, и даже будет показывать
степень влияния &amp;ldquo;признаков&amp;rdquo; на результат. Но это влияние будет миражом, фейковой новостью,
и только введёт в заблуждение пользователя модели. Любой из сконструированных
нами признаков в принципе может оказаться таким шумом, и &lt;em&gt;ухудшить&lt;/em&gt;
способности модели к предсказанию на новых данных. Как же определить, в каких признаках
содержится полезный сигнал, а в каких только шум?&lt;/p&gt;

&lt;p&gt;Некоторые виды моделей, например деревья решений,
способны выдавать показатели &amp;ldquo;важности&amp;rdquo; признаков (feature importance), и обычно полезные признаки
отбирают именно по этим показателям. Но эта &amp;ldquo;важность&amp;rdquo; точно так же
переобучается, как и сама модель: в конечном итоге важность это
показатель того, сколько корреляций с целевой переменной обнаружила модель в
признаке. Это корреляции могут оказаться просто случайными.&lt;/p&gt;

&lt;p&gt;Мы пойдём более честным, хотя и вычислительно затратным путём, и будем
использовать метод, называемый &lt;em&gt;Sequential Feature Selection&lt;/em&gt;. Суть метода
простая: в начале есть набор из всех имеющихся признаков.
На каждом шаге мы убираем по очереди один из признаков, заново обучаем модель,
и смотрим, как изменится качество её работы на тестовой выборке. Если признак
был бесполезен и содержал шум, качество повысится. Если мы наоборот убрали полезный
признак, качество упадёт. По итогам исключается признак, без которого
модель работает лучше всего, и цикл начинается снова, уже с уменьшенным набором признаков.
Так признаки исключаются до тех пор, пока не останется ни одного. Признаки,
исключенные последними &amp;ndash; самые важные; признаки, исключенные первыми &amp;ndash; самые
бесполезные, или даже вредные.&lt;/p&gt;

&lt;p&gt;Вычислительная сложность такого метода $q \times n^2/2$ циклов обучение-валидация,
 где $n$ это количество признаков, $q$ это количество разбиений в кросс-валидации. Это довольно много, поэтому
получить разумное время вычислений удалось только при использовании нескольких GPU.&lt;/p&gt;

&lt;p&gt;Ниже представлены результаты для одного сайта (часть признаков скрыта):
&lt;figure &gt;
&lt;img width=&#34;854&#34; height=&#34;401&#34; class=&#34;lazy&#34; src=&#34;luxshop_sfs_lr.png&#34;
      data-src=&#34;luxshop_sfs.png&#34; data-srcset=&#34;luxshop_sfs@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Результат работы полного цикла Sequential feature selection.
В основном доминируют признаки, относящиеся к адресам просмотренных страниц,
на их фоне роль других признаков слабо различима.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;851&#34; height=&#34;401&#34; class=&#34;lazy&#34; src=&#34;luxshop_sfs_cut_lr.png&#34;
      data-src=&#34;luxshop_sfs_cut.png&#34; data-srcset=&#34;luxshop_sfs_cut@2x.png 2x&#34;&gt;
  &lt;figcaption&gt;&lt;p&gt;Отдельно &amp;ldquo;маловажные&amp;rdquo; признаки. Видно, как ошибка модели (logloss)
сначала падает, потом начинает расти.&lt;/p&gt;
&lt;/figcaption&gt; 
&lt;/figure&gt;

&lt;p&gt;Признаки, удаление которых привело к улучшению работы модели (т.е. вредные), показаны
красным; признаки, удаление которых привело к улучшению (полезные) показаны
зелёным; нейтральные &amp;ndash; серым.&lt;/p&gt;

&lt;p&gt;По результатам feature selection для каждого сайта
были отброшены признаки из красной зоны и часть признаков из серой.&lt;/p&gt;

&lt;h2 id=&#34;модель-для-luxshop-ml-результаты&#34;&gt;Модель для luxshop.ml, результаты&lt;/h2&gt;

&lt;p&gt;Проверка обученной модели на тестовой выборке дала AUC около 86%,
что очень неплохо для предсказаний действий посетителей, ведущих
себя в общем то случайным образом. AUC заметно вырос по сравнению
с предыдущей моделью по первому переходу на сайт (было ~75%), потому что
появилось много дополнительной информации из истории посетителя.&lt;/p&gt;

&lt;p&gt;В предыдущих моделях все используемые признаки были категорийными,
и мы измеряли только влияние наличия признака, т.е. да/нет. В новых моделях
многие признаки это числовые значения, например время с момента
последнего перехода может быть любым числом от нуля до $\approx3.2 \times 10^7$ (количество секунд в году),
поэтому поход к интерпретации влияния признаков будет другим.&lt;/p&gt;

&lt;p&gt;Для начала визуализируем признаки, обладающие наибольшим абсолютным
влиянием на модель:&lt;/p&gt;

&lt;figure &gt;
&lt;img width=&#34;641&#34; height=&#34;694&#34; class=&#34;lazy&#34; src=&#34;top_absshap_luxhop_lr.png&#34;
      data-src=&#34;top_absshap_luxhop.png&#34; data-srcset=&#34;top_absshap_luxhop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Это довольно сложная визуализация, требующая пояснений.
Цвет объекта на диаграмме соответствует числовому значению
признака. Минимальным значениям (в нашем случае это обычно ноль) соответствуют голубые цвета,
максимальным значениям &amp;ndash; красные, промежуточным значениям &amp;ndash; оттенки синего, фиолетового и пурпурного.
Диапазон от минимальных до максимальных значений индивидуален для каждого признака,
например у бинарных признаков (таких как &lt;strong&gt;regionCity=RussiaMoscow&lt;/strong&gt;) всего два значения, &lt;strong&gt;0&lt;/strong&gt; и &lt;strong&gt;1&lt;/strong&gt;,
и, соответственно,
всего два цвета, красный и голубой.&lt;/p&gt;

&lt;p&gt;Значение некоторых признаков равно ∅. Этим символом обозначается отсутствие
логического значения, например &lt;strong&gt;lastSearchEngine=∅&lt;/strong&gt;, если переход совершался не из поисковой
машины, или &lt;strong&gt;path&lt;sub&gt;2&lt;/sub&gt;=∅&lt;/strong&gt;, если в URL перехода был только первый уровень,
например &lt;code&gt;/&lt;/code&gt; или &lt;code&gt;/catalog&lt;/code&gt;. Можно интерпретировать символ ∅, как &amp;ldquo;дырка от бублика&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Каждый признак представлен набором точек: одна точка это один пример
из тестовой выборки. Точек много, поэтому они сливаются в вытянутые по горизонтали облака.
Координата точки по оси X соответствует степени влияния признака на результат.
Вдоль диаграммы проходит вертикальная нулевая ось (серая линия),
чем точка правее от этой оси, тем больше признак действует &amp;ldquo;в плюс&amp;rdquo;, чем левее,
тем больше &amp;ldquo;в минус&amp;rdquo;. Размер этого влияния есть на шкале внизу диаграммы.&lt;/p&gt;

&lt;p&gt;Чтобы облако точек для каждого признака не превратилось в горизонтальную линию,
точки случайным образом разбросаны немного вверх и вниз от горизонтальной оси,
соответствующей признаку. Чем больше точек уложено на отрезке оси, тем толще получается
в этом месте облако после &amp;ldquo;разброса&amp;rdquo;. Например, если облако толще всего в околонулевых
координатах по оси X, это означает, что для большинства примеров этот
признак оказывает околонулевое воздействие на результат.&lt;/p&gt;

&lt;p&gt;Чтобы стало понятнее, рассмотрим несколько топовых признаков по отдельности.&lt;/p&gt;

&lt;h3 id=&#34;hahahugoshortcode-0xc000cf6f00-9-hbhb&#34;&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/basket/&lt;/span&gt;&lt;/h3&gt;

&lt;figure style=&#34;margin-top: 0; margin-bottom: 0&#34;&gt;
&lt;img width=&#34;407&#34; height=&#34;79&#34; class=&#34;lazy&#34; src=&#34;single_f_0_lr.png&#34;
      data-src=&#34;single_f_0.png&#34; data-srcset=&#34;single_f_0@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Количество просмотров страниц в истории, URL которых начинался с &lt;code&gt;/basket/&lt;/code&gt;.
Нулевые значения (голубое облако слева) оказывают отрицательное воздействие,
положительные (вытянутая красно-фиолетовая линия справа) &amp;ndash; сильно положительное.
Степень положительного воздействия значительно отличается от примера к примеру
 (большая дисперсия, вытянутая линия вместо облака). Эти же значения
 можно отобразить на развёрнутой диаграмме:
&lt;figure style=&#34;margin-top: 32px; margin-bottom: 32px&#34;&gt;
&lt;img width=&#34;468&#34; height=&#34;332&#34; class=&#34;lazy&#34; src=&#34;path1_basket_luxshop_lr.png&#34;
      data-src=&#34;path1_basket_luxshop.png&#34; data-srcset=&#34;path1_basket_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Эта диаграмма тоже требует пояснений. По оси X &amp;ndash; значения нашего признака,
в логарифмической шкале (от 0 до примерно 300). По оси Y &amp;ndash; соответствующее воздействие на результат модели.
Каждая точка это один пример из тестовой выборки. Значения немного &amp;ldquo;разбросаны&amp;rdquo; по оси X
 для лучшей визуализации, в противном случае например все значения, где
X=0, выстроились бы в узкую вертикальную полоску, на которой сложно было
бы разглядеть детали. Чем ближе значение к нулю, тем больше
&amp;ldquo;разброс&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Окраска точек отличается от окраски на мини-графике. Цвет каждой точки
соответствует значению второго признака, в данном случае второй признак
это &lt;strong&gt;UTMMedium=∅&lt;/strong&gt;. Символ ∅, напомню, обозначает &amp;ldquo;пустоту&amp;rdquo;, т.е. если в URL
перехода на сайт не было метки &lt;strong&gt;UTMMedium&lt;/strong&gt;, значение признака &lt;strong&gt;UTMMedium=∅&lt;/strong&gt; равно единице,
а если метка была, значение равно нулю. Но какое отношение
&lt;strong&gt;UTMMedium&lt;/strong&gt; имеет к признаку &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/basket/&lt;/span&gt;&lt;/strong&gt;, зачем нам
вообще второй признак?&lt;/p&gt;

&lt;p&gt;Дело в том, что мы используем достаточно сложные модели, в которых большую
роль играет взаимодействие признаков (feature interaction). Другими словами,
влияние, которое оказывает на результат работы модели один признак, может зависеть
не только от значения самого этого признака, но и от значений других признаков,
&lt;em&gt;взаимодействующих&lt;/em&gt; с первым. Визуализировать такое множественное взаимодействие
непросто, поэтому применяется упрощённый подход: берётся единственный признак, который
более всего взаимодействует с основным, и его значения
используются, как палитра для раскраски точек. Соответствие цветов значениям
вторичного признака отображается на вертикальной цветной полоске справа.
В данном случае у нас бинарный признак, 0 или 1, поэтому на полоске
всего два цвета, нулю соответствует голубой, а единице &amp;ndash; красный.&lt;/p&gt;

&lt;p&gt;Итак, что же мы видим на подробной диаграмме?&lt;/p&gt;

&lt;p&gt;Если в истории посетителя не было ни одного захода на страницы, связанные
с корзиной, т.е. значение признака &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/basket/&lt;/span&gt;&lt;/strong&gt; равно нулю,
вероятность конверсии понижается примерно на 0.5 единиц. При этом, если
в URL перехода на сайт нет метки &lt;strong&gt;UTMMedium&lt;/strong&gt;, негативное влияние
нулевого значения основного признака ослабляется: мы видим, что верх левого &amp;ldquo;столбика&amp;rdquo;
на графике окрашен в красный цвет, а низ &amp;ndash; в синий. Красный соответствует
единичному значению вторичного признака, т.е. &amp;ldquo;метки нет&amp;rdquo;, а синий - наоборот,
&amp;ldquo;метка есть&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Если в истории был хотя бы один заход на страницу, относящуюся к корзине,
вероятность конверсии сразу резко возрастает (для всех значений X &amp;gt; 0 вероятность
повышается на 1-2 единицы). При этом значение вторичного признака уже не играет роли:
красные и синие точки в &amp;ldquo;ненулевой&amp;rdquo; части графика перемешаны более-менее равномерно.&lt;/p&gt;

&lt;p&gt;При повышении значения признака, положительное влияние растёт, т.е. чем
чаще посетитель заходил в корзину, тем выше вероятность конверсии;
при более чем 100 заходах положительное влияние может достигать трёх единиц.&lt;/p&gt;

&lt;p&gt;Вот такое длинное описание потребовалось, чтобы расшифровать такую
маленькую диаграмму 😌 Но мы теперь умеем интерпретировать такие диаграммы,
и следующие описания будут короче.&lt;/p&gt;

&lt;h3 id=&#34;regioncity-russiamoscow&#34;&gt;regionCity=RussiaMoscow&lt;/h3&gt;

&lt;figure style=&#34;margin-top: 0; margin-bottom: 0&#34;&gt;
&lt;img width=&#34;407&#34; height=&#34;79&#34; class=&#34;lazy&#34; src=&#34;single_f_1_lr.png&#34;
      data-src=&#34;single_f_1.png&#34; data-srcset=&#34;single_f_1@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Это бинарный признак, интерпретация очень простая: положительные значения
(посетитель из Москвы, красное облако справа) повышают конверсию, отрицательные (не из Москвы, голубое облако слева)
&amp;ndash; понижают.
&lt;figure &gt;
&lt;img width=&#34;463&#34; height=&#34;321&#34; class=&#34;lazy&#34; src=&#34;regioncity_moscow_luxshop_lr.png&#34;
      data-src=&#34;regioncity_moscow_luxshop.png&#34; data-srcset=&#34;regioncity_moscow_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Заметно любопытное взаимодействие: если посетитель перешёл из поисковика
(нулевое значение &lt;strong&gt;lastAdvEngine=∅&lt;/strong&gt;, голубая окраска точек),
влияние Москвы, как отрицательное, так и положительное, снижается.&lt;/p&gt;

&lt;h3 id=&#34;time-since-prev-sess&#34;&gt;time_since_prev_sess&lt;/h3&gt;

&lt;figure style=&#34;margin-top: 0; margin-bottom: 0&#34;&gt;
&lt;img width=&#34;407&#34; height=&#34;79&#34; class=&#34;lazy&#34; src=&#34;single_f_5_lr.png&#34;
      data-src=&#34;single_f_5.png&#34; data-srcset=&#34;single_f_5@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;По мини-графику сложно сказать что-то конкретное, кроме того, что в большинстве случаев
воздействие на конверсию околонулевое или слабо отрицательное (по положению утолщений).
&lt;figure &gt;
&lt;img width=&#34;463&#34; height=&#34;321&#34; class=&#34;lazy&#34; src=&#34;time_since_prev_sess_luxshop_lr.png&#34;
      data-src=&#34;time_since_prev_sess_luxshop.png&#34; data-srcset=&#34;time_since_prev_sess_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Виден большой разрыв в значениях, вызванный 8-часовым тайм-аутом сессии.
Если переход происходит в рамках первой сессии, время от предыдущей сессии
принимается равным нулю, в противном случае время от конца предыдущей
сессии будет как минимум 8 часов.&lt;/p&gt;

&lt;p&gt;Переход в первой сессии несколько снижает вероятность конверсии.
Для переходов, совершённых в следующих сессиях, зависимость очень любопытна:
есть &amp;ldquo;хорошее&amp;rdquo; время, примерно в течение недели после сессии, и &amp;ldquo;плохое&amp;rdquo;,
от недели до двух месяцев. По видимому, в течение первой недели посетитель
ещё &amp;ldquo;горячий&amp;rdquo;, с твёрдым намерением сделать покупку, а потом &amp;ldquo;остывает&amp;rdquo;
и заходит на сайт просто посмотреть на товары, не собираясь их
приобретать, или просто помимо своей воли перекидывается на сайт рекламными
системами.&lt;/p&gt;

&lt;p&gt;Прослеживается также интересное взаимодействие со вторым признаком.&lt;/p&gt;

&lt;h3 id=&#34;hahahugoshortcode-0xc000cf6f00-18-hbhb&#34;&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=∅&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Это значение признака интерпретируется, как
количество раз, когда посетитель заходил на &amp;ldquo;топовые&amp;rdquo; страницы
сайта, не опускаясь вглубь, на второй уровень.
&lt;figure style=&#34;margin-top: 0; margin-bottom: 0&#34;&gt;
&lt;img width=&#34;407&#34; height=&#34;79&#34; class=&#34;lazy&#34; src=&#34;single_f_7_lr.png&#34;
      data-src=&#34;single_f_7.png&#34; data-srcset=&#34;single_f_7@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;По мини-графику можно предположить, что большие значения признака имеют
в среднем нулевой эффект (красные точки сконцентрированы около нуля),
а околонулевые и близкие к ним значения могут проявлять
как положительное, так и отрицательное воздействие на конверсию. Также
в большинстве случаев воздействие признака на конверсию равно нулю или
 чуть больше нуля (есть утолщение в районе нуля).
&lt;figure &gt;
&lt;img width=&#34;468&#34; height=&#34;332&#34; class=&#34;lazy&#34; src=&#34;sum_path2_none_luxshop_lr.png&#34;
      data-src=&#34;sum_path2_none_luxshop.png&#34; data-srcset=&#34;sum_path2_none_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Развёрнутая диаграмма подтверждает наше предположение:
Нулевое значение отрицательно действует на конверсию, единичное -
максимально действует в &amp;ldquo;плюс&amp;rdquo;, и по мере увеличения значений признака
воздействие уменьшается и становится снова отрицательным.&lt;/p&gt;

&lt;p&gt;Также любопытно взаимодействие со вторым признаком: messenger, как UTM
источник, сильно увеличивает негативное воздействие для нулевого значения
основного признака.&lt;/p&gt;

&lt;p&gt;Дальше будем рассматривать только развёрнутые диаграммы, т.к. дано
уже достаточно примеров интерпретации мини-графиков.&lt;/p&gt;

&lt;h3 id=&#34;hahahugoshortcode-0xc000cf6f00-21-hbhb&#34;&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$pageview_duration&lt;/span&gt;&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;468&#34; height=&#34;329&#34; class=&#34;lazy&#34; src=&#34;sum_pageview_duration_luxshop_lr.png&#34;
      data-src=&#34;sum_pageview_duration_luxshop.png&#34; data-srcset=&#34;sum_pageview_duration_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Хороший посетитель просматривал сайт достаточное время, но не слишком долго,
оптимальное время от 10 минут до нескольких часов. Посетители, которые разглядывали
сайт магазина более 6 часов, видимо используют его, просто как источник
для вдохновения.&lt;/p&gt;

&lt;h3 id=&#34;hahahugoshortcode-0xc000cf6f00-23-hbhb&#34;&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/delivery/&lt;/span&gt;&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;464&#34; height=&#34;332&#34; class=&#34;lazy&#34; src=&#34;sum_path1_delivery_luxshop_lr.png&#34;
      data-src=&#34;sum_path1_delivery_luxshop.png&#34; data-srcset=&#34;sum_path1_delivery_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Идеальный посетитель интересуется доставкой дважды. Лучше, если это происходит
в первой или второй сессии. Для посетителей, интересовавшихся доставкой
более 7 раз, есть подозрение на проблемы с памятью: возможно, делать
покупки они тоже забывают.&lt;/p&gt;

&lt;h3 id=&#34;landing-num&#34;&gt;landing_num&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;471&#34; height=&#34;323&#34; class=&#34;lazy&#34; src=&#34;landing_num_luxshop_lr.png&#34;
      data-src=&#34;landing_num_luxshop.png&#34; data-srcset=&#34;landing_num_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Принято считать, что чем чаще посетитель заходит в магазин, тем
выше его лояльность, и тем лучший он покупатель. Но эта диаграмма
показывает прямо обратное. Забегая вперёд, скажу, что похожая
зависимость наблюдается и на других сайтах, т.е. это не случайное
статистическое отклонение и не особенность этой модели. Причины
такой зависимости будут более подробно разобраны позже, на примере сайта
shop.ml.&lt;/p&gt;

&lt;h3 id=&#34;time-since-sess-start&#34;&gt;time_since_sess_start&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;479&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;time_since_sess_start_luxshop_lr.png&#34;
      data-src=&#34;time_since_sess_start_luxshop.png&#34; data-srcset=&#34;time_since_sess_start_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Модель показывает, что посетителю в идеале нужен примерно час от начала сессии,
чтобы дозреть до решения о покупке.&lt;/p&gt;

&lt;h3 id=&#34;user-age&#34;&gt;user_age&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;463&#34; height=&#34;321&#34; class=&#34;lazy&#34; src=&#34;user_age_luxshop_lr.png&#34;
      data-src=&#34;user_age_luxshop.png&#34; data-srcset=&#34;user_age_luxshop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Как и в случае с &lt;strong&gt;landing_num&lt;/strong&gt;, здесь не наблюдается однозначного повышения
&amp;ldquo;качества&amp;rdquo; посетителей с увеличением времени их знакомства с магазином.
Наоборот вероятность конверсии с возрастом падает, и только у &amp;ldquo;старой гвардии&amp;rdquo;
возрастом более 120 дней опять начинает повышается.
$$\DeclareMathOperator{\E}{E}$$&lt;/p&gt;

&lt;h2 id=&#34;ранжирование-по-среднему-воздействию-признака&#34;&gt;Ранжирование по среднему воздействию признака&lt;/h2&gt;

&lt;p&gt;До этого момента мы ранжировали признаки по силе их абсолютного воздействия,
то есть по $\sum_n|S_i|$, где $S_i$ это SHAP value i-го признака, $n$ &amp;ndash;
количество примеров в тестовой выборке. Но можно отранжировать признаки
и по матожиданию $\E[S_i]$, как мы делали это в предыдущей статье,
и построить соответствующую диаграмму. Тогда в топ выйдут признаки, которые
в среднем влияют максимально положительно или максимально отрицательно:
&lt;figure &gt;
&lt;img width=&#34;520&#34; height=&#34;737&#34; class=&#34;lazy&#34; src=&#34;luxshop_top_pos_neg_lr.png&#34;
      data-src=&#34;luxshop_top_pos_neg.png&#34; data-srcset=&#34;luxshop_top_pos_neg@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Всю верхушку положительного топа оккупировал признак &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/basket/&lt;/span&gt;&lt;/strong&gt;, это
соответствует тому, что мы уже видели на диаграммах. Этот же признак, но
с нулевым значением &amp;ndash; второй по негативному воздействию. Также в топе
неожиданно засветился признак &lt;strong&gt;path&lt;sub&gt;1&lt;/sub&gt;=/404&lt;/strong&gt;, скорее всего
соответствующий адресу страницы с ошибкой &lt;code&gt;404 Page not found&lt;/code&gt;.
Можно предположить, что такая ошибка выдавалась посетителям, у которых было
твёрдое намерение приобрести определённый товар, исчезнувший из ассортимента
магазина, и они всё равно делали заказ, приобретая замену.&lt;/p&gt;

&lt;h2 id=&#34;user-scorecards&#34;&gt;User scorecards&lt;/h2&gt;

&lt;p&gt;Из рассмотренных примеров видно, что интерпретация воздействия признаков
в продвинутой модели это не такая простая задача. Все диаграммы, которые мы построили,
не охватывают думаю и нескольких процентов возможных кейсов по воздействию
на конверсию и особенно по взаимодействию признаков друг с другом.&lt;/p&gt;

&lt;p&gt;Поэтому для продвинутых моделей особенно хороши &lt;em&gt;user scorecards&lt;/em&gt;, которые
позволяют без построения гипотез и абстрактных рассуждений просто взять и увидеть,
как конкретные значения признаков влияют на вероятность конверсии конкретного посетителя сайта.
&lt;figure &gt;
&lt;img width=&#34;612&#34; height=&#34;734&#34; class=&#34;lazy&#34; src=&#34;luxshop_scorecards_lr.png&#34;
      data-src=&#34;luxshop_scorecards.png&#34; data-srcset=&#34;luxshop_scorecards@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Напомню, черная горизонтальная черта это то, насколько вероятность
конверсии данного посетителя отличается от средней по сайту (средней конверсии
соответствует горизонтальная пунктирная линия, идущая из нулевой точки); красные столбцы
над этой чертой &amp;ndash; положительно влияющие признаки, размер каждого
столбца равен силе влияния признака; синие столбцы под чертой &amp;ndash; признаки
с отрицательным влиянием.&lt;/p&gt;

&lt;p&gt;Видно, что на конверсию каждого посетителя влияет большое количество признаков,
их даже не получилось все детализировать на диаграмме
(розовая и светло-голубая &amp;ldquo;шапки&amp;rdquo; сверху и снизу у каждого посетителя)&lt;/p&gt;

&lt;h1 id=&#34;результаты-для-shop-ml&#34;&gt;Результаты для shop.ml&lt;/h1&gt;

&lt;p&gt;Пр проверке модели на тестовой выборке получен AUC 84%. Посмотрим,
что есть интересного в признаках:
&lt;figure &gt;
&lt;img width=&#34;638&#34; height=&#34;687&#34; class=&#34;lazy&#34; src=&#34;top_absshap_shopml_lr.png&#34;
      data-src=&#34;top_absshap_shopml.png&#34; data-srcset=&#34;top_absshap_shopml@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Прежде всего, отметим сильное влияние признака &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/cart/&lt;/span&gt;&lt;/strong&gt;.
&lt;figure &gt;
&lt;img width=&#34;460&#34; height=&#34;331&#34; class=&#34;lazy&#34; src=&#34;sum_path1_shop_lr.png&#34;
      data-src=&#34;sum_path1_shop.png&#34; data-srcset=&#34;sum_path1_shop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Уже единственный заход на страницу, связанную с корзиной, поднимает
вероятность конверсии на полпункта, а три захода &amp;ndash; на целый пункт (напомню,
один пункт это изменение вероятности конверсии примерно в два раза).
Ни один другой признак не влияет так сильно.&lt;/p&gt;

&lt;p&gt;Cильное влияние и у признака  &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=/cart/done&lt;/span&gt;&lt;/strong&gt;,
также в топе есть &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=/cart/laststep&lt;/span&gt;&lt;/strong&gt; и &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/delivery/&lt;/span&gt;&lt;/strong&gt;. Похоже, что работа
с корзиной и доставкой является критически важным фактором в оценке посетителя.&lt;/p&gt;

&lt;p&gt;Теперь посмотрим на признаки с выраженным отрицательным влиянием. Здесь нас
ждёт сюрприз.
&lt;figure &gt;
&lt;img width=&#34;471&#34; height=&#34;323&#34; class=&#34;lazy&#34; src=&#34;landing_num_shop_lr.png&#34;
      data-src=&#34;landing_num_shop.png&#34; data-srcset=&#34;landing_num_shop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Признак с самым явно выраженным негативным влиянием это &lt;strong&gt;landing_num&lt;/strong&gt;, порядковый номер захода на сайт.
Про росте значения признака конверсионность посетителя резко падает.
Казалось бы, должно быть наоборот: чем чаще человек пользуется сайтом,
тем более это лояльный покупатель?
&lt;figure &gt;
&lt;img width=&#34;476&#34; height=&#34;331&#34; class=&#34;lazy&#34; src=&#34;sum_utmc_shop_lr.png&#34;
      data-src=&#34;sum_utmc_shop.png&#34; data-srcset=&#34;sum_utmc_shop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Схожим образом работает признак &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$UTMCampaign=∅&lt;/span&gt;&lt;/strong&gt; и близкие к нему
по смыслу признаки &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$referer=∅&lt;/span&gt;&lt;/strong&gt; и &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$lastSocialNetwork=∅&lt;/span&gt;&lt;/strong&gt;.
&lt;figure &gt;
&lt;img width=&#34;479&#34; height=&#34;323&#34; class=&#34;lazy&#34; src=&#34;user_age_shop_lr.png&#34;
      data-src=&#34;user_age_shop.png&#34; data-srcset=&#34;user_age_shop@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;При увеличении &amp;ldquo;возраста&amp;rdquo; посетителя его конверсионность тоже падает.
Что же происходит с посетителями, у них постепенно вырабатывается
отвращение к покупкам?&lt;/p&gt;

&lt;p&gt;На самом деле нет. В данных сайта shop.ml заметно, что часть трафика
приходит из довольно сомнительных источников. Представим источник,
перекидывающий людей на сайт через popunder, clickunder или другим не слишком
цивилизованным способом. У посетителей из таких источников будет
много переходов, но так как они не собираются ничего покупать, они не
будут работать с корзиной. Таких посетителей много: из всех
посетителей, совершивших более 10 переходов на сайт shop.ml, меньше
трети когда либо пользовались корзиной.&lt;/p&gt;

&lt;p&gt;Но если посетитель совершает переходы и не работает с корзиной, значит
это мусорный трафик, и модели надо научиться его распознавать? Да, и именно
поэтому у признаков &lt;strong&gt;landing_num&lt;/strong&gt;, &lt;strong&gt;user_age&lt;/strong&gt; и признаков, связанными с пустыми
&lt;em&gt;UTMCampaign / referer / socialNetwork&lt;/em&gt;,
наблюдается негативное влияние. Как только посетитель воспользуется корзиной,
весь негатив от этих признаков сразу будет скомпенсирован
сильным положительным влиянием признака &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/cart/&lt;/span&gt;&lt;/strong&gt;.
Ну а пока посетитель набирает переходы на сайт и не использует корзину,
априори считаем его низкокачественным трафиком &amp;ndash; вполне разумно.&lt;/p&gt;

&lt;p&gt;В подтверждение этой версии, на диаграммах заметно взаимодействие
признака &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;1&lt;/sub&gt;=/cart/&lt;/span&gt;&lt;/strong&gt; и генерирующих негатив
признаков &lt;strong&gt;landing_num&lt;/strong&gt; и  &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$UTMCampaign=∅&lt;/span&gt;&lt;/strong&gt;: работа с корзиной
снижает степень негатива.&lt;/p&gt;

&lt;p&gt;Взаимную компенсацию признаков видно и на scorecards:
&lt;figure &gt;
&lt;img width=&#34;612&#34; height=&#34;734&#34; class=&#34;lazy&#34; src=&#34;shopml_scorecards_lr.png&#34;
      data-src=&#34;shopml_scorecards.png&#34; data-srcset=&#34;shopml_scorecards@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;У посетителя &lt;strong&gt;A&lt;/strong&gt; присутствует весь набор негативных признаков, тем
не менее у него повышенная более чем на 2.5 пункта вероятность конверсии,
потому что есть работа с корзиной. У посетителя &lt;strong&gt;B&lt;/strong&gt; негативные признаки
выражены не так явно, но прогноз конверсии в целом негативен, т.к. заходов в корзину не было.&lt;/p&gt;

&lt;p&gt;Итак, фактически у нас получилась модель оценки качества трафика,
использующая конверсионность, как метрику качества. Неплохо, не правда ли?&lt;/p&gt;

&lt;h1 id=&#34;результаты-для-courses-ml&#34;&gt;Результаты для courses.ml&lt;/h1&gt;

&lt;p&gt;Для этого сайта не существует цели &amp;ldquo;заказ&amp;rdquo;, поэтому сделаем синтетическую
цель, соответствующую бизнес-задачам сайта. Сайт существует, чтобы обучать,
поэтому конверсию будем засчитывать, если посетитель продержался
на сайте ещё как минимум полчаса после перехода. Будем считать, что в эти
полчаса он обучался, а не просто рассматривал картинки.&lt;/p&gt;

&lt;p&gt;Суммарное влияние признаков:
&lt;figure &gt;
&lt;img width=&#34;637&#34; height=&#34;578&#34; class=&#34;lazy&#34; src=&#34;top_absshap_courses_lr.png&#34;
      data-src=&#34;top_absshap_courses.png&#34; data-srcset=&#34;top_absshap_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;h3 id=&#34;hahahugoshortcode-0xc000cf6f00-48-hbhb&#34;&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$pageview_duration&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;Более всего на результат влияет суммарное время просмотра страниц, т.е.
модель предполагает, что если посетитель уже потратил заметное количество времени на обучение,
он будет продолжать обучаться дальше.
&lt;figure &gt;
&lt;img width=&#34;468&#34; height=&#34;328&#34; class=&#34;lazy&#34; src=&#34;sum_pageview_duration_courses_lr.png&#34;
      data-src=&#34;sum_pageview_duration_courses.png&#34; data-srcset=&#34;sum_pageview_duration_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;На детальной диаграмме зависимость выражена очень явно.&lt;/p&gt;

&lt;h3 id=&#34;time-since-prev-sess-1&#34;&gt;time_since_prev_sess&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;480&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;time_since_prev_sess_courses_lr.png&#34;
      data-src=&#34;time_since_prev_sess_courses.png&#34; data-srcset=&#34;time_since_prev_sess_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Прослеживается интересная форма зависимости, похожая на синусоиду. Рассмотрим
этот участок поближе, и в линейной шкале по оси X:
&lt;figure &gt;
&lt;img width=&#34;480&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;time_since_prev_sess_courses_cut_lr.png&#34;
      data-src=&#34;time_since_prev_sess_courses_cut.png&#34; data-srcset=&#34;time_since_prev_sess_courses_cut@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Действительно синусоида! Наша модель самостоятельно вывела такую сложную
зависимость из входных данных, которые в общем то не предполагали
никакой периодичности. Но какова причина появления этой синусоиды?&lt;/p&gt;

&lt;p&gt;Люди серьезно занимающиеся самообучением, обычно проходят курс по вечерам после работы
или после домашних дел. Или по утрам: конкретное время неважно, главное, что
обучение происходит примерно в одно и то же время каждый день.
В течение для есть довольно узкий интервал времени, в течение которого человек может сесть за обучение
и по настоящему в него погрузиться. Если начать слишком рано, скорее всего
найдутся другие занятия: работа, дом, дети, и т.п. Если начать слишком поздно,
то просто пора будет спать. То есть, если с момента завершения предыдущей
сессии обучения прошло менее 12 часов или более 22-24 часов, то новая сессия
скорее всего получится неудачной или вовсе не начнётся. Примерно это мы и видим на диаграмме.&lt;/p&gt;

&lt;p&gt;Синусоида с периодом в сутки образуется, потому что не принципиально, когда закончилась предыдущая
сессия: вчера, позавчера, или три дня назад. Всё равно начало следующей сессии
наиболее вероятно только в определённом интервале времени.&lt;/p&gt;

&lt;p&gt;Почему максимум вероятности приходится не ровно на 24 часа с момента предыдущей сессии?
Потому что время отсчитывается от конца предыдущей сессии, а не от начала. Сессии
имеют достаточно большую длительность, она отображена справа на шкале взаимодействующего
признака &lt;strong&gt;avg_sess_len&lt;/strong&gt;. Размерность шкалы &amp;ndash; секунды. Для понимания
масштаба: 5 часов это 18000 секунд. Там же видно, что наибольший &amp;ldquo;размах&amp;rdquo;
синусоиды наблюдается у посетителей с длинными сессиями. Если сеансы обучения
короткие, то обучаться можно в принципе в любое время дня, и роль суточной
периодичности снижается.&lt;/p&gt;

&lt;h3 id=&#34;sess-num&#34;&gt;sess_num&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;494&#34; height=&#34;322&#34; class=&#34;lazy&#34; src=&#34;sess_num_courses_lr.png&#34;
      data-src=&#34;sess_num_courses.png&#34; data-srcset=&#34;sess_num_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Онлайн обучение это занятие для достаточно терпеливых и упорных людей.
Терпения и упорства не всем хватает надолго. После четвёртой сессии
видно отрицательное влияние растущего номера сессии: известно, что онлайн
курсы проходит до конца только небольшой процент начавших обучение.
Кроме того, для усидчивых посетителей будет наблюдаться компенсационный
эффект от растущего &lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$pageview_duration&lt;/span&gt;&lt;/strong&gt; (он виден и по взаимодействию признаков), а большие
значения &lt;strong&gt;sess_num&lt;/strong&gt; могут использоваться моделью для определения случайных
переходов на сайт &amp;ndash; аналогично механизму, описанному для сайта shop.ml.&lt;/p&gt;

&lt;h3 id=&#34;time-since-sess-start-1&#34;&gt;time_since_sess_start&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;463&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;time_since_sess_start_courses_lr.png&#34;
      data-src=&#34;time_since_sess_start_courses.png&#34; data-srcset=&#34;time_since_sess_start_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Для переходов, которые не совпали с началом сессии, модель вероятно рассуждает
так: если посетитель просидел на сайте больше 10 минут, то скорее всего
просидит и ещё полчаса. А вот если время с начала сессии приближается к 6 часам,
то пора бы уже и закругляться!&lt;/p&gt;

&lt;h3 id=&#34;time-since-prev-landing&#34;&gt;time_since_prev_landing&lt;/h3&gt;

&lt;figure &gt;
&lt;img width=&#34;480&#34; height=&#34;320&#34; class=&#34;lazy&#34; src=&#34;time_since_prev_landing_courses_lr.png&#34;
      data-src=&#34;time_since_prev_landing_courses.png&#34; data-srcset=&#34;time_since_prev_landing_courses@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Синусоида, аналогичная &lt;strong&gt;time_since_prev_sess&lt;/strong&gt;, но максимумы приходятся
уже ровно на 24 часа, т.к. нет поправки на продолжительность сессии.&lt;/p&gt;

&lt;h2 id=&#34;дополнительные-визуализации&#34;&gt;Дополнительные визуализации&lt;/h2&gt;

&lt;figure &gt;
&lt;img width=&#34;507&#34; height=&#34;659&#34; class=&#34;lazy&#34; src=&#34;courses_top_pos_neg_lr.png&#34;
      data-src=&#34;courses_top_pos_neg.png&#34; data-srcset=&#34;courses_top_pos_neg@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;

&lt;p&gt;Эта визуализация представляет собой взгляд с альтернативной точки зрения, выявляющий
несколько другие признаки. На что можно обратить внимание:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;path&lt;sub&gt;1&lt;/sub&gt;=/unsubscribe/&lt;/strong&gt; резко понижает вероятность продолжения сессии. Неудивительно.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$path&lt;sub&gt;2&lt;/sub&gt;=/lesson/J&lt;/span&gt;&lt;/strong&gt; &amp;ndash; по видимому очень неудачный курс:
единственного захода на эту страницу достаточно, чтобы посетитель перестал надолго задерживаться на сайте.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$linkUrl=&lt;a href=&#34;https://vk.com/courses&#34; target=&#34;_blank&#34;&gt;https://vk.com/courses&lt;/a&gt;&lt;/span&gt; &amp;gt; 8&lt;/strong&gt; &amp;ndash; вероятно, на эту
ссылку кликают посетители, которые не удовлетворены имеющимися на сайте курсами.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span style=&#34;white-space: nowrap;&#34;&gt;$\sum$lastTrafficSource=saved &amp;gt; 2&lt;/span&gt;&lt;/strong&gt; &amp;ndash; посетители, которые сохранили
страничку сайта себе на компьютер, и переходят с неё на сайт,
явно заинтересованы в продолжительном обучении.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;И, в дополнение, образцы scorecards для пары &amp;ldquo;хороших&amp;rdquo; и пары &amp;ldquo;плохих&amp;rdquo;
посетителей:
&lt;figure &gt;
&lt;img width=&#34;604&#34; height=&#34;625&#34; class=&#34;lazy&#34; src=&#34;courses_scorecards_lr.png&#34;
      data-src=&#34;courses_scorecards.png&#34; data-srcset=&#34;courses_scorecards@2x.png 2x&#34;&gt;
  
&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;заключение&#34;&gt;Заключение&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;We have so much data on the web, almost all of it available for free,
that we dive into the the data ocean hoping that magically awesome
things will follow. They never do.&lt;/p&gt;

&lt;p&gt;Avinash Kaushik.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Основная проблема интернет-аналитики, на мой взгляд, в том, что в ней физически огромное количество
данных, но эти данные приносят удивительно мало пользы своим обладателям.&lt;/p&gt;

&lt;p&gt;Почему? Потому что это не самые удобные для обработки данные, у них сложная структура,
это sparse categorical multivariate time series &amp;ndash; подобные типы данных мейнстримовая
Machine Learning (а сейчас и AI) индустрия традиционно обходит стороной.
Ведь гораздо эффективнее впечатлять инвесторов распознаванием
изображений, генерацией музыки или чат-ботами, чем копанием в каких то там логах сессий 🙊&lt;/p&gt;

&lt;p&gt;Отчасти поэтому традиционные инструменты интернет аналитики
застряли на примитивном уровне и не способны извлечь из данных даже
малую часть содержащейся в них полезной информации. За время работы с
моделями, описанными в этой статье, я, честно говоря,
узнал о поведении посетителей сайтов намного больше, чем за пару лет работы
над &lt;a href=&#34;https://metrika.yandex.ru/promo/webvisor/&#34; target=&#34;_blank&#34;&gt;WebVisor&lt;/a&gt;
и несколько лет работы в &lt;a href=&#34;https://metrika.yandex.ru&#34; target=&#34;_blank&#34;&gt;Яндекс.Метрике&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Хочется верить, что разумное применение технологий машинного обучения и AI
поможет наконец раскрыть потенциал океана данных интернет-аналитики,
и что &amp;ldquo;awesome things&amp;rdquo; действительно &amp;ldquo;will follow&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science III. Как отличить хорошее от плохого?</title>
      <link>https://suilin.ru/post/conversion_factors/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/conversion_factors/</guid>
      <description>

&lt;p&gt;$$\DeclareMathOperator{\E}{E}$$
Как понять, что влияет на конверсию положительно, а что отрицательно?
В любом отчёте Google Analytics или Я.Метрики есть столбец
&amp;ldquo;конверсия&amp;rdquo;, и сравнив конверсию в разных строках отчёта, казалось бы,
можно получить исчерпывающий ответ.&lt;/p&gt;

&lt;p&gt;Но не надо забывать, что любой отчёт показывает срез данных, сделанный
по &lt;em&gt;единственному&lt;/em&gt; измерению (источник трафика, география, и т.п.), что
даёт неполную, а иногда даже неверную картину. Рассмотрим простой пример:
есть два города, Москва и Екатеринбург и два вида трафика,
органический и реферальный. Данные по кол-ву визитов и конверсий сведены
в кросс-таблицу в виде &lt;code&gt;конверсии/визиты = конверсионность&lt;/code&gt;:&lt;/p&gt;

&lt;style&gt;th {text-align: center;}&lt;/style&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Referral&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Organic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Всего (город)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Москва&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$1/100=1.00\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$15/1000=1.50\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$16/1100=\mathbf{1.45}\%$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Екатеринбург&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$12/1000=1.20\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$2/100=2.00\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$14/1100=\mathbf{1.27}\%$&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Всего (источник)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$13/1100=1.18\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;$17/1100=1.54\%$&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;В отчёте по географии будет конверсионность 1.45% для Москвы и
 1.27% для Екатеринбурга (последняя колонка), т.е. трафик из Москвы кажется более
 конверсионным. Но если присмотреться внимательнее, видно, что
 конверсионность трафика из Москвы на самом деле &lt;strong&gt;меньше&lt;/strong&gt; для
 обоих видов трафика, 1.0% против 1.2% в реферальном трафике и 1.5% против 2.0%
 в органике!&lt;/p&gt;

&lt;p&gt;Причина кажущейся высокой конверсионности Москвы в том, что в ней больше
 больше доля органического трафика, который по своей природе более конверсионный.
То есть, чтобы сделать полноценный вывод о том, что влияет на конверсионность,
анализировать единственное измерение недостаточно.&lt;/p&gt;

&lt;p&gt;Теоретически можно построить отчёт одновременно по двум измерениям,
 но во-первых надо ещё догадаться, какие измерения пересекать и на какие цифры обращать внимание, во-вторых
 в реальной жизни измерений много, десятки, и все они &lt;em&gt;одновременно&lt;/em&gt; взаимодействуют
 друг с другом. На конверсионность влияет одновременно и география, и источник трафика, и
 время суток, и то, с какого устройства
 заходят на сайт, и многое другое. Построить отчёт сразу по десятку измерений и понять,
 от чего на самом деле зависит конверсия, для обычного человека нереально: слишком много данных.&lt;/p&gt;

&lt;p&gt;Нужны более совершенные автоматические способы оценки воздействия факторов,
 учитывающие совместный вклад всех измерений. Результатом может быть
 сводный отчёт/инфографика, присваивающие рейтинг каждому фактору,
 например Екатеринбург даёт +0.3 к конверсии, а реферальное происхождение
 трафика даёт -0.5.&lt;/p&gt;

&lt;h2 id=&#34;линейная-модель&#34;&gt;Линейная модель&lt;/h2&gt;

&lt;p&gt;Самый простой и часто используемый способ решения задач, похожих на нашу -
использование линейной модели. Дадим каждому фактору
вес $w_i$, являющийся оценкой влияния фактора.
Если наличие соответствующего фактора увеличивает вероятность конверсии, то вес положителен:
чем он больше, тем больше увеличивается вероятность. Отрицательные веса
наоборот уменьшают вероятность конверсии.&lt;/p&gt;

&lt;p&gt;Сумма всех факторов каждого $j$-го посетителя, умноженных на их веса, даст
значение $z_j$, называемое также &lt;em&gt;logits&lt;/em&gt;. Факторы в нашем случае задаются
индикаторными переменными, принимающими значения 1 (фактор присутствует) и 0 (фактор отсутствует). Для приведённого
выше примера было бы четыре фактора &lt;code&gt;город=Москва&lt;/code&gt;, &lt;code&gt;город=Екатеринбург&lt;/code&gt;,
                                    &lt;code&gt;источник=Organic&lt;/code&gt;, &lt;code&gt;источник=Referral&lt;/code&gt;
и четыре соответствующих переменных $x_1,\dots,x_4, \; x \in \{0,1\}$ , плюс одна свободная переменная $b$,
или &lt;em&gt;bias&lt;/em&gt;, которая будет примерно соответствовать средней конверсии сайта:
$$z_j = w_1x_{j,1} + w_2x_{j,2} + w_3x_{j,3} + w_4x_{j,4} + b$$
или в обобщённой векторной записи:
$$z_j=\mathbf{w}^\top\mathbf{x_j} + b$$&lt;/p&gt;

&lt;p&gt;Переменные, формирующие вектор $\mathbf{x_j}$, характеризующий каждого посетителя сайта,
в машинном обучении называются &lt;em&gt;признаками&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;В зависимости от того, что мы хотим в итоге вычислить,
выбирается link-функция, преобразующая logits в целевую переменную ($y$).
В нашем случае примем за целевую переменную факт наличия конверсии, т.е.
$y=1$, если конверсия случилась, и $y=0$, если не случилась.
В этом случае link-функцией будет &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%BE%D0%B8%D0%B4%D0%B0&#34; target=&#34;_blank&#34;&gt;сигмоид&lt;/a&gt;:
$$\sigma(z) = \frac{1}{1+e^{-z}}$$
Модель будет выдавать вероятность конверсии для заданного набора факторов ($\mathbf{x}$).
Такой тип модели называется &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F&#34; target=&#34;_blank&#34;&gt;логистическая регрессия&lt;/a&gt;:
$$\Pr(y=1|\mathbf{x})=\sigma(\mathbf{w}^{\top}\mathbf{x} + b)$$&lt;/p&gt;

&lt;p&gt;Далее используется стандартная процедура машинного обучения, подбирающая
такие веса для факторов, которые максимизируют правдоподобие модели.
На деталях обучения не будем останавливаться, интереснее другое &amp;ndash; что получится на реальных сайтах?&lt;/p&gt;

&lt;h2 id=&#34;сайты&#34;&gt;Сайты&lt;/h2&gt;

&lt;p&gt;Эту и все последующие модели будем тестировать на данных реальных сайтов,
полученных через Logs API Я.Метрики.
Будем использовать три сайта, дадим им условные названия:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;shop.ml&lt;/strong&gt; &amp;ndash; e-commerce сайт, продающий недорогие товары, в основном хозяйственно-бытового назначения&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;luxshop.ml&lt;/strong&gt; &amp;ndash; тоже e-commerce, с достаточно дорогими товарами, не являющимися предметами первой необходимости&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;courses.ml&lt;/strong&gt; &amp;ndash; сайт с онлайн курсами.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Настоящих адресов раскрывать не будем, т.к. в результатах работы моделей
находятся достаточно чувствительные данные. Адреса страниц сайтов и имена рекламных кампаний тоже
изменены, чтобы сайт было невозможно идентифицировать.&lt;/p&gt;

&lt;p&gt;Для каждого сайта используется выборка данных за один год, в которую
входит от 1 до 10 млн. уникальных посетителей. Число реальных посетителей
в 2-4 раза меньше, из за того, что посетители часто теряют cookies
(чистят, переустанавливают windows, меняют смартфоны, и т.п.)&lt;/p&gt;

&lt;h2 id=&#34;оценка-качества-моделей&#34;&gt;Оценка качества моделей&lt;/h2&gt;

&lt;p&gt;Чтобы понять, насколько модель соотносится с реальной жизнью, будем
оценивать её качество по результатам прогноза на тестовой выборке, т.е. на данных,
которые модель не видела в процессе обучения и настройки параметров. Есть два варианта формирования
тестовой выборки:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Равномерный сэмпл, например случайным образом отобранные 10% от всех данных. Это
традиционный способ, принятый в machine learning.&lt;/li&gt;
&lt;li&gt;Данные делятся на две части, &amp;ldquo;прошлое&amp;rdquo; и &amp;ldquo;будущее&amp;rdquo;, за будущее принимаются
данные например за последний месяц, за прошлое - всё остальное. &amp;ldquo;Будущее&amp;rdquo;
это тестовая выборка, &amp;ldquo;прошлое&amp;rdquo; - обучающая выборка.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Последний способ
ближе соответствует тому, как модели применяются в реальной жизни, поэтому
мы будем использовать в основном его. Прогноз из &amp;ldquo;прошлого&amp;rdquo; в &amp;ldquo;будущее&amp;rdquo; является
более сложным, т.к. здесь проявляется нестационарность: например
рекламные кампании, которые давали хорошую конверсионность в прошлом,
совсем не обязательно будут так же хорошо работать в будущем.&lt;/p&gt;

&lt;p&gt;Для единообразия, и чтобы модели
были сравнимы друг с другом, будем всегда использовать метрику качества
&lt;a href=&#34;https://ru.wikipedia.org/wiki/ROC-%D0%BA%D1%80%D0%B8%D0%B2%D0%B0%D1%8F#%D0%9F%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C_%D0%BF%D0%BE%D0%B4_%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9&#34; target=&#34;_blank&#34;&gt;AUC&lt;/a&gt; (Area Under Curve).&lt;/p&gt;

&lt;h2 id=&#34;структура-модели&#34;&gt;Структура модели&lt;/h2&gt;

&lt;p&gt;Вернёмся к нашей модели. Будем прогнозировать конверсии для посетителей,
впервые оказавшихся на сайте, т.е. не имеющих никакой предыдущей истории.
Конверсия должна случиться в рамках первой сессии, т.е. отложенные конверсии
не учитываются. Модель работает со следующими признаками:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;browser&lt;/strong&gt; &amp;ndash; браузер посетителя&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;device&lt;/strong&gt; &amp;ndash; тип устройства, desktop/smartphone/tablet/tv,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;advEngine&lt;/strong&gt; &amp;ndash; рекламная система, из которой произошел переход&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;searchEngine&lt;/strong&gt; &amp;ndash; поисковик, из которого произошел переход&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;socialNetwork&lt;/strong&gt;  &amp;ndash; соцсеть, из которой произошёл переход&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;referer&lt;/strong&gt; &amp;ndash; сайт, с которого произошел переход&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trafficSource&lt;/strong&gt; &amp;ndash; тип перехода: прямой, organic, реферальный и т.п.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mobilePhone&lt;/strong&gt; &amp;ndash; бренд смартфона/планшета&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;networkType&lt;/strong&gt; &amp;ndash; тип сетевого подключения, если известен - ethernet/wifi/etc&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt; &amp;ndash; операционная система посетителя&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;city&lt;/strong&gt; &amp;ndash; город посетителя&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;country&lt;/strong&gt; &amp;ndash; страна посетителя&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;urlDomain&lt;/strong&gt; &amp;ndash; домен из адреса landing page. Смысл в том, что у сайта
может быть несколько доменов, некоторые используются для внутренних нужд,
например тестирования перед выкаткой релиза (test.shop.ml, dev.shop.ml),
и модели надо уметь различать эти домены.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;landing_l1&lt;/strong&gt; &amp;ndash; первый компонент пути из landing page. Например, для адреса
&lt;code&gt;http://shop.ml/catalog/electronics/audio/1&lt;/code&gt; это будет &lt;code&gt;/catalog/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;landing_l2&lt;/strong&gt; &amp;ndash; второй компонент пути из landing page. Для адреса
&lt;code&gt;http://shop.ml/catalog/electronics/audio/1&lt;/code&gt; это будет &lt;code&gt;/catalog/electronics/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UTMCampaign&lt;/strong&gt;, &lt;strong&gt;UTMMedium&lt;/strong&gt;, &lt;strong&gt;UTMSource&lt;/strong&gt;, &lt;strong&gt;UTMTerm&lt;/strong&gt; &amp;ndash; UTM тэги, которые
присутствовали в landing page URL.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;openstatService&lt;/strong&gt;, &lt;strong&gt;openstatSource&lt;/strong&gt; - OpenStat разметка, автоматически
проставляется для переходов из Я.Директа.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Все эти признаки являются категорийными, т.е. каждый из них для конкретного
посетителя принимает одно значение из набора возможных, например &lt;code&gt;city:Moscow&lt;/code&gt;,
&lt;code&gt;city:Krasnoyarsk&lt;/code&gt;, и т.п. Набор возможных значений ограничен частотой встречаемости:
значения, встречающиеся реже определённого порога, помещены в отдельную категорию &lt;code&gt;(other)&lt;/code&gt;,
например &lt;code&gt;city:(other)&lt;/code&gt;. У признака может не быть явного значения, например
значение &lt;code&gt;mobilePhone&lt;/code&gt; не определено, если посетитель зашёл на сайт c desktop устройства.
В этом случае используется специальная категория &amp;ldquo;значение отсутствует&amp;rdquo;: &lt;code&gt;mobilePhone:-&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Категорийные признаки переводятся в бинарные (т.е. принимающие значения только 0 или 1)
с помощью &lt;a href=&#34;https://en.wikipedia.org/wiki/One-hot&#34; target=&#34;_blank&#34;&gt;one-hot кодирования&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;линейная-модель-результаты&#34;&gt;Линейная модель, результаты&lt;/h2&gt;

&lt;p&gt;Конверсионность на e-commerce сайтах невысокая, в районе 0.1-0.3%. Т.е.
на 1000 посетителей приходится всего 1-3 конверсии, что делает обучение
модели не совсем тривиальной задачей из за несбалансированности количества
позитивных и негативных примеров. Так, для сайта luxshop.ru в обучающей
выборке всего 2458 позитивных примеров (конверсий), несмотря на казалось
бы большой объем выборки 2.5 млн посетителей.&lt;/p&gt;

&lt;p&gt;Тем не менее, линейная модель вполне работоспособна на таких данных: AUC для сайтов
shop.ml и luxshop.ml находится в районе &lt;strong&gt;76%-78%&lt;/strong&gt;. AUC, заметно отличающийся от 50%, говорит о том, что модель
выявила статистические закономерности в данных, поэтому веса факторов, которые
получились в результате обучения, не являются случайными.&lt;/p&gt;

&lt;p&gt;Посмотрим на результаты. Выведем факторы, которые больше всего влияют на
конверсию, как в положительную так и в отрицательную стороны, для магазина
shop.ml:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;shop_ml_linreg.png&#34; width=&#34;527&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Положительные факторы сверху и окрашены красным, отрицательные - снизу и окрашены синим.
Наблюдаемая картина в целом не противоречит здравому смыслу, например для отрицательных факторов:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;OS &lt;em&gt;Ubuntu&lt;/em&gt; скорее всего используется ботами или для тестирования&lt;/li&gt;
&lt;li&gt;Посетители из &lt;em&gt;Украины&lt;/em&gt; и &lt;em&gt;Казахстана&lt;/em&gt;  вряд ли будет делать заказы в магазине, работающем
в другой стране.&lt;/li&gt;
&lt;li&gt;браузер &lt;em&gt;Opera Mini&lt;/em&gt; уже весьма маргинален, и скорее всего говорит
об использовании устаревшего смартфона и пониженной платежеспособности владельца.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Но это ещё не окончательная версия. Обратим внимание на фактор &lt;code&gt;country:Russia&lt;/code&gt;.
Он имеет солидный положительный вес ~1.4, при этом доля России в трафике
сайта ~95%. C позиции здравого смысла страновые факторы для
остальных 5% трафика должны иметь огромный отрицательный вес (в районе -27), чтобы
уравновесить положительное влияние России. Однако этого не наблюдается, и
если учесть влияние всех страновых факторов для каждого посетителя из обучающий выборки,
то в среднем получается положительное
влияние ~1.34. То есть веса, получаемые из линейной модели,
не гарантируют что сумма факторов для каждой группы признаков (страна, город, и т.п.) будет в среднем нейтральной.
Но это легко исправить, рассчитав для каждого признака поправку (насколько
среднее по всем посетителям влияние факторов по этой переменной отклоняется от нуля),
и скорректировав веса на эту поправку.
$$\phi_i=w_i - \frac{1}{N}\sum_{k=1}^N\sum_{j\in G_i} w_jx_{k, j}$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\phi_i$ &amp;ndash; фактор для $i$-го признака.&lt;/li&gt;
&lt;li&gt;$w_i$ &amp;ndash; вес из линейной модели для $i$-го признака.&lt;/li&gt;
&lt;li&gt;$N$ &amp;ndash; количество примеров в обучающей выборке.&lt;/li&gt;
&lt;li&gt;$G_i$ &amp;ndash; индексы всех признаков, относящихся к той же группе, что $i$-ый признак, например
индексы всех страновых признаков.&lt;/li&gt;
&lt;li&gt;$x_{k, j}$ &amp;ndash; значение (0 или 1) $j$-го признака для $k$-го
примера из обучающей выборки.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Посмотрим, что получилось:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;shop_ml_linreg2.png&#34; width=&#34;568&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Страновые факторы действительно стали другими, остальные тоже немного скорректировались.
Как и ожидалось, у России
теперь фактор, близкий к нулю, поэтому она не попала в топ, зато в топе
много зарубежных стран с отрицательными факторами, что хорошо соответствует
здравому смыслу.&lt;/p&gt;

&lt;p&gt;Самое большое положительное влияние, с большим отрывом &amp;ndash; у белорусского города Могилёв.
Действительно, конверсия трафика из Могилёва целых 3.6%, при средней конверсии
0.38%, и средней конверсии трафика из Беларуси (без Могилёва) 0.31%. Возможно,
это аномалия, вызванная неверным определением географического положения (география
 определяется по IP адресам).&lt;/p&gt;

&lt;p&gt;Итак, линейная модель вполне работоспособна. В то же время у неё есть очевидные недостатки,
связанные с её простотой:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Линейная модель хорошо работает только для самых простых прогнозов.
Если есть взаимодействие факторов &amp;ndash; например, конверсионность выше у посетителей из Москвы,
но только в том случае, если они заходят со смартфонов, а с десктопных устройств конверсионность
наоборот ниже &amp;ndash; то линейная модель будет неспособна обнаружить такое взаимодействие.
Теоретически можно добавить в модель
синтетические факторы второго и более порядка, т.е. попарные сочетания всех факторов первого уровня, но тогда общее
количество факторов станет огромным, что затруднит интерпретацию модели и усложнит обучение.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Если имеются нелинейные зависимости между фактором и целевой переменной, или факторов друг c другом,
линейная модель будет неспособна с этим работать (что очевидно даже из названия &lt;em&gt;линейная&lt;/em&gt;). Сейчас
это проблема не проявилась, потому что в модели участвуют только категорийные признаки,
для работы с которыми достаточно линейной комбинации,
но если добавить в модель, например, время от последней сессии или количество сессий,
то нелинейность очень пригодится.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Учитывая недостатки линейной модели, мы сейчас не будем подробно рассматривать её
работу на других сайтах, вместо этого поищем другое, более универсальное решение.&lt;/p&gt;

&lt;h2 id=&#34;вклады-признаков-в-результат&#34;&gt;Вклады признаков в результат&lt;/h2&gt;

&lt;p&gt;Долгое время существовал выбор из двух взаимоисключающих вариантов:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Использовать простую, но при этом интерпретируемую
модель (линейная модель и её разновидности, различные параметрические модели).
В основном такие модели применяются в мире статистики, где традиционно требуется
объяснение полученных результатов.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Использовать сложную модель, которая может обучиться потенциально
чему угодно (нейросеть, gradient boosting, SVM, etc),
но при этом описать в доступных обычному человеку терминах, что
происходит внутри модели, и как она принимает решения, будет нереально.
Модель будет просто магическим чёрным ящиком, выдающим прогнозы. Такие
модели применяются в мире машинного обучения, там где важен сам
прогноз, а не объяснение, почему он получился именно таким.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Но существует альтернатива, позволяющая совместить и сложность
модели и её интерпретируемость.&lt;/p&gt;

&lt;p&gt;Любые модели, используемые в машинном обучении, можно представить
как функцию от признаков (features): $f(x_1, x_2, &amp;hellip; x_n)$.
Каждому значению признака $x_i$ можно сопоставить его &amp;ldquo;вклад&amp;rdquo; $\phi_i$.
Вклад &amp;ndash; это то, насколько данное значение воздействует на результат работы модели,
отклоняя его от среднестатистического значения. На примере нашей линейной модели:
есть признак &lt;code&gt;страна&lt;/code&gt;, его значение, отличное от &lt;code&gt;Россия&lt;/code&gt;, отклоняет
результат в отрицательную сторону (вероятность конверсии уменьшается).
Для признака &lt;code&gt;город&lt;/code&gt; значение &lt;code&gt;Могилёв&lt;/code&gt; наоборот отклоняет результат в
положительную сторону. Сумма вкладов всех признаков для конкретного
примера (в нашем случае для посетителя) равна отклонению результата работы
модели на этом конкретном примере от среднестатистического результата (в нашем
случае от средней конверсионности по всему сайту):&lt;/p&gt;

&lt;p&gt;$$\sum\phi_i = f(x) - \E[f(X)] $$
где $x = (x_1,\dots, x_n)$ &amp;ndash; набор признаков одного посетителя, для которого рассчитываются $\phi_i$,
$X$ &amp;ndash; признаки всех посетителей, $\E[\cdot]$ &amp;ndash; матожидание.&lt;/p&gt;

&lt;p&gt;Естественно, при сложении положительные и отрицательные вклады могут
полностью или частично нейтрализовать друг друга. Например, вероятность
конверсии посетителя из Литвы (&lt;code&gt;country:Lithuania&lt;/code&gt;, вклад -1), который
зашёл на сайт со страницы &lt;code&gt;landing_l2:/proctuct/1H&lt;/code&gt; (вклад +1), будет примерно
равна средней конверсионности по сайту, если не учитывать вклады остальных
признаков.&lt;/p&gt;

&lt;p&gt;В линейной модели вклады $\phi_i$ получаются из весов $w_i$ и не зависят
от конкретного посетителя, например вклад от признака &lt;code&gt;city:Могилёв&lt;/code&gt;
будет одинаковым для любого посетителя.  Но в более сложных моделях вклад
$\phi_i$ может зависеть от взаимодействий признаков друг с другом, и рассчитывается
индивидуально для каждого примера. Другим словами, вклад признака &lt;code&gt;city:Могилёв&lt;/code&gt; может
быть разным для разных посетителей из Могилёва, в зависимости от значений других признаков.&lt;/p&gt;

&lt;p&gt;Для линейной модели расчёт вкладов $\phi_i$ очень простой, и мы его уже
проделали выше. Но как рассчитываются вклады для сложных моделей?&lt;/p&gt;

&lt;h2 id=&#34;вектор-шепли&#34;&gt;Вектор Шепли&lt;/h2&gt;

&lt;p&gt;Решение приходит с довольно неожиданной стороны, из &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80&#34; target=&#34;_blank&#34;&gt;теории игр&lt;/a&gt;,
основные положения которой был разработаны в 50-х года прошлого века.
Представим группу игроков, играющих в любую игру с денежным выигрышем,
и кооперирующихся друг с другом (т.е. играющих не друг против друга, а в команде).
Цель игроков - получение максимального выигрыша. Но вот выигрыш получен,
как справедливо разделить его между игроками? У игроков может быть неодинаковый вклад в игру,
один игрок опытный, и &amp;ldquo;тянул на себе&amp;rdquo; всю партию, другой &amp;ndash; новичок, который только учится
играть, и польза от которого вообще сомнительна. Очевидно, нужен
способ для справедливого расчёта &lt;em&gt;персонального&lt;/em&gt; вклада каждого игрока.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Справедливость&amp;rdquo; можно выразить в более конкретных терминах:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Сумма вознаграждений всех игроков равна общему выигрышу (Эффективность)&lt;/li&gt;
&lt;li&gt;Если два игрока принесли одинаковую пользу, они получают равное вознаграждение, независимо
от того, в составе каких команд они играли. (Симметричность)&lt;/li&gt;
&lt;li&gt;Если игрок не принёс никакой пользы, его вознаграждение равно нулю. (Аксиома болвана)&lt;/li&gt;
&lt;li&gt;Если группа игроков играет несколько партий, сумма вознаграждения игрока
равна сумме его вознаграждений в каждой партии. (Аддитивность)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Теория игр постулирует, что единственный способ расчёта, удовлетворяющий
всем четырём условиям, это &lt;em&gt;&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_%D0%A8%D0%B5%D0%BF%D0%BB%D0%B8&#34; target=&#34;_blank&#34;&gt;вектор Шепли&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;shapely.png&#34; width=&#34;622&#34; height=&#34;211&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Расчёт вектора Шепли можно представить через маржинальные вклады игроков,
когда вклад игрока оценивается по росту выигрыша команды, к которой он присоединился:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;В начале вся команда состоит из игрока &lt;strong&gt;A&lt;/strong&gt;. Выигрыш $7, значит и
вклад игрока &lt;strong&gt;A&lt;/strong&gt; равен 7.&lt;/li&gt;
&lt;li&gt;Когда к нему присоединяется игрок &lt;strong&gt;C&lt;/strong&gt;, выигрыш команды растёт до $15. Значит
вклад игрока &lt;strong&gt;C&lt;/strong&gt; равен 8.&lt;/li&gt;
&lt;li&gt;Присоединяется игрок &lt;strong&gt;B&lt;/strong&gt;, выигрыш растёт до $19. Вклад игрока &lt;strong&gt;B&lt;/strong&gt; равен
4 единицам, и т.п.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Но это ещё не полноценный расчёт. Умения игроков могут пересекаться,
например игрок &lt;strong&gt;B&lt;/strong&gt; может обладать примерно теми же навыками, что и игрок &lt;strong&gt;А&lt;/strong&gt;.
В одиночку &lt;strong&gt;A&lt;/strong&gt; выигрывает $7, а присоединение к нему игрока &lt;strong&gt;B&lt;/strong&gt;
не поднимает выигрыш, т.е. вклад игрока &lt;strong&gt;B&lt;/strong&gt; получается нулевым, хотя
&lt;strong&gt;B&lt;/strong&gt; в одиночку выигрывает $4.
Получается, что результат зависит от порядка
добавления игроков. Чтобы убрать эту зависимость, надо
учесть все возможные варианты, например для игрока &lt;strong&gt;С&lt;/strong&gt;: &lt;strong&gt;C&lt;/strong&gt;, &lt;strong&gt;A&lt;/strong&gt; + &lt;strong&gt;С&lt;/strong&gt;, &lt;strong&gt;AB&lt;/strong&gt; + &lt;strong&gt;C&lt;/strong&gt;, &lt;strong&gt;B&lt;/strong&gt; + &lt;strong&gt;C&lt;/strong&gt;
и взять среднее значение вклада.&lt;/p&gt;

&lt;p&gt;Некоторые последовательности будут избыточными, например &lt;strong&gt;AB&lt;/strong&gt; + &lt;strong&gt;C&lt;/strong&gt; и &lt;strong&gt;BA&lt;/strong&gt; + &lt;strong&gt;C&lt;/strong&gt;
эквивалентны с точки зрения оценки вклада &lt;strong&gt;С&lt;/strong&gt;, поэтому вектор Шэпли
рассчитывают не через последовательности, а на основе множеств,
в которые может войти игрок, такие множества называются &lt;em&gt;коалиции&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Резюмируя всё сказанное выше, получаем определение:
Вектор Шепли (Shapely values) - это распределение выигрышей,
соответствующее среднему маржинальному вкладу каждого игрока во всех возможных
коалициях с его участием.&lt;/p&gt;

&lt;p&gt;Полную формулу расчёта вектора Шэпли не привожу, она довольно громоздкая.
Формула есть в &lt;a href=&#34;https://en.wikipedia.org/wiki/Shapley_value&#34; target=&#34;_blank&#34;&gt;Википедии&lt;/a&gt;, или в книге &lt;em&gt;Interpretable Machine Learning&lt;/em&gt;
есть &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/shapley.html&#34; target=&#34;_blank&#34;&gt;подробная глава&lt;/a&gt; о
применении shapely values для интерпретации моделей.&lt;/p&gt;

&lt;h2 id=&#34;shap&#34;&gt;SHAP&lt;/h2&gt;

&lt;p&gt;Вернёмся к машинному обучению. Если рассматривать игру, как
получение моделью результата на основе заданного примера,
а выигрыш, как разницу
между матожиданием результата на всех имеющихся примерах
и результатом, полученном на заданном примере:
$f(x) - \E[f(X)]$ (выигрыш может быть и отрицательным),
то вклады игроков в игру это не что иное, как
вклад $\phi_i$ каждого значения признака в &amp;ldquo;выигрыш&amp;rdquo;, т.е. насколько сильно
это значение признака повлияло на результат. Концепция
из теории игр оказывается вполне применимой в машинном обучении.&lt;/p&gt;

&lt;p&gt;При расчёте вектора Шэпли необходимо формировать коалиции из ограниченного
набора признаков, но далеко не каждая модель позволяет просто взять и убрать признак без
необходимости заново обучаться с нуля. Потому на практике для формирования
коалиций обычно не убирают &amp;ldquo;лишние&amp;rdquo; признаки, а заменяют их на случайные
значения из &amp;ldquo;фонового&amp;rdquo; набора данных.
Усреднённый результат модели со случайными значениями признака эквивалентен
результату модели, в которой этот признак вообще отсутствует.&lt;/p&gt;

&lt;p&gt;Прямой расчёт вектора Шэпли возможен только
для совсем маленьких моделей, т.к. число возможных сочетаний (коалиций)
$n$ признаков это $2^n$. С увеличением количества признаков время
расчёта растёт экспоненциально и быстро выходит за пределы разумного.
В нашей простой модели всего 21 признак, $2^{21}$ это уже 2097152
возможных коалиций. А если учесть, что эти признаки категорийные, и для работы
их надо перевести в бинарное кодирование, то получится около 1000 признаков.
$2^{1000}$ коалиций это число, в котором 302 цифры (для сравнения, во всей Вселенной
содержится всего лишь около $2^{265}$ атомов). По этой причине вектора Шепли
долгое время не представляли практического интереса для машинного обучения.&lt;/p&gt;

&lt;p&gt;Но недавно были разработаны способы эффективного расчёта векторов Шэпли
(библиотека &lt;a href=&#34;https://github.com/slundberg/shap&#34; target=&#34;_blank&#34;&gt;SHAP&lt;/a&gt; - SHapely Additive Explanations),
которые используют дополнительную информацию о структуре модели и в результате
проводят вычисления за приемлемое время. Результаты,
выдаваемые библиотекой, могут быть не совсем &amp;ldquo;честными&amp;rdquo; векторами
Шэпли, а их аппроксимацией. Будем называть их &lt;em&gt;SHAP values&lt;/em&gt;.
Такие расчёты сейчас лучше всего проработаны
для методов обучения, использующих tree ensembles, например &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting&#34; target=&#34;_blank&#34;&gt;Gradient Tree Boosting&lt;/a&gt;
и нейросети.
Попробуем рассчитать SHAP values для моделей, обученных с использованием
библиотеки XGBoost.&lt;/p&gt;

&lt;h1 id=&#34;xgboost-shap&#34;&gt;XGBoost + SHAP&lt;/h1&gt;

&lt;p&gt;Новые модели обучены с помощью XGBoost на том же наборе признаков, что и
линейные модели. В результате получился AUC несколько выше (на 1-2%), чем
у логистической регрессии: XGBoost использует взаимодействие между признаками, которое было
недоступно для линейных моделей. Возможно, результат
может быть ещё лучше при более тщательной настройке гиперпараметров XGBoost.&lt;/p&gt;

&lt;p&gt;Чтобы увидеть факторы, воздействующие на конверсию, рассчитаем
SHAP values для каждого примера из тестовой выборки.
В отличие от линейной модели, где величина каждого фактора была
константой, SHAP values индивидуальны для каждого примера из выборки, поэтому
на выходе получается не единственное значение для каждого фактора, а распределение.
Таким образом мы получаем ещё и приблизительную оценку уверенности в степени воздействия фактора.&lt;/p&gt;

&lt;h2 id=&#34;shop-ml&#34;&gt;shop.ml&lt;/h2&gt;

&lt;p&gt;Для визуализации распределений будем использовать &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%AF%D1%89%D0%B8%D0%BA_%D1%81_%D1%83%D1%81%D0%B0%D0%BC%D0%B8&#34; target=&#34;_blank&#34;&gt;box plot&lt;/a&gt;.
Концы &amp;ldquo;усов&amp;rdquo;
соответствуют минимальному и масксимальному значениям SHAP, закрашенный
прямоугольник &amp;ndash; диапазону, в котором сконцентрирована основная
масса значений. Оранжевая вертикальная линия внутри прямоугольника
соответствует медианному значению.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;shop_ml_shap.png&#34; width=&#34;416&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Видно, что в целом результаты похожи
на линейную регрессию: большое положительное влияние оказывают некоторые
landing pages и рефереры, отрицательное влияние &amp;ndash; в основном страны,
отличные от России. Город Могилёв, который был рекордсменом положительного
влияния в линейной модели, тоже имеет положительное влияние, но не такое
экстремальное: среднее всего +0.5, что даже не позволило ему войти в топ 20,
поэтому его нет на диаграмме.&lt;/p&gt;

&lt;p&gt;Факторы, рассчитанные на основе SHAP values, более консервативны:
большое положительное или отрицательное влияние приписывается фактору, только
если модель видит достаточно большое количество подтверждающих примеров
в выборке. Это хорошо для анализа воздействия на конверсию,
 т.к. значения конверсионности, рассчитанные на маленькой выборке,
 ненадёжны (см. &lt;a href=&#34;https://suilin.ru/post/conversion_numbers/&#34;&gt;Каким цифррам можно верить?&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Можно проанализировать и факторы, относящиеся к одному и тому же признаку,
например по метке UTMSource:



&lt;figure&gt;

&lt;img src=&#34;shop_ml_shap_utmsource.png&#34; width=&#34;460&#34; height=&#34;372&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;В этому случае получается некоторый аналог отчёта Я.Метрики или Google Analytics,
но дающий более внятную картину &amp;ldquo;что хорошо, а что плохо&amp;rdquo;. Для большинства
меток нет выявленной закономерности по воздействию на конверсионность, поэтому
их медиана находится в околонулевой зоне. В то же время метки, явно
действующие &amp;ldquo;в плюс&amp;rdquo; или &amp;ldquo;в минус&amp;rdquo; чётко отделены от основной массы,
это вместо каши из цифр, которая была бы в стандартном отчёте.&lt;/p&gt;

&lt;h2 id=&#34;luxshop-ml&#34;&gt;luxshop.ml&lt;/h2&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;luxshop_ml_shap.png&#34; width=&#34;452&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;
В этом магазине очень сильное влияние у landing страниц, ассоциированных
с корзиной. Похоже, что использовался retargeting: если посетитель оставлял товары
в корзине и уходил без покупки, его &amp;ldquo;возвращали&amp;rdquo; в магазин.
Теоретически, когда такой посетитель вернётся
в магазин, это будет уже повторный визит, и он не должен попасть в выборку.
Но отслеживание посетителей через cookies не на 100% точное, видимо
посетитель успевал потерять или очистить cookies.&lt;/p&gt;

&lt;p&gt;Чтобы остальные факторы были более заметны, отобразим ту же диаграмму
без топовых посадочных страниц:



&lt;figure&gt;

&lt;img src=&#34;luxshop_ml_shap_cut.png&#34; width=&#34;452&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Можно сделать выводы:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Очень хорошо по сравнению со всем остальным выглядят прямые заходы (&lt;code&gt;trafficSource:direct&lt;/code&gt;).
Видимо у магазина работает оффлайн реклама, и/или имеется сильный бренд.&lt;/li&gt;
&lt;li&gt;Трафик из Москвы конвертируется &lt;strong&gt;намного&lt;/strong&gt; лучше, чем из остальных городов.
Что не удивительно, учитывая стоимость товаров.&lt;/li&gt;
&lt;li&gt;Заходы с лицевой страницы сайта &lt;code&gt;landing_l1:/&lt;/code&gt; повышают конверсию.
То есть людям важнее бренд в целом, чем конкретный товар.&lt;/li&gt;
&lt;li&gt;Конверсия ощутимо зависит от типа устройства: desktop устройства &lt;code&gt;device:desktop&lt;/code&gt;
конвертируются лучше мобильных &lt;code&gt;device:mobile&lt;/code&gt;. Возможно это связано с тем,
что покупка достаточно дорогих товаров не происходит спонтанно
(достал в метро смартфон и купил), нужно время на рассматривание и размышления.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Для наглядности, дополнительно выведем факторы только по типу устройства и только по городу:&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;luxshop_ml_shap_device.png&#34; width=&#34;378&#34; height=&#34;114&#34; /&gt;


&lt;/figure&gt;



&lt;figure&gt;

&lt;img src=&#34;luxshop_ml_shap_city.png&#34; width=&#34;461&#34; height=&#34;604&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;скоринг-посетителей&#34;&gt;Скоринг посетителей&lt;/h2&gt;

&lt;p&gt;До этого момента мы рассматривали суммарную статистику по факторам для
всей выборки посетителей. Но не менее интересно увидеть индивидуальные
факторы для отдельных посетителей. В линейной модели это не имело смысла,
так как факторы для всех одни и те же, но SHAP рассчитывает индивидуальные
факторы на каждый пример (в нашем случае на каждого посетителя).
Но сначала надо выяснить, в какой шкале вычисляются факторы. Например, фактор, дающий +1,
это сколько к конверсии?&lt;/p&gt;

&lt;p&gt;И линейная модель и большинство других моделей в машинном обучении не
оперируют напрямую вероятностями, выраженными в процентах. Более
удобная для вычислений шкала это log-odds. Если вероятность события это
$p$, то odds, или шансы события, это $p/(1-p)$. В быту шансы записывают
как $x:y$, где $x$ и $y$ соответствуют числителю и знаменателю в выражении,
конвертирующем вероятность в шансы. Например, вероятность 50% это шансы 1:1
$$\frac{0.5}{1-0.5} = \frac{0.5}{0.5} = 1:1$$
вероятность 1% это шансы 1:99
$$\frac{0.01}{1-0.01} = \frac{1}{100-1} = 1:99 $$&lt;/p&gt;

&lt;p&gt;log-odds это логарифм шансов, или логистическая функция, давшая название
логистической регрессии, которую мы применяли, работая с линейной моделью:&lt;/p&gt;

&lt;p&gt;$$\textit{log-odds}(p) \equiv logit(p)=\log\left(\frac{p}{1-p}\right)$$&lt;/p&gt;

&lt;p&gt;Внутри моделей все вычисления происходят в logit шкале (она же log-odds).
 То есть вероятности 50% соответствует значение $\log(0.5/(1-0.5)) = \log(1) = 0$.
 Обычно используется натуральный логарифм, но мы будем использовать логарифм по основанию 2,
 с которым удобнее работать вручную.
 Тогда вероятности 1% соответствует значение
 $$\log_2\left(\frac{1}{100-1}\right) = \log_2(1/99) = -6.63$$
 а вероятности 99% значение
 $$\log_2\left(\frac{99}{100-99}\right) = \log_2(99/1) = -6.63$$&lt;/p&gt;

&lt;p&gt;Векторы Шэпли рассчитываются в этой же шкале, фактор +1 сдвигает
 log-odds на одну единицу. Это означает, что шансы увеличиваются в два раза (т.к. используется основание 2).
 Например, если средняя конверсионность 1%, или 1:99, то фактор +1 даст конверсионность
 2:99 или 1.98%. Для типичных значений конверсионности можно приблизительно считать
 изменение шансов в $n$ раз эквивалентным изменению конверсионности в $n$ раз ($1.98\% \approx 2 \times 1\%$), тогда:&lt;/p&gt;

&lt;p&gt;$$p_f = 2^{f}\cdot\E[p] $$
 где $f$ &amp;ndash; значение фактора, $p_f$ &amp;ndash; конверсионность с учётом влияния фактора $f$,
 $\E[p]$ &amp;ndash; средняя конверсионность. Фактор +1 увеличивает конверсионность в два раза,
 фактор -1 соответственно уменьшает в два раза, и т.п.&lt;/p&gt;

&lt;p&gt;Теперь собственно скоринг. Будем отображать scorecard каждого посетителя в виде
 столбца, у которого есть:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Базовое значение (жирная черная горизонтальная линия). Позиция базового значения
по шкале Y соответствует тому, насколько сдвинута по мнению модели
вероятность конверсии посетителя относительно матожидания конверсии
по всем посетителям, т.е &amp;ldquo;средней&amp;rdquo; конверсионности.&lt;/li&gt;
&lt;li&gt;Положительные факторы (сегменты красного цвета над базовым значением).
Высота сегмента соответствует вкладу фактора. Чтобы не загромождать
диаграмму, отображаются только факторы, имеющие вклад больше 0.02.&lt;/li&gt;
&lt;li&gt;Отрицательные факторы (сегменты синего цвета под базовым значением).&lt;/li&gt;
&lt;/ul&gt;




&lt;figure&gt;

&lt;img src=&#34;luxshop_ml_scores.png&#34; width=&#34;554&#34; height=&#34;467&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Сумма положительных и отрицательных факторов равна базовому значению.
Шкала по оси Y &amp;ndash; log-odds. Нулевой уровень шкалы (отмечен пунктиром)
соответствует средней конверсии.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Посетитель &lt;strong&gt;A&lt;/strong&gt; имеет самые высокие шансы сконвертироваться, его базовый
уровень $+3.3$, что соответствует увеличению конверсии в $2^{3.3} \approx 9.85$ раз,
т.е. почти десятикратное увеличение. Это результат удачного сочетания
факторов: прямой заход, Москва, стационарный компьютер.&lt;/li&gt;
&lt;li&gt;Посетитель &lt;strong&gt;С&lt;/strong&gt; наоборот практически лишён положительных факторов, всё
работает против него. Его базовый уровень $-1.3$, это уменьшение
конверсионности в $2^{1.3} \approx 2.46$ раза.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Посетители &lt;strong&gt;B&lt;/strong&gt; и &lt;strong&gt;D&lt;/strong&gt; имеют промежуточную конверсионность, близкую к
средней по сайту.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scorecards позволяют наглядно увидеть, что модель &amp;ldquo;думает&amp;rdquo; о любом
посетителе, а также сравнивать посетителей друг с другом. Они могут быть
быть например хорошим дополнением к информации о посетителе в Я.Метрике.
В простой модели, которой мы сейчас пользуемся, scorecard носит
скорее разъяснительный характер, т.к. после завершения первого визита всё
равно известно, сконвертировался посетитель или нет. Но потом мы рассмотрим
и более сложные модели, которые предсказывают поведение посетителя в будущем,
и там scorecard станет важным рабочим инструментом.&lt;/p&gt;

&lt;h2 id=&#34;courses-ml&#34;&gt;courses.ml&lt;/h2&gt;

&lt;p&gt;Этот сайт не имеет &amp;ldquo;конверсии&amp;rdquo; в традиционном понимании, его цель &amp;ndash;
сделать так, чтобы посетитель заинтересовался, остался и начал проходить курсы.
Поэтому вместо конверсии будем использовать вероятность того, что
после первого посещения посетитель вернётся на сайт в течение месяца.&lt;/p&gt;

&lt;p&gt;Сайт привлекает посетителей в основном через размещение ссылок
на сайтах подходящей тематики, а не через традиционную онлайн рекламу,
поэтому расклад факторов, влияющих на конверсию, здесь сильно отличается
от магазинов:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;courses_ml_shap.png&#34; width=&#34;471&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Видно сильное отрицательное влияние служебных доменов: модель поняла,
что посетители, заходящие с этих доменов, ведут себя совсем не так, как остальные.
Чтобы они не мешали анализу, исключим их и построим диаграмму ещё раз:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;courses_ml_shap_cut.png&#34; width=&#34;471&#34; height=&#34;617&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;В топе положительных факторов переходы с &lt;code&gt;coursera.org&lt;/code&gt;, т.к. там есть аудитория,
уже заинтересованная в обучении, а также домен партнёра &lt;code&gt;partner.courses.ml&lt;/code&gt;,
это white label сервис на отдельном домене. Также
в топе много переходов с почтовых сервисов, т.е. хорошо работает
привлечение (или возврат) посетителей через рассылки.&lt;/p&gt;

&lt;p&gt;В топе отрицательных факторов:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;landing_l1:/unsubscribe/&lt;/code&gt; &amp;ndash; отписка от рассылки (неудивительно)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;browser:—&lt;/code&gt; &amp;ndash; посетители, у которых не определился браузер, вероятно боты.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;urlDomain:(other)&lt;/code&gt; &amp;ndash; низкочастотные доменные имена, попавшие в группу &amp;ldquo;остальное&amp;rdquo;, видимо
это тоже служебные домены.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;referer:lady.mail.ru&lt;/code&gt; &amp;ndash; переходы с &lt;a href=&#34;http://lady.mail.ru&#34; target=&#34;_blank&#34;&gt;http://lady.mail.ru&lt;/a&gt;. На сайте courses.ml курсы в основном IT тематики,
и к сожалению нет курсов &amp;ldquo;как найти богатого мужа&amp;rdquo; или &amp;ldquo;101 кулинарный
шедевр с майонезом&amp;rdquo;. Поэтому интерес со стороны аудитории lady.mail.ru
весьма низок.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scorecards:



&lt;figure&gt;

&lt;img src=&#34;courses_ml_scores.png&#34; width=&#34;498&#34; height=&#34;684&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Посетитель &lt;strong&gt;A&lt;/strong&gt; демонстрирует повышение вероятности повторного визита в $2^{3.5} \approx 11.3$
раза по отношению к среднему уровню, посетитель &lt;strong&gt;B&lt;/strong&gt; &amp;ndash; понижение в
$2^{4.2} \approx 18.4$ раз, у посетителей &lt;strong&gt;D&lt;/strong&gt; и &lt;strong&gt;C&lt;/strong&gt; положительные и отрицательные факторы уравновешивают друг друга
и получаются вероятности, близкие к средним.&lt;/p&gt;

&lt;h1 id=&#34;резюме&#34;&gt;Резюме&lt;/h1&gt;

&lt;p&gt;Мы научились применять модели машинного обучения для анализа
факторов, влияющих на конверсию. Благодаря применению векторов Шэпли
интерпретация модели любой сложности сводится к линейной комбинации факторов,
которая легко воспринимаются человеком и легко отображается графически (scorecards).&lt;/p&gt;

&lt;p&gt;Пока рассматривались только простые модели, использующие
исключительно параметры первого перехода на сайт. Более интересны модели,
работающие с информацией о поведении посетителя на сайте, они позволяют
точнее предсказывать дальнейшие действия посетителя.
Такие продвинутые модели мы рассмотрим в следующих статьях (&lt;a href=&#34;https://suilin.ru/post/conversion_history&#34;&gt;1&lt;/a&gt;,
&lt;a href=&#34;https://suilin.ru/post/clv&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://suilin.ru/post/user_churn&#34;&gt;3&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Но даже такие довольно простые выводы, которые можно получить на основе
рассмотренных моделей, уже облегчают работу с Интернет-аналитикой.
Вместо копания в десятках отчётов можно сразу получить готовые ответы &amp;ndash;
это и есть результат, к которому надо стремиться.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science II. Оптимизируем неизвестность</title>
      <link>https://suilin.ru/post/conversion_opt/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/conversion_opt/</guid>
      <description>

&lt;p&gt;При управлении онлайновыми рекламными кампаниями при подключении новых
источников (объявлений, баннеров, SMM и т.п.) часто приходится решать проблему:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;С одной стороны, разумно подождать, пока источник не выдаст побольше
переходов, посмотреть на конверсии, и тогда принимать решение, оставить его в рекламной
кампании или отключить.&lt;/li&gt;
&lt;li&gt;Но если долго ждать, тогда рекламный бюджет будет зря расходоваться на
неэффективные источники.&lt;/li&gt;
&lt;li&gt;С другой стороны, если сократить ожидание, можно случайно отключить источник,
который на самом деле конверсионный,
и наоборот, оставить неэффективный источник, случайно показавший
высокую конверсию.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Очевидно, нужно найти некий баланс: с одной стороны, дать
источникам достаточно времени, чтобы проявить себя, с другой стороны вовремя отключать неэффективных.&lt;/p&gt;

&lt;p&gt;Поиском такого баланса мы и займемся. Для этого будем тестировать
эффективность стратегий управления источниками на модели, имитирующей
поступление трафика и конверсии на реальном сайте.&lt;/p&gt;

&lt;h1 id=&#34;модель&#34;&gt;Модель&lt;/h1&gt;

&lt;p&gt;Мы будем моделировать рекламную кампанию, которая длится $T$ дней.
Средний объем трафика, приходящего на сайт &amp;ndash; $V$ визитов в день,
этот трафик распределяется между $N$ источниками. У каждого источника есть вес,
$w_{i,t}$ обозначающий долю всего трафика, которая приходится на источник
в день $t$:&lt;/p&gt;

&lt;p&gt;$$\sum_{i=1}^N w_{i,t} = 1, \;  t \in 1 \dots T$$&lt;/p&gt;

&lt;p&gt;На старте у источников одинаковые равные веса $w_{i, 1}=\frac{1}{N}$.
Затем каждый день стратегия меняет вес источников на своё усмотрение.&lt;/p&gt;

&lt;p&gt;Вес $w_{i,t}$ может быть нулевым, это означает, что источник отключен.&lt;/p&gt;

&lt;p&gt;Каждый источник в соответствии со своим весом генерирует количество визитов
с матожиданием $\lambda_{i,t}$ (интенсивность визитов):
$$\lambda_{i,t} = w_{i,t} V$$&lt;/p&gt;

&lt;p&gt;Но это среднее количество визитов, а реальное их количество
моделируется как
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81_%D0%9F%D1%83%D0%B0%D1%81%D1%81%D0%BE%D0%BD%D0%B0&#34; target=&#34;_blank&#34;&gt;процесс Пуассона&lt;/a&gt;
с интенсивностью $\lambda_{i,t}$. Тогда количество визитов $v_{i,t}$, которое выдаст источник в каждый конкретный день
$t$, сэмплируется из распределения Пуассона:
$$v_{i,t} \sim \mathrm{Pois}(\lambda_{i,t})$$
Таким образом моделируется ситуация из реальной жизни: далеко
не всегда можно управлять &lt;em&gt;точным&lt;/em&gt; количеством переходов на сайт из источника.&lt;/p&gt;

&lt;p&gt;Каждый источник имеет латентную конверсионность $\theta_i$ (т.е. это истинная конверсионность источника,
 которая проявится на большом количестве трафика).
Распределение
конверсионностей источников смоделируем логнормальным распределением:
$$\theta_i \sim \exp\left(\mathcal{N}(\mu, \sigma^2)\right)$$
Для наших экспериментов примем, что медианная конверсионность источников 1%:
$\mu = \ln(0.01)$ и $\sigma = 0.5$. Получится распределение,
похожее на то, что встречается на реальных сайтах:



&lt;figure&gt;

&lt;img src=&#34;lognorm.png&#34; width=&#34;384&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Логнормальное распределение несимметрично (длинный хвост справа), поэтому
для него матожидание (т.е. средняя конверсионность) и медиана будут немного отличаться:
$$\mathrm{E}=\exp\left(\mu + \frac{\sigma^2}{2}\right) \\&lt;br /&gt;
\mathrm{Median}=\exp(\mu)$$
В используемом нами распределении средняя конверсионность будет $\approx 1.13\%$&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Латентная конверсионность источника $\theta_i$ недоступна стратегии,
доступна только реализация конверсионности $r_{i,t}$:
$$c_{i,t} \sim \mathrm{Bin}(v_{i,t}, \theta_i) \\&lt;br /&gt;
r_{i,t} = \frac{\sum_{j=1}^t c_{i,j}}{\sum_{j=1}^t v_{i,j}}$$
т.е. количество конверсий у источника в конкретный день $c_{i,t}$ сэмплируется из биномиального
распределения, а параметры биномиального распределения, в свою очередь,
тоже сэмплируются из своих распределений, описанных выше. Сэмплирование
количества визитов $v_{i,t}$ происходит каждый день, сэмплирование конверсионностей $\theta_i$
один раз перед стартом каждого эксперимента.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;mab_model.png&#34; alt=&#34;Принципиальная схема модели&#34; width=&#34;515&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Принципиальная схема модели
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;параметры-по-умолчанию&#34;&gt;Параметры по умолчанию&lt;/h2&gt;

&lt;p&gt;Сведём вместе все значения параметров по умолчанию:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$N=100$. В каждом эксперименте участвует 100 источников, из которых
будем отбирать лучших.&lt;/li&gt;
&lt;li&gt;$T=90$. Рекламная кампания (т.е. один эксперимент) длится 90 дней.&lt;/li&gt;
&lt;li&gt;$V=100$. Каждый день между источниками распределяется 100 визитов.
При таком объеме трафика на каждый источник приходится в среднем
всего &lt;strong&gt;один визит в день&lt;/strong&gt;, и за всё время эксперимента произойдет в среднем
&lt;strong&gt;одна конверсия&lt;/strong&gt; на источник! Очевидно, что выявление лучших источников при столь
малом объеме трафика это нетривиальная задача. Для некоторых
экспериментов будем использовать менее экстремальное значение $V=1000$&lt;/li&gt;
&lt;li&gt;Конверсионность источников берётся из логнормального распределения
с параметрами $\mu=\ln(0.01), \sigma=0.5$, что соответствует средней
конверсионности $\approx 1.13\%$.&lt;/li&gt;
&lt;li&gt;Результаты каждого эксперимента будут отличаться друг от друга
из за использования стохастических переменных, поэтому итоговый результат
будем рассчитывать, как среднее по 2000 повторений эксперимента.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;цель-стратегий&#34;&gt;Цель стратегий&lt;/h2&gt;

&lt;p&gt;Стратегия в конце каждого дня (момент времени $t$) смотрит на накопленные результаты работы
источников (кол-во визитов, кол-во конверсий и вычисленная на их основе конверсионность) и принимает решение, какие
веса дать источникам на следующий день, т.е. на момент времени $t+1$.&lt;/p&gt;

&lt;p&gt;Цель стратегии &amp;ndash; получить максимальное кол-во конверсий за время рекламной кампании,
т.е. дать максимальный вес источникам с высокой конверсионностью
и минимальный вес всем остальным. В реальной жизни ёмкость источников ограничена,
поэтому существует дополнительное условие:
$w_{i} \leq w_i^{max}$&lt;/p&gt;

&lt;p&gt;Каким должен быть максимальный вес $w_i^{max}$? Обычно источники
с высокой конверсионностью имеют меньшую ёмкость, так как по сути являются
узкими (и часто дорогими) сегментами аудитории. Чтобы отразить это в модели,
сделаем максимальный вес источника обратно пропорциональным его конверсионности:
$$w_{i}^{max} = \frac{k}{\theta_i} $$
где $\theta_i$ это латентная конверсионность, $k$ &amp;ndash; коэффициент, регулирующий
среднюю ёмкость источников. Примем $k$ равным матожиданию конверсионности:
$$k=\mathrm{E}(\theta)$$
Тогда источник со &amp;ldquo;средней&amp;rdquo; конверсионностью будет иметь максимальный вес, равный
единице. Источник &amp;ldquo;хуже среднего&amp;rdquo; сможет быть единственным активным
источником для сайта, а источник &amp;ldquo;лучше среднего&amp;rdquo; &amp;ndash; не сможет. Если конверсионность
источника в 2 раза выше средней, он сможет иметь максимум 50% трафика,
если в 4 раза выше средней &amp;ndash; 25% трафика, и т.п.&lt;/p&gt;

&lt;p&gt;В идеале к концу эксперимента должен остаться активным только топ
лучших источников. При используемом распределении конверсионностей в идеальный топ будет
входить в 80% случаев 3 источника и в 20% случаев 4 источника.&lt;/p&gt;

&lt;h2 id=&#34;оценка-стратегий&#34;&gt;Оценка стратегий&lt;/h2&gt;

&lt;p&gt;Эффективность стратегий оценивается по улучшению конверсионности по сравнению с baseline (0%)
и идеальным вариантом (100%).
За baseline принимается средняя конверсионность, которая будет, если вообще ничего
не делать и оставить все источники в равных долях, как они были на старте.
За идеальный вариант принимается максимальная конверсионность, которая была бы,
если с первого же дня оставить только лучшие источники и отключить все остальные.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Общее улучшение&lt;/em&gt; &amp;ndash; это то, как полученные результаты соотносятся с baseline и максимумом:
$$improvement=\frac{result-baseline}{maximum-baseline} \times 100\%$$
Улучшение рассчитывается для каждого дня отдельно, затем результат усредняется.
Если не получилось ничего улучшить по сравнению с baseline, улучшение будет 0%,
если наоборот с первого дня удалось достичь максимальной возможной конверсионности,
улучшение будет 100%. Реальное улучшение обычно будет между 0% и 100%
(но может быть и отрицательным, если результатом стратегии стало ухудшение
 конверсионности вместо улучшения).&lt;/p&gt;

&lt;p&gt;Также интересно &lt;em&gt;финальное улучшение&lt;/em&gt;, это то, где между baseline и максимумом
был результат в последний день рекламной кампании.&lt;/p&gt;

&lt;h1 id=&#34;наивная-стратегия&#34;&gt;Наивная стратегия&lt;/h1&gt;

&lt;p&gt;Начнём с самого простого, и протестируем наивную стратегию,
имитирующую традиционный способ управления источниками:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Ждём, пока все источники не увидят в среднем $M$ визитов&lt;/li&gt;
&lt;li&gt;Оставляем лучшие источники, отключаем все остальные.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Каким должно быть $M$, т.е. сколько надо ждать &amp;ndash; неизвестно, поэтому
попробуем разные значения:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;naive_h.png&#34; width=&#34;391&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Видно, что при любых $M$ улучшение не поднимается выше 17%. Если
ждать недолго, то не успеем накопить достоверную информацию о конверсионности,
а если ждать дольше, то неэффективные источники отключатся только
в конце рекламной кампании. Естественно, при длительном ожидании финальное
улучшение будет высоким, но для всей рекламной кампании
важно именно общее улучшение.&lt;/p&gt;

&lt;p&gt;Посмотрим на динамику конверсионности и
распределение улучшений при оптимальном $M=38$:



&lt;figure&gt;

&lt;img src=&#34;naive.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;На левой диаграмме пунктирными линиями представлены baseline и максимальная
возможная конверсионность. График реальной конверсионности, являющийся результатом
работы стратегии &amp;ndash; голубая линия с верхней и нижней границами, соответствующими доверительному интервалу 95%.
Общее улучшение соответствует
площади под этим графиком по отношению ко всей площади между зеленой
и оранжевой пунктирными линиями. Общее/финальное улучшения отображены
в заголовке диаграммы.&lt;/p&gt;

&lt;p&gt;На правой диаграмме видно, что в части экспериментов улучшение было отрицательным, т.е. ожидания
в 38 дней явно недостаточно для получения достоверной информации о конверсионности.&lt;/p&gt;

&lt;h1 id=&#34;многорукие-бандиты&#34;&gt;Многорукие бандиты&lt;/h1&gt;

&lt;p&gt;Проблему, которую решают наши стратегии, можно сформулировать так:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Найти баланс между а) исследованием конверсионности источников (&lt;strong&gt;exploration&lt;/strong&gt;),
и б) использованием найденной конверсионности для оптимизации (&lt;strong&gt;exploitation&lt;/strong&gt;), таким образом,
чтобы максимизировать выигрыш (в нашем случае количество конверсий).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Эта проблема известна в более широком смысле, как проблема
&lt;em&gt;многорукого бандита&lt;/em&gt;  ( &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34; target=&#34;_blank&#34;&gt;multi-armed bandit&lt;/a&gt;, MAB). Представим набор
игральных автоматов (слот-машин, также называемых однорукими бандитами), каждый из которых генерирует единичный выигрыш
с вероятностью $\theta_i$, неизвестной игроку. Задача игрока &amp;ndash; за конечное время получить
максимальный выигрыш, т.е. поиграв на каждом бандите,
приблизительно определить его $\theta_i$  (exploration phase), и по результатам играть только на
&amp;ldquo;прибыльных&amp;rdquo; бандитах (exploitation phase).&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;bandits.jpg&#34; width=&#34;672&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;В классической постановке MAB-проблемы игрок за один шаг взаимодействует (arm pull)
только с одним бандитом, и на основе полученного результата принимает решение
о выборе бандита для следующего шага. Такая постановка хороша для проведения
A/B тестов, но совершенно не подходит для управления источниками: представьте, если бы
мы запускали трафик на сайт по одному посетителю, и на основе того, сконвертировался
он или нет, решали, из какого источника должен придти следующий посетитель. Выглядит
малореалистично, не так ли?&lt;/p&gt;

&lt;p&gt;В нашем случае:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Игрок не имеет прямого контроля над тем, какой конкретный бандит
будет использоваться на каждом шаге, он может управлять только вероятностью игры на
каждом бандите (вероятности это веса источников $w_i$).&lt;/li&gt;
&lt;li&gt;В каждом раунде происходит множество игр, в MAB терминологии это называется
*batch arm pulls*&lt;sup&gt;&lt;a href=&#34;#jun2016top&#34; id=&#34;jun2016top_t&#34;&gt;[1]&lt;/a&gt;&lt;/sup&gt;. Текущая &amp;ldquo;прибыльность&amp;rdquo; бандитов вычисляется после окончания всего раунда.
В нашем случае один раунд это один день.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Вообще MAB-стратегии очень хорошо изучены, но именно в классическом варианте
проблемы. Проблему в нашей постановке, как это ни удивительно, практически никто не исследовал.
Но ничто не мешает восполнить этот пробел и провести исследования самостоятельно.
Мы адаптируем несколько известных MAB-стратегий к нашей задаче и посмотрим на результаты.&lt;/p&gt;

&lt;h1 id=&#34;successivehalving&#34;&gt;SuccessiveHalving&lt;/h1&gt;

&lt;p&gt;Очень простая стратегия, не требующая сложных вычислений, и отлично подходящая для &amp;ldquo;ручного&amp;rdquo; применения.
Впервые описана в &lt;sup&gt;&lt;a href=&#34;#karnin2013almost&#34; id=&#34;karnin2013almost_t&#34;&gt;[2]&lt;/a&gt;&lt;/sup&gt; под названием SequentalHalving.
Алгоритм:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Так же как и в наивной стратегии, выбираем порог ожидания $M$ визитов&lt;/li&gt;
&lt;li&gt;Когда порог достигнут, отключаем половину источников, которые показали
худшую конверсию, и удваиваем порог.&lt;/li&gt;
&lt;li&gt;Повторяем шаг 2 до тех пор, пока не останется минимально возможное
количество источников (в нашем случае 3 или 4)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Смысл стратегии в том, что по мере отключения явно плохих источников, у нас остаётся
больше ресурсов, чтобы исследовать потенциально хорошие источники.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;succ_halving.png&#34; alt=&#34;Результаты SuccessiveHalving  при оптимальном $M=11$.&#34; width=&#34;710&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Результаты SuccessiveHalving  при оптимальном $M=11$.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
SuccessiveHalving демонстрирует результат примерно в два раза лучше по сравнению с наивной стратегией
 (улучшение &lt;strong&gt;33.7%&lt;/strong&gt; против &lt;strong&gt;16.4%&lt;/strong&gt;). Ухудшение вместо улучшения теперь
 почти не происходит, а финальное улучшение в отдельных случаях достигает 100%,
 т.е. стратегии удаётся выйти на &lt;em&gt;оптимальный&lt;/em&gt; набор источников.
 Ступеньки на графике конверсионности образуются
 после каждого &amp;ldquo;уполовинивания&amp;rdquo; источников.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;succ_halving_w.png&#34; width=&#34;668&#34; /&gt;


&lt;/figure&gt;
Для этой и следующих стратегий будем визуализировать также динамику
изменения весов источников во времени, для трёх случайно выбранных экспериментов.
Видно, что сначала веса распределены поровну (exploration phase), затем
плохие источники постепенно отключаются, и их вес переходит к
перспективным источникам. В районе 40-го дня отключаются все плохие
источники, и происходит переход к чистому exploitation.&lt;/p&gt;

&lt;h1 id=&#34;varepsilon-decreasing&#34;&gt;$\varepsilon$-decreasing&lt;/h1&gt;

&lt;p&gt;Основная идея $\varepsilon$-стратегий это явное разделение ресурсов, отводимых
на exploration и exploitation. Выбирается число $0&amp;lt;\varepsilon&amp;lt;1$, на exploration
отводится доля ресурсов, равная $\varepsilon$, а на exploitation доля $1-\varepsilon$.
Наивная стратегия тоже является $\varepsilon$-стратегией, в терминологии MAB
она называется $\varepsilon$-first, т.к. сначала происходит 100% exploration (до момента
времени $t_M$, когда накопится $M$ визитов),
а затем 100% exploitation (до последнего дня $T$):
$$\varepsilon=\frac{t_M}{T}$$&lt;/p&gt;

&lt;p&gt;Exploration и exploitation могут быть совмещены во времени, т.е.
доля трафика $1-\varepsilon$ отводится &amp;ldquo;хорошим&amp;rdquo; источникам, а остальной трафик
распределяется между всеми другими &amp;ndash; такая стратегия будет называться $\varepsilon$-greedy.
Более оптимальны $\varepsilon$-decreasing стратегии&lt;sup&gt;&lt;a href=&#34;#cesa1998finite&#34; id=&#34;cesa1998finite_t&#34;&gt;[3]&lt;/a&gt;&lt;/sup&gt;,&lt;sup&gt;&lt;a href=&#34;#auer2002finite&#34; id=&#34;auer2002finite_t&#34;&gt;[4]&lt;/a&gt;&lt;/sup&gt;,
в которых $\varepsilon$ изменяется во времени:
сначала больше ресурсов отводится на exploration, но постепенно $\varepsilon$ уменьшается почти до нуля:
$$\varepsilon= \min\left(1, \frac{\varepsilon_0}{t}\right), \;  t \in 1,\dotsc,T$$
где $\varepsilon_0$ это базовое значение $\varepsilon$ (может быть больше единицы).
Для нашей модели оптимум $\varepsilon_0=3.5$:



&lt;figure&gt;

&lt;img src=&#34;e-decreasing.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;
Результаты заметно улучшились по сравнению с SuccessiveHalving (&lt;strong&gt;38.2%&lt;/strong&gt; против &lt;strong&gt;33.7%&lt;/strong&gt;).
Это связано с тем, что $\varepsilon$-decreasing стратегия начинает exploitation уже на третий день и изменяет
веса источников ежедневно, а не скачками.



&lt;figure&gt;

&lt;img src=&#34;e-decreasing-w.png&#34; width=&#34;668&#34; /&gt;


&lt;/figure&gt;
На диаграммах с весами заметно характерное поведение: веса &amp;ldquo;фоновых&amp;rdquo;
источников плавно уменьшаются (градиент от фиолетового к чёрному),
в то же время стратегия пытается выявить лучшие источники, активно
переключаясь между кандидатами в течение первого месяца.&lt;/p&gt;

&lt;h1 id=&#34;softmax&#34;&gt;Softmax&lt;/h1&gt;

&lt;p&gt;В предыдущих стратегиях трафик распределялся поровну между лучшими источниками
(а также между активными &amp;ldquo;худшими&amp;rdquo;). Но почему бы распределять
трафик не поровну, а в соответствии с &amp;ldquo;качеством&amp;rdquo; источника, т.е.
его конверсионностью? Эту идею воплощает softmax стратегия:
$$w_i=\frac{e^{r_i/\tau}}{\sum_i e^{r_i/\tau}}$$
где $r_i$ это наблюдаемая конверсионность источника, $w_i$ &amp;ndash; вес источника.
Правая часть формулы представляет собой &lt;a href=&#34;https://ru.wikipedia.org/wiki/Softmax&#34; target=&#34;_blank&#34;&gt;softmax-функцию&lt;/a&gt;,
аналогичную &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_distribution&#34; target=&#34;_blank&#34;&gt;распределению
Гиббса-Больцмана&lt;/a&gt;,
поэтому компонент $\tau$ называют &lt;em&gt;температурой&lt;/em&gt;. Конечно, никакого отношения
к статистической физике это стратегия не имеет, использование softmax функции это просто удобный
эмпирический способ выразить концепцию &amp;ldquo;источник получает долю трафика, соответствующую его качеству&amp;rdquo;.
Идея такого использования softmax предложена в &lt;sup&gt;&lt;a href=&#34;#luce2012individual&#34; id=&#34;luce2012individual_t&#34;&gt;[5]&lt;/a&gt;&lt;/sup&gt;,
softmax-стратегия в применении к MAB проблемам проанализирована в &lt;sup&gt;&lt;a href=&#34;#cesa1998finite&#34; id=&#34;cesa1998finite_t&#34;&gt;[3]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Температура&amp;rdquo; обычно принимается обратно пропорциональной времени:
$$\tau=\frac{\tau_0}{t}$$



&lt;figure&gt;

&lt;img src=&#34;softmax.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;
Результаты softmax стратегии лучше, чем $\varepsilon$-decreasing (&lt;strong&gt;39.2%&lt;/strong&gt; против &lt;strong&gt;38.2%&lt;/strong&gt;),
т.е. неравномерное распределение весов это работоспособная идея.
Видно, что стратегия пытается начинать активный exploration с первого же дня,
но часто принимает ошибочные решения и в следующие дни конверсия немного падает.



&lt;figure&gt;

&lt;img src=&#34;softmax_w.png&#34; width=&#34;668&#34; /&gt;


&lt;/figure&gt;
Стратегия очень оптимистична по поводу новых кандидатов: при появлении
нового &amp;ldquo;фаворита&amp;rdquo; почти весь вес сразу переносится на него,
но в следующие дни оптимизм уменьшается.&lt;/p&gt;

&lt;h1 id=&#34;байесовские-стратегии&#34;&gt;Байесовские стратегии&lt;/h1&gt;

&lt;p&gt;Всем стратегиям, которые мы уже протестировали, присущ один
недостаток: они сравнивают источники друг с другом только по их наблюдаемой
конверсионности. Такое сравнение предполагает, что наблюдаемая конверсионность содержит
одинаковое количество информации для всех источников, т.е. они стартуют в один и тот
же момент времени, выдают сопоставимое количество трафика, и рекламная
кампания завершается одномоментно, в заранее известный день.&lt;/p&gt;

&lt;p&gt;В самом деле, нет смысла сравнивать конверсионность источника, который только
что стартовал (она скорее всего будет нулевой) с конверсионностью источника,
через который прошло уже несколько тысяч визитов &amp;ndash; такое сравнение сразу
забракует &amp;ldquo;молодой&amp;rdquo; источник и стратегия будет работать неправильно.&lt;/p&gt;

&lt;p&gt;Но условие &amp;ldquo;одновременный старт, сопоставимый трафик, одновременное завершение&amp;rdquo; далеко не всегда
выполнимо. Рекламные кампании могут включать в себя &amp;ldquo;старые&amp;rdquo; источники,
к ним в любой момент могут быть добавлены новые. Также постоянно происходит
ротация источников, неэффективные выводятся из кампании, вместо них добавляются свежие.
Момент завершения рекламной кампании малопредсказуем и зависит от её успешности, финансовых возможностей рекламодателя,
текущих цен на платный трафик, и т.д. Поэтому уже рассмотренные стратегии
будут хорошо работать только в тщательно контролируемых условиях. Для промышленного
применения нужны anytime-стратегии, способные адекватно учитывать уже накопленную
в источниках информацию и стартовать в любой момент, не привязываясь ко времени.&lt;/p&gt;

&lt;p&gt;Очевидно, вместо наблюдаемой конверсионности такие стратегии должны
использовать количество визитов и количество конверсий.
И тут мы возвращаемся к байесовским методам работы с конверсией, описанным
в &lt;a href=&#34;../conversion_numbers&#34;&gt;предыдущей статье&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;credible-bounds-racing&#34;&gt;Credible Bounds Racing&lt;/h2&gt;

&lt;p&gt;Вернёмся немного назад и вспомним картинку, на которой у источников есть
&amp;ldquo;усы&amp;rdquo;. Каждый ус обозначает верхнюю и нижнюю границы credible interval,
например для интервала 90% нижняя граница интерпретируется как
&amp;ldquo;вероятность 5%, что конверсионность будет меньше этой границы&amp;rdquo;,
верхняя соответственно &amp;ldquo;вероятность 5%, что конверсионность окажется больше&amp;rdquo;.



&lt;figure&gt;

&lt;img src=&#34;forestplot.png&#34; width=&#34;302&#34; /&gt;


&lt;/figure&gt;
Эти границы &amp;ndash; всё, что нужно для работы простейшей байесовской стратегии. Алгоритм:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Смотрим, есть ли источники, у которых верхняя граница интервала меньше, чем
нижняя граница любого другого источника (т.е. &amp;ldquo;усы&amp;rdquo; не перекрываются). На приведённой
выше диаграмме это источники A и B.&lt;/li&gt;
&lt;li&gt;Если такие источники-аутсайдеры есть, значит они явно хуже одного из существующих источников, и их можно отключить.&lt;/li&gt;
&lt;li&gt;На следующий день повторяем всё с пункта 1. Если по новым данным
видно, что источник был отключен зря (т.е. его верхняя граница
опять перекрывается со всеми остальными), включаем его обратно.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;При такой стратегии в конце концов останется один источник, у которого не
хватит трафика на весь сайт. Поэтому вводится дополнительное условие на
отключение: отключаемый источник не должен входить в топ &amp;ldquo;самых перспективных&amp;rdquo;.
Топ рассчитывается так: сортируем источники по верхней границе credible interval
(т.е. какую конверсионность они &lt;em&gt;могут&lt;/em&gt; показать), и отбираем их в топ, начиная от самого
перспективного, пока не наберется общий максимальный вес больше единицы, т.е. пока топ не будет
способен сформировать весь трафик для сайта.&lt;/p&gt;

&lt;p&gt;Такой же алгоритм используется при ручном управлении источниками
на основе диаграммы с &amp;ldquo;усами&amp;rdquo;. Собственно, приведённое описание алгоритма это всего
лишь формализация методики ручного управления.&lt;/p&gt;

&lt;p&gt;Осталось только понять, какой должна быть оптимальная ширина credible interval, т.к.
при традиционной ширине 90% все &amp;ldquo;усы&amp;rdquo; будут перекрываться до последнего дня
рекламной кампании &amp;ndash; у нас слишком мало визитов. Выясним оптимум экспериментальным путём:



&lt;figure&gt;

&lt;img src=&#34;cb_racing_opt.png&#34; width=&#34;390&#34; /&gt;


&lt;/figure&gt;
Оптимальный квантиль 0.34, что соответствует ширине интервала всего 32%.



&lt;figure&gt;

&lt;img src=&#34;cb_racing.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;
Результат стратегии чуть хуже (на 0.2%), чем у предыдущего чемпиона (softmax).
Но при этом, как уже говорилось, эта стратегия более применима
для управления реальными источниками.
 


&lt;figure&gt;

&lt;img src=&#34;cb_racing_w.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;
 Видно, что стратегия пытается отобрать оптимальные источники уже на второй день,
 но потом понимает, что поторопилась, и продолжает exploration. Если сделать
 ширину credible interval немного больше, эти артефакты исчезнут.&lt;/p&gt;

&lt;p&gt;Посмотрим, каких результатов можно добиться (и при какой ширине правдоподобного интервала)
в других ситуациях:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Более продолжительная рекламная кампания.&lt;/li&gt;
&lt;li&gt;Больший объем трафика.



&lt;figure&gt;

&lt;img src=&#34;cb_racing_h.png&#34; width=&#34;708&#34; /&gt;


&lt;/figure&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Получились похожие диаграммы. Если пересчитать продолжительность
рекламной кампании на левой диаграмме в эффективное количество визитов
на источник за всё время кампании, получим копию правой диаграммы. Т.е.
результаты стратегии и оптимальный credible interval зависят
только от среднего количества визитов на источник. Если это количество
увеличить до 900 (90 дней x 1000 визитов в день / 100 источников),
 можно получить улучшение &lt;strong&gt;&amp;gt;70%&lt;/strong&gt;,
а при увеличении до 9000 улучшение превышает &lt;strong&gt;90%&lt;/strong&gt;, т.е. близко
к максимально возможной эффективности.&lt;/p&gt;

&lt;h2 id=&#34;probability-matching&#34;&gt;Probability matching&lt;/h2&gt;

&lt;p&gt;Если у нас есть все апостериорные распределения, почему бы не
ответить напрямую на вопрос, который нас на самом деле интересует? Нет, не о смысле
жизни, а более простой:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Какова вероятность того, что $i$-ый источник имеет латентную конверсионность выше, чем все остальные?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Если мы знаем эту вероятность, то логично установить веса источников
пропорциональными ей, и это будет оптимальным решением нашей задачи!
В самом деле, если вероятность нулевая, то источнику
надо дать нулевой вес, если вероятность 100%, то весь вес надо перенести
на этот лучший источник, если вероятности для двух источников 50:50,
надо дать им одинаковые веса по 50% трафика, и т.п. Сопоставление
вероятности оптимальности каждой &amp;ldquo;руке бандита&amp;rdquo; или в нашем случае источнику,
называется Probability Matching.&lt;sup&gt;&lt;a href=&#34;#scott2010modern&#34; id=&#34;scott2010modern_t&#34;&gt;[6]&lt;/a&gt;&lt;/sup&gt;
$$w_i=\Pr(\theta_i=\max\{\theta_1,\dotsc,\theta_N\})$$
где $\theta_i$ это латентная конверсионность $i$-го источника. Это выражение
можно представить, как матожидание индикаторной функции:
$$\mathbb{I}_i(\theta)=\begin{cases}
      1 &amp;amp;\text{if } \theta_i=\max\{\theta_1,\dotsc,\theta_N\}, \\&lt;br /&gt;
      0 &amp;amp;\text{otherwise }
      \end{cases}$$
$$w_i=\mathrm{E}\left(\mathbb{I}_i(\theta)\right)=\int \mathbb{I}_i(\theta)p(\theta)\mathrm{d}\theta$$
Апостериорное распределение переменной $\theta$ мы вычисляем через
сопряжённое бета распределение:
$$p(\theta_i)=\mathrm{Be}(\theta_i|\alpha_i,\beta_i) \\&lt;br /&gt;
\alpha_i=\alpha_{prior} + S_i \\&lt;br /&gt;
\beta_i=\beta_{prior} + N_i - S_i$$
где $S_i$ это количество наблюдаемых успехов (конверсий), $N_i$ - количество
наблюдений, т.е. визитов у $i$-го источника.&lt;/p&gt;

&lt;p&gt;Вероятность того, что значение $\theta_j$ окажется меньше $\theta_i$, задаётся
через кумулятивную функцию бета распределения:
$$\Pr(\theta_j &amp;lt; \theta_i) = \mathrm{Be_{CDF}}(\theta_i|\alpha_j,\beta_j)$$
Скомбинировав всё вместе, получаем:
$$w_i=\int_0^1 \mathrm{Be}(\theta_i|\alpha_i,\beta_i)\prod_{i \ne j} \mathrm{Be_{CDF}}(\theta_i|\alpha_j,\beta_j) \mathrm{d}\theta_i$$
Интеграл легко рассчитывается числовыми методами:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from scipy.stats import beta
from scipy.integrate import quad

def opt_prob(α, β):
    def integrand(θ, i):
        logp = beta.logpdf(θ, α[i], β[i])
        logcdf = beta.logcdf(θ, α, β)
        logcdf[i] = 0
        return np.exp(logcdf.sum() + logp)
    return [quad(lambda θ: integrand(θ, i), 0, 1)[0] for i in range(len(α))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Числовое интегрирование работает достаточно медленно, альтернативный и более универсальный способ
расчёта это сэмплирование из апостериорного распределения. Согласно
закону больших чисел, при большом количестве сэмплов среднее значение
выборки сойдется к матожиданию:
$$w_i=\lim\limits_{M \to \infty}\frac{1}{M} \sum_{m=1}^M \mathbb{I}_i(\theta_m) $$
Т.е. надо взять $M$ сэмплов, и для каждого источника вычислить,
в каком проценте сэмплов его значение конверсионности оказалось максимальным.
На практике достаточно 1K&amp;ndash;8K сэмплов, для ускорения можно проводить
сэмплирование и часть расчёта на GPU.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;mirror_c.jpg&#34; width=&#34;515&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Расчет через сэмплирование для классической MAB-задачи (когда не нужны
веса, а надо просто выбрать следующего бандита) известен, как
Thompson sampling.&lt;sup&gt;&lt;a href=&#34;#russo2018tutorial&#34; id=&#34;russo2018tutorial_t&#34;&gt;[7]&lt;/a&gt;&lt;/sup&gt;
В этом случае достаточно единственного сэмпла,
и для следующего шага выбирается бандит, у которого в сэмпле оказалось максимальное значение вероятности.
Thompson sampling был предложен ещё в 1933 году, но тогда вычисления были
слишком дорогими, метод был забыт, и о нём вспомнили только в конце XX века.
Доказательство его оптимальности было получено в 1997 году.&lt;/p&gt;

&lt;p&gt;Probability matching &amp;ndash; оптимальная стратегия, всегда сходящаяся к выбору
лучших источников. Это схождение происходит достаточно быстро по меркам MAB-стратегий,
но слишком медленно для нас: probability matching не сделает
нулевым вес источника, пока не убедится в том, что он абсолютно безнадёжен.
Но чтобы убедиться в этом, требуется гораздо больше визитов, чем
имеющиеся у нас 90 на источник.&lt;/p&gt;

&lt;p&gt;Вместо выбора гарантированно &lt;em&gt;лучших&lt;/em&gt; источников
в отдалённом будущем, нам нужен возможно не самый оптимальный выбор
просто &lt;em&gt;хороших&lt;/em&gt; источников в течение первых недель работы стратегии. Чтобы
сдвинуть баланс в сторону exploitation, добавим новый параметр $\rho$,
который будем называть жадностью:
$$w&amp;rsquo;_i = w_i^\rho, \; \rho \geq 1 $$
где $w&amp;rsquo;_i$ &amp;ndash; эффективный вес $i$-го источника, который будет использовать
стратегия, $w_i$ &amp;ndash; теоретически оптимальный вес, рассчитанный через
probability matching, $\rho$ &amp;ndash; степень, в которую возводится теоретический
вес. Чем больше $\rho$, тем больше ресурсов переходит к уже известным &amp;ldquo;хорошим&amp;rdquo; источникам
и меньше ресурсов остаётся на exploration. Оптимальная жадность для
стандартных условий эксперимента: $\rho\approx7$. Посмотрим на результаты:&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;prob_match.png&#34; width=&#34;710&#34; /&gt;


&lt;/figure&gt;
У нас новый чемпион! Улучшение &lt;strong&gt;41.1%&lt;/strong&gt; это почти на 2 единицы
больше, чем предыдущий рекорд &lt;strong&gt;39.2%&lt;/strong&gt;. Probability matching
действительно &lt;em&gt;оптимальная&lt;/em&gt; стратегия.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;prob_match_w.png&#34; width=&#34;668&#34; /&gt;


&lt;/figure&gt;
Если стратегия не уверена в своём выборе, она не прекращает exploration
до самого последнего момента, в надежде добраться до истины. Если
выбор очевиден (попались явно хорошие источники), вес переходит к ним.
Exploration выглядит несколько хаотичным из за шума, вносимого
случайным сэмплированием, но на результаты этот шум не влияет.&lt;/p&gt;

&lt;p&gt;Посмотрим, какие результаты покажет probability matching на других
объемах визитов:



&lt;figure&gt;

&lt;img src=&#34;prob_match_v.png&#34; width=&#34;708&#34; /&gt;


&lt;/figure&gt;
Результаты превосходны. Улучшение &amp;gt;80% достигается сразу после
планки в 1000 визитов (т.е. 10 визитов в день на источник), а после
5000 визитов доступно улучшение &amp;gt;90%!&lt;/p&gt;

&lt;p&gt;Ещё очень важная деталь: эффективность стратегии слабо зависит от
выбора гиперпараметра $\rho$. В принципе на всём диапазоне протестированных
объемов одинаковое значение $\rho=3$ показало бы неплохие результаты.
Для сравнения можно посмотреть на аналогичные диаграммы для Credible
Bounds Racing: там правильный выбор ширины интервала играет решающую роль.&lt;/p&gt;

&lt;h2 id=&#34;неточное-априорное-распределение&#34;&gt;Неточное априорное распределение&lt;/h2&gt;

&lt;p&gt;Кроме $\rho$ есть еще один неявный гиперпараметр &amp;ndash; априорное распределение конверсионностей.
Для тестирования стратегий я просто сгенерировал выборку 100 сэмплов из
используемого моделью логнормального
распределения и с помощью MLE вычислил параметры
соответствующего бета распределения. Понятно, что в реальной жизни
сэмплы из распределения латентных конверсионностей взять негде, разве что
попросить их у Господа, поэтому придётся использовать наблюдаемую
конверсионность, что негативно скажется на точности. Как неточное
определение параметров априорного распределения повлияет на результат? Давайте проверим.



&lt;figure&gt;

&lt;img src=&#34;prob_match_priors.png&#34; width=&#34;343&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Для практических расчётов бета распределение удобно параметризовать не через $\alpha$
и $\beta$, а через конверсионность $r$ (играет роль матожидания)
 и эффективное количество визитов $v$ (играет роль дисперсии):
 $$\alpha = rv \\&lt;br /&gt;
 \beta = (1-r)v$$
&amp;ldquo;Правильные&amp;rdquo; значения $\hat{r}$ и $\hat{v}$, полученные через MLE, отображены
на диаграмме зелёным пунктиром. Точка пересечения пунктирных линий &amp;ndash;
параметры априорного распределения, которые использовались во всех предыдущих экспериментах.
Чтобы определить устойчивость стратегии к ошибкам в априорном распределении,
я провёл расчёты в диапазоне конверсионностей $[0.5\hat{r}, 2\hat{r}]$ и диапазоне
количества визитов $[\hat{v}/3,3\hat{v}]$, при объеме 1000 визитов в день.&lt;/p&gt;

&lt;p&gt;На диаграмме видно, что стратегия не чувствительна к ошибкам
в эффективном количестве визитов, область оптимальных результатов
простирается от $v=100$ до $v=1100$. Ошибки в конверсионности более критичны.
На практике затруднения обычно вызывает как раз определение правильного
$v$, а промахнуться в два раза мимо конверсионности довольно сложно. При определении
$v$ лучше ошибиться в сторону меньшего количества визитов (т.е. большей дисперсии):
видно, что область оптимальных результатов расширяется к низу.&lt;/p&gt;

&lt;h2 id=&#34;применимость-для-других-распределений-конверсионности&#34;&gt;Применимость для других распределений конверсионности&lt;/h2&gt;

&lt;p&gt;Посмотрим, как ведёт себя
probability matching при использовании распределений конверсионностей, отличных
от распределения по умолчанию (матожидание конверсии $1.13\%$, $\sigma=0.5$).
Возьмем диапазон средней конверсии от 0.2% до 5% и стандартного отклонения 0.25&amp;ndash;1.5.
Примеры таких распределений приведены ниже:



&lt;figure&gt;

&lt;img src=&#34;conv_samples.png&#34; width=&#34;424&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Стратегия работоспособна во всём диапазоне, но результаты, естественно,
различаются:



&lt;figure&gt;

&lt;img src=&#34;prob_match_%d1%811.png&#34; width=&#34;425&#34; /&gt;


&lt;/figure&gt;
С увеличением средней конверсионности эффективность стратегии растёт,
т.к. появляется больше информации о конверсиях. При увеличении дисперсии
эффективность также растёт, так как увеличивается различие между
&amp;ldquo;плохими&amp;rdquo; и &amp;ldquo;хорошими&amp;rdquo; источниками. Чем больше это различие, тем
раньше можно его выявить, и тем лучше результат. При благоприятных
условиях (высокая конверсионность + высокая дисперсия) стратегия
достигает улучшения &amp;gt;80% (напомню, это результат для 100 визитов в день,
т.е. в среднем один (!) визит на источник). Если стратегия работает с
объемом 1000 визитов в день, результат ещё лучше:



&lt;figure&gt;

&lt;img src=&#34;prob_match_%d1%812.png&#34; width=&#34;425&#34; /&gt;


&lt;/figure&gt;
Белая область на диаграмме &amp;ndash; это улучшение &amp;gt;90%.&lt;/p&gt;

&lt;h2 id=&#34;нестационарная-конверсионность&#34;&gt;Нестационарная конверсионность&lt;/h2&gt;

&lt;p&gt;Во всех предыдущих экспериментах мы исходили из того, что латентная
конверсионность источников, единожды заданная на старте, не меняется в ходе эксперимента.
В реальной жизни этого никто не может обещать. Более того, стабильная
конверсионность источников будет скорее исключением, чем правилом. Сможет ли
probability matching справиться с этим?&lt;/p&gt;

&lt;p&gt;Смоделируем нестационарность как геометрическое случайное блуждание латентной конверсионности
c Гауссовскими приращениями и возвратом к среднему:
$$\theta&amp;rsquo;_0 = \ln(\theta_0) \\&lt;br /&gt;
\mu_t = \eta(\theta&amp;rsquo;_0 - \theta&amp;rsquo;_t) \\&lt;br /&gt;
\theta&amp;rsquo;_{t+1} = \theta&amp;rsquo;_t + \mathcal{N}(\mu_t, \sigma^2) \\&lt;br /&gt;
\theta_{t+1} = \exp(\theta&amp;rsquo;_{t+1}) $$
где $\theta_0$ &amp;ndash; начальная конверсионность, $\eta$ &amp;ndash; скорость возврата
к среднему, $\sigma$ &amp;ndash; волатильность.
 Изменение конверсионности
в ходе эксперимента может выглядеть например так:



&lt;figure&gt;

&lt;img src=&#34;volatility.png&#34; width=&#34;397&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Чтобы стратегия могла адаптироваться к нестационарности, научим её
&amp;ldquo;забывать&amp;rdquo; прошлое. Обычно суммарное количество конверсий к моменту
времени $t$ получается сложением конверсий за все предыдущие дни:
$$c_t = \sum_{i=1}^t c&amp;rsquo;_i$$
где $c&amp;rsquo;_i$ это количество конверсий в день $i$. Теперь модифицируем суммирование,
и будем домножать каждое предыдущее кол-во конверсий на коэффициент затухания, $\gamma$:
$$c_t = \gamma c_{t-1} + c&amp;rsquo;_t, \; \gamma \leq 1 $$
Таким образом на момент времени $t$ от конверсий первого дня
останется часть, пропорциональная $\gamma^t$. Стратегия &lt;em&gt;забудет&lt;/em&gt; часть
отдалённого прошлого. Такая же операция проделывается с визитами, в результате
на выходе получим эффективное количество визитов и конверсий, которое будет
меньше, чем просто сумма.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;prob_match_drift.png&#34; alt=&#34;Результаты работы probability matching c $\gamma=0.8$, $\rho=2.1$,  $N=1000$ (количество визитов в день увеличено, чтобы у стратегии оставались ресурсы на exploration).&#34; width=&#34;710&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Результаты работы probability matching c $\gamma=0.8$, $\rho=2.1$,  $N=1000$ (количество визитов в день увеличено, чтобы у стратегии оставались ресурсы на exploration).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Максимальная конверсионность (зелёный пунктир) постепенно увеличивается,
потому что используются относительные приращения конверсионности,
и для смещения вверх есть больше места, чем для
смещения вниз (снизу конверсионность ограничена нулём). Видно, что график
реальной конверсионности почти успевает за графиком максимальной, т.е.
стратегия видит изменения и адаптируется к ним. Также помогает
относительно небольшое $\rho$, стимулирующее exploration.&lt;/p&gt;

&lt;p&gt;Для сравнения: получено улучшение &lt;strong&gt;65.7%&lt;/strong&gt;, улучшение при работе без затухания ($\gamma=1$)
составит &lt;strong&gt;59%&lt;/strong&gt;, улучшение при тех же условиях в стационарном режиме
(т.е. с неизменной конверсией) &amp;ndash; около &lt;strong&gt;75%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;prob_match_drift_w.png&#34; width=&#34;668&#34; /&gt;


&lt;/figure&gt;
На диаграмме весов видно, что стратегия регулярно переключается
между режимами exploration и exploitation, т.к. конверсионность
источников постепенно изменяется.&lt;/p&gt;

&lt;h1 id=&#34;итоги&#34;&gt;Итоги&lt;/h1&gt;

&lt;p&gt;Если пересчитать результаты работы стратегий в прирост количества конверсий
на сайте, Probability Matching даcт прирост около &lt;strong&gt;33%&lt;/strong&gt; относительно
&lt;em&gt;оптимизированной&lt;/em&gt; наивной стратегии,
CB Racing соответственно немного меньше. Учитывая, что в реальной
жизни источниками часто управляют хаотично и качество такого управления
не дотягивает даже до наивной стратегии, прирост может быть ещё выше.&lt;/p&gt;

&lt;p&gt;У байесовских стратегий хорошие перспективы применения, учитывая то,
что они не обязаны непосредственно управлять источниками. Вычисленное
с помощью probability matching оптимальное количество трафика
для источников можно отображать
в отчётах просто в качестве подсказки &amp;ldquo;как лучше распределить трафик&amp;rdquo;.
Ну а CB Racing это не только стратегия, но и инструкция по использованию
&amp;ldquo;усов&amp;rdquo; &amp;ndash; правдоподобных интервалов конверсионности.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;33.jpg&#34; width=&#34;387&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Probability matching устойчив к ошибкам в подборе гиперпараметров, это тоже важно для практического применения.
Я протестировал
эту стратегию во всех режимах, со всеми возможными ошибками, на которые хватило фантазии,
и ни в одном их них не было серьезных сбоев.&lt;/p&gt;

&lt;p&gt;Probability matching также легко расширяется для использования дополнительной
контекстной информации. В реальной жизни нам известно об источниках и о трафике,
который из них приходит, гораздо больше, чем просто количество конверсий.
Тип источника, рекламная система, поведение посетителей из источника на сайте,
просмотренные страницы, продолжительность визита, и т.д. &amp;ndash; всю эту
информацию можно и нужно использовать для повышения точности прогноза.
Возможности здесь практически безграничны, т.к. можно сэмплировать
конверсионность не только из аналитически заданных распределений, но
и из моделей машинного обучения, включая многослойные нейросети&lt;sup&gt;&lt;a href=&#34;#collier2018deep&#34; id=&#34;collier2018deep_t&#34;&gt;[8]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Естественно, область применения не ограничивается сайтами:
реклама мобильных приложений, игровая реклама, call tracking &amp;ndash; стратегии
могут использоваться везде, где существует выбор между несколькими альтернативными
источниками пользователей/клиентов.&lt;/p&gt;

&lt;p&gt;Код для работы с моделью и стратегиями, описанными в статье, выложен в GitHub:
&lt;a href=&#34;https://github.com/Arturus/conv_simulator&#34; target=&#34;_blank&#34;&gt;https://github.com/Arturus/conv_simulator&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Также в Google Colab доступен готовый для работы Notebook со стратегиями:
&lt;a href=&#34;https://colab.research.google.com/github/Arturus/conv_simulator/blob/master/demo.ipynb&#34; target=&#34;_blank&#34;&gt;https://colab.research.google.com/github/Arturus/conv_simulator/blob/master/demo.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;допущения&#34;&gt;Допущения&lt;/h2&gt;

&lt;p&gt;Чтобы не переусложнять модель и не загромождать текст подробностями,
были сделаны некоторые допущения:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Не учитывается сезонность, т.е. колебания объема трафика и конверсионности между
рабочими и выходными днями.&lt;/li&gt;
&lt;li&gt;Стоимость трафика у всех источников принимается одинаковой. В реальной
жизни для коммерческих источников надо было бы учитывать не кол-во
конверсий на единицу трафика (конверсионность), а кол-во конверсий на
единицу потраченных средств (стоимость конверсии), или прибыль на единицу затрат (ROI).&lt;/li&gt;
&lt;li&gt;Не учитываются отложенные конверсии, т.е. предполагается, что
конверсия совершается в течение суток, между обновлениями весов источников. Для сайтов
с &amp;ldquo;медленной&amp;rdquo; конверсией надо вводить поправки к наблюдаемому
числу конверсий.&lt;/li&gt;
&lt;li&gt;Поддерживаются только модели атрибуции, сопоставляющие
конверсию с единственным источником (т.е. по первому или последнему переходу)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr style=&#34;margin-top: 3em;&#34;&gt;
&lt;h2&gt;Источники&lt;/h2&gt;
&lt;ol&gt;
  

  
  &lt;li id=&#34;jun2016top&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Top arm identification in multi-armed bandits with batch arm pulls.&lt;/span&gt;

     &lt;a href=&#34;#jun2016top_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       K. Jun,
     
       K. Jamieson,
     
       R. Nowak,
     
       X. Zhu,
     
    2016.
    AISTATS.
     (pp. 139-148)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;karnin2013almost&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Almost optimal exploration in multi-armed bandits&lt;/span&gt;

     &lt;a href=&#34;#karnin2013almost_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       Z. Karnin,
     
       T. Koren,
     
       O. Somekh,
     
    2013.
    ICML.
     (pp. 1238-1246)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;cesa1998finite&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Finite-time regret bounds for the multiarmed bandit problem.&lt;/span&gt;

     &lt;a href=&#34;#cesa1998finite_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       N. Cesa-Bianchi,
     
       P. Fischer,
     
    1998.
    ICML.
     (pp. 100-108)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;auer2002finite&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Finite-time analysis of the multiarmed bandit problem&lt;/span&gt;

     &lt;a href=&#34;#auer2002finite_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       P. Auer,
     
       N. Cesa-Bianchi,
     
       P. Fischer,
     
    2002.
    Machine learning.
    47.2-3 (pp. 235-256)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;luce2012individual&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Individual choice behavior: A theoretical analysis&lt;/span&gt;

     &lt;a href=&#34;#luce2012individual_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       R. Luce,
     
    1959.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;scott2010modern&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;A modern bayesian look at the multi-armed bandit&lt;/span&gt;

     &lt;a href=&#34;#scott2010modern_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       S. Scott,
     
    2010.
    Applied Stochastic Models in Business and Industry.
    26.6 (pp. 639-658)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;russo2018tutorial&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;A tutorial on thompson sampling&lt;/span&gt;

     &lt;a href=&#34;#russo2018tutorial_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       D. Russo,
     
       B. Van Roy,
     
       A. Kazerouni,
     
       I. Osband,
     
       Z. Wen,
     
      
     
    2018.
    Foundations and Trends in Machine Learning.
    11.1 (pp. 1-96)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;collier2018deep&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Deep contextual multi-armed bandits&lt;/span&gt;

    [&lt;a href=&#34;https://arxiv.org/abs/1807.09809&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#collier2018deep_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       M. Collier,
     
       H. Llorens,
     
    2018.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Конверсия и data science I. Как увидеть невидимое.</title>
      <link>https://suilin.ru/post/conversion_numbers/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/conversion_numbers/</guid>
      <description>

&lt;h1 id=&#34;введение&#34;&gt;Введение&lt;/h1&gt;

&lt;p&gt;Этой статьей я начинаю цикл о проблемах Интернет-аналитики (аналитики в широком смысле: аналитика
сайтов, аналитика мобильных приложений, игровая аналитика и т.п.) и возможных способах
их решения с позиций современного data science. Над этими проблемами
я начал размышлять давно, ещё когда делал свои первые проекты по
 интернет-аналитике, а потом когда создавал Вебвизор и работал в Яндекс.Метрике.
Но до практического
их решения я добрался только сейчас, когда появилось время на изучение
нужной литературы и расширение своего кругозора. Можно сказать,
что появилась возможность закрыть гештальт, и я рад ей воспользоваться.&lt;/p&gt;

&lt;p&gt;Несмотря на бурное развитие data science и машинного обучения,
современная Интернет-аналитика по большей части осталась такой же, как
и 10 лет назад. Те же отчеты, графики, сегментирование, и т.д.
По большому счёту это просто удобный интерфейс к табличной базе данных с функциями
агрегации и выборки нужных сегментов. Всё осмысление выдаваемых отчётами
цифр ложится на плечи пользователя. Между тем, напрямую
глядя на эти цифры, можно получить на так уж много полезной информации, и гораздо больше бесполезной.
Я попробую описать методики
работы с этими сырыми данными (и по возможности создать практическую реализацию этих методик),
чтобы повысить value современной интернет-аналитики, сделать её более
полезной и информативной.&lt;/p&gt;

&lt;h2 id=&#34;терминология&#34;&gt;Терминология&lt;/h2&gt;

&lt;p&gt;В русской терминологии существует некоторая путаница, словом &amp;ldquo;конверсия&amp;rdquo;
может обозначаться и событие совершения посетителем целевого действия (достижение цели)
и соотношение кол-ва визитов, где произошла конверсия, к общему количеству визитов (conversion rate, коэффициент конверсии, показатель конверсии).
Чтобы избежать неоднозначности, я буду использовать для соотношения
$\frac{визиты\,с\,конверсией}{все\,визиты}$ термин
&lt;strong&gt;конверсионность&lt;/strong&gt;, как меру способности входящего трафика генерировать конверсии,
а для события достижения цели - слово &lt;strong&gt;конверсия&lt;/strong&gt; (т.е. так же,
как его использует Google Analytics).&lt;/p&gt;

&lt;h1 id=&#34;проблема-измерения-конверсионности&#34;&gt;Проблема измерения конверсионности&lt;/h1&gt;

&lt;p&gt;Итак, конверсионность, или коэффициент конверсии (в терминах Google Analytics),
или просто конверсия (в терминах Яндекс.Метрики), это отношение количества
&amp;ldquo;конверсионных&amp;rdquo;, достигших цели визитов к общему количеству визитов. Вместо визитов
могут быть посетители, конверсией может быть покупка товара в магазине,
регистрация, приобретение артефакта в игре, суть от этого не меняется.
Конверсионность обычно рассматривают в разрезе источников трафика, собственно
это основной показатель качества источника. Чем выше конверсионность,
тем лучше. Если бы конверсионность измерялась на основе непрерывных физических величин,
таких как градусы или километры, на этом можно было бы завершить рассказ
и перейти к более интересным темам. Проблема в том, что и конверсии и визиты,
на основе которых измеряют конверсионность, это дискретные события, а конверсии
еще и относительно редкие.&lt;/p&gt;

&lt;p&gt;Если есть два визита и ноль достижений цели, это означает, что конверсионность источника равна нулю и его надо немедленно отключить?
А если один визит и одно достижение цели, то конверсионность 100%? Мы нашли курицу, несущую золотые яйца, и надо отключить все другие источники, кроме этого?
С точки зрения здравого смысла - нет, надо еще подождать, и тогда станет более ясно.
Но сколько подождать? Если есть 10 визитов, уже можно верить конверсионности, или ещё нет?
Или надо сто визитов? Тысячу визитов?&lt;/p&gt;

&lt;p&gt;Увы, современная интернет-аналитика
не дает сколько нибудь внятного ответа на такой простой вопрос, возникающий каждый раз
 оценке конверсионности недавно появившихся или просто слабых источников трафика, а также
 при подключении новых рекламных объявлений. Будем искать ответ сами.&lt;/p&gt;

&lt;h1 id=&#34;конверсионность-как-вероятность&#34;&gt;Конверсионность как вероятность&lt;/h1&gt;

&lt;p&gt;Посмотрим на конверсии, как на вероятностный процесс. Будем считать каждый визит
на сайт попыткой конверсии. Т.е. посетитель, приходящий на сайт, может сконвертироваться, а может и не сконвертироваться.
Вероятность конверсии это и есть конверсионность, принимающая значения от 0 до 1 (или от 0% до 100%).
Вероятность успехов (конверсий) в ходе независимых попыток описывается простейшим
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%91%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D0%BB%D0%B8&#34; target=&#34;_blank&#34;&gt;распределением Бернулли&lt;/a&gt;:
 $$\Pr(X=1)=p=1-\Pr(X=0)$$
 где $X$ это случайная величина, которая
принимает одно из двух значений: $1$ (успех, произошла конверсия) или $0$ (неудача, конверсии не было), а $p$ это вероятность
успеха, она же конверсионность в нашем случае. Выражение $\Pr(X=y)$ означает &lt;em&gt;&amp;ldquo;вероятность того, что
 случайная величина $X$ окажется равна $y$&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Обычно интересны не единичные события, а то, что происходит на некотором
интервале времени. Допустим, на сайт пришло 1000 посетителей из источника с конверсионностью 0.01, или 1%
(сейчас предположим, что нам известно точное значение конверсионности).
Сколько посетителей сконвертируется? Ровно 10? Совсем не обязательно. Количество
успехов из $n$ попыток, с вероятностью успеха каждой попытки, равной $p$, описывается &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%91%D0%B8%D0%BD%D0%BE%D0%BC%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;биномиальным
распределением&lt;/a&gt;:
$X \sim \mathrm{Bin}(n,p)$.
Соответственно, вероятность получить $k$ конверсий описывается функцией
вероятности биномиального распределения:
$$\Pr(X=k)={n\choose k}p^k(1-p)^{n-k}$$
$$\binom n k =\frac{n!}{k!(n-k)!}$$
Второе выражение это &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%91%D0%B8%D0%BD%D0%BE%D0%BC%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82&#34; target=&#34;_blank&#34;&gt;биномиальный коэффициент&lt;/a&gt;, который и дал название распределению.
Выглядит страшновато, но ничего сверхъестественного в этих формулах нет. Представим,
что каждая попытка это подбрасывание монетки, успех если выпал орёл, неуспех если решка.
Если подбрасываем монетку один раз, имеем 50% вероятность успеха.
Если подбрасываем два раза, есть четыре варианта развития событий:
&lt;code&gt;орёл-орёл&lt;/code&gt;, &lt;code&gt;орёл-решка&lt;/code&gt;, &lt;code&gt;решка-орёл&lt;/code&gt;, &lt;code&gt;решка-решка&lt;/code&gt;. То есть 25% вероятность
получить два успеха, 50% вероятность получить один успех и 25% вероятность получить ноль успехов.
Можно продолжать и дальше, но то, что мы посчитали, это уже биномиальное распределение
для $n=2$ и $p=0.5$!&lt;/p&gt;

&lt;p&gt;Формула биномиального распределения просто задаёт обобщенный способ такого расчёта,
для любых $n$ и $p$.&lt;/p&gt;

&lt;p&gt;Распределение Бернулли это частный случай биномиального распределения для единичной попытки ($n=1$),
поэтому эти распределения взаимозаменяемы. Обычно распределение Бернулли используют, когда рассматривают
единичные события, а биномиальное - когда рассматривают сразу много событий, например все визиты за день.&lt;/p&gt;

&lt;p&gt;Вернёмся к конверсиям, и построим график вероятности количества конверсий для наших 1000 визитов,
т.е. $n=1000$ и $p=0.01$ (конверсионность 1%)&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;binomial_1000.png&#34; width=&#34;415&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Вероятность получить ровно 10 конверсий - всего 12.6%! Примерно такая же
вероятность получить 9 конверсий, а вообще можно получить любое число
от 0 до примерно 21.&lt;/p&gt;

&lt;p&gt;В чём же дело?&lt;/p&gt;

&lt;h1 id=&#34;конверсионность-как-матожидание&#34;&gt;Конверсионность как матожидание&lt;/h1&gt;

&lt;p&gt;На самом деле существует две конверсионности. Первая это параметр $p$ в распределении
Бернулли и биномиальном распределении, определяющий вероятность
конверсии посетителя. Назовём его истинной конверсионностью. Для распределения
Бернулли, описывающего и конверсию и бросание монеты, параметр $p$ равен
 математическому ожиданию случайной переменной:
$$p=\mathrm{E}(X)$$
Вторая конверсионность - это реализованная (наблюдаемая) конверсионность, т.е. результаты &amp;ldquo;подбрасывания монетки&amp;rdquo;.
Та самая цифра, которую мы видим в отчётах Метрики и Google Analytics под названием &lt;em&gt;&amp;ldquo;конверсия&amp;rdquo;&lt;/em&gt; и &lt;em&gt;&amp;ldquo;коэффициент конверсии&amp;rdquo;&lt;/em&gt;,
являющаяся средним количеством конверсий на $n$ визитов. Визуализацию того, как
эта конверсионность соотносится с первой, истинной конверсионностью,
мы только что сделали (биномиальное распределение).&lt;/p&gt;

&lt;p&gt;На практике математическое ожидание обычно считают эквивалентным среднему значению,
но это не совсем так. Согласно &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%B8%D1%85_%D1%87%D0%B8%D1%81%D0%B5%D0%BB&#34; target=&#34;_blank&#34;&gt;Закону больших чисел&lt;/a&gt;
среднее значение стремится к матожиданию на бесконечно большой выборке:
$$\frac{1}{n} \sum_{i=1}^n X_i \rightarrow \mathrm{E}[ X ], \quad n \rightarrow \infty$$&lt;/p&gt;

&lt;p&gt;Но закон больших чисел ничего не говорит о том, &lt;strong&gt;насколько быстро&lt;/strong&gt; среднее
сойдется к матожиданию. Для многих распределений, встречающихся в жизни,
по выборке из 100 значений уже можно вполне точно оценить матожидание.
Но интернет-аналитике здесь не повезло, распределение Бернулли с маленьким
значением $p$ это как раз пример &lt;strong&gt;медленного&lt;/strong&gt; схождения. Проиллюстрируем
это графически.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;obs_conv.png&#34; width=&#34;495&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;На диаграмме изображена наблюдаемая конверсионность для 10 источников трафика
с одинаковой истинной конверсионностью, равной 1%. Каждый источник случайным образом берет
(сэмплирует) конверсии из распределения Бернулли с $p=0.01$. Это модель
&lt;em&gt;реальных&lt;/em&gt; источников и &lt;em&gt;реальных&lt;/em&gt; цифр в отчётах.
Как видно, до 100
визитов значения конверсионности почти случайны, к 1000 визитов они наконец
группируются вокруг истинного значения, но только к 100 000 визитов ($10^5$)
наблюдается более-менее полное схождение к точному значению. Повторюсь,
&lt;strong&gt;сто тысяч визитов&lt;/strong&gt;. Многие ли дожидаютя 100К визитов, прежде чем оценить конверсионность?&lt;/p&gt;

&lt;p&gt;В реальности всё ещё хуже, т.к. за время, которое пройдет до
набирания 100К визитов, свойства источника скорее всего успеют измениться
(например там появится другая аудитория), и истинная конверсионность тоже изменится.
Узнать правду (истинную конверсионность) в такой ситуации вообще невозможно.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;obs_conv2.png&#34; width=&#34;495&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Если конверсионность менее одного процента (на последнем графике смоделирована
истинная конверсионность 0.2%), ситуация усугубляется. Стабилизация значений наступает только
к 10000 визитов.&lt;/p&gt;

&lt;p&gt;Чтобы нагляднее оценить неточность наблюдаемой конверсионности, построим графики биномиальных
распределений при фиксированном $p$ и разных $n$ в сравнимом масштабе по осям X и Y.
Для этого разделим значения по оси X (кол-во конверсий)
на количество визитов $n$, и получим как раз возможные наблюдаемые значения конверсионности
при заданном истинном значении. Значения по оси Y наоборот умножим на кол-во визитов:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;binomial_many.png&#34; width=&#34;484&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Все графики построены для истинной конверсионности 1%. Видно, что когда есть только 100 визитов,
у нас примерно одинаковые шансы наблюдать конверсионность и 0% и 1%, а также ощутимая
возможность увидеть конверсионность 2%, 3% и более. Для 1000 визитов значения уже
явно сгруппированы вокруг истинной конверсионности (но пока в диапазоне плюс-минус километр),
 ну а для 10000 визитов уже есть
хороший шанс увидеть относительно точное значение истинной конверсионности.&lt;/p&gt;

&lt;p&gt;Итак, биномиальное распределение говорит нам, какие наблюдаемые значения
конверсионности можно получить при заданном истинном значении. Но на практике
обычно требуется решение обратной задачи: есть наблюдаемые (неточные) значения конверсионности,
на основе которых надо оценить истинную (точную) конверсионность.
Эту задача наиболее удобно решается с помощью Байесовского подхода.&lt;/p&gt;

&lt;h1 id=&#34;конверсионность-как-апостериорное-распределение&#34;&gt;Конверсионность как апостериорное распределение&lt;/h1&gt;

&lt;p&gt;Давайте отложим в сторону теорию вероятностей, и посмотрим, как работают
с конверсией обычные люди. Представим, что есть человек, профессионально занимающийся
закупками трафика и ведением рекламных кампаний (назовём его Трафик-менеджер), у
него есть история предыдущих объявлений в Директе, средняя наблюдаемая конверсия там в районе 2%.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Подключается новое объявление и набирает два визита и ноль достижений цели. Какая у него конверсия?
Трафик менеджер скажет, что пока у нас маловато данных, чтобы делать какие то выводы,
но скорее всего конверсия будет в районе 1-5%, потому что у всех других объявлений
она была в этом диапазоне. Но точно не 50%, и вероятно не 0%, несмотря на то,
что прямо сейчас она равна нулю.&lt;/li&gt;
&lt;li&gt;Затем объявление набирает 10 визитов из них 2 конверсии. Трафик-менеджер скажет,
что это объявление имеет шансы быть хорошим, конверсия точно не ноль, но скорее всего и не 20%,
потому что такая конверсия выглядит нереалистично, более реально иметь конверсию 2-6%.&lt;/li&gt;
&lt;li&gt;Объявление набирает 100 визитов, из них 4 конверсии. Трафик-менеджер скажет, что
конверсия объявления в районе 3-5%, но надо подождать ещё, цифра может измениться.&lt;/li&gt;
&lt;li&gt;И наконец, когда объявление наберет 1000 визитов из них 35 конверсий, трафик-менеджер скажет,
что достаточно уверен в конверсии около 3.5%. Но последняя цифра скорее всего ещё изменится,
и будет например 3.3% или 3.7%.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Рассуждения нашего трафик-менеджера это не что иное, как &lt;em&gt;байесовский подход&lt;/em&gt; к решению проблемы,
применяемый на интуитивном уровне! В чём он заключается?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Есть вероятностная модель, содержащая ненаблюдаемые напрямую параметры (их
называют латентными, или &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D1%80%D1%8B%D1%82%D0%B0%D1%8F_%D0%BF%D0%B5%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F&#34; target=&#34;_blank&#34;&gt;скрытыми переменными&lt;/a&gt;)
и описание, как эти параметры связаны с наблюдаемыми величинами.
В нашем случае единственный параметр это $p$, вероятность конверсии. Наблюдаемые величины
это количество событий &lt;code&gt;визит&lt;/code&gt; и &lt;code&gt;конверсия&lt;/code&gt;, они связаны с $p$ биномиальным распределением.
Задача - на основе данных о наблюдаемых величинах получить оценку значений ненаблюдаемых (латентных) параметров.
Оценка всегда имеет вид распределения вероятностей.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Есть некоторая информация о диапазоне возможных значений параметров и
приблизительной вероятности того или иного значения, основанная на предыдущем опыте.
Если мы попросим нашего трафик-менеджера визуализировать его представления о том, какой вообще может быть
конверсионность, он нарисует примерно такой график:&lt;img src=&#39;trafman_prior.png&#39; width=&#39;419&#39;&gt;
Эта информация называется &lt;em&gt;априорное распределение&lt;/em&gt;. Априорное распределение, как правило,
имеет большую дисперсию (график &amp;ldquo;размазан&amp;rdquo; вдоль оси X) и таким образом выражает низкую уверенность в конкретных значениях:
конверсионность может быть и 1%, и 2%, и 3%, и больше.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Через модель прогоняются данные в виде наблюдаемых значений, и априорное распределение модифицируется
таким образом, чтобы лучше соответствовать данным (т.е. чтобы повысить &lt;a href=&#34;https://en.wikipedia.org/wiki/Likelihood_function&#34; target=&#34;_blank&#34;&gt;правдоподобие&lt;/a&gt; модели).
Чем больше данных прогоняется через модель, тем сильнее модифицируется априорное распределение.
Получившееся распределение, учитывающее и априорное знание, и реальные наблюдения, называется &lt;em&gt;апостериорное распределение&lt;/em&gt;
(о происхождении терминов &lt;em&gt;априорный&lt;/em&gt; и &lt;em&gt;апостериорный&lt;/em&gt; можно прочитать в &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%90%D0%BF%D1%80%D0%B8%D0%BE%D1%80%D0%B8&#34; target=&#34;_blank&#34;&gt;Википедии&lt;/a&gt;).
Если все данные недоступны сразу (в нашем случае рекламная кампания идет в реальном времени и
данные поступают порциями, например раз в день), то апостериорное распределение, полученное
после одной порции данных, может использоваться как априорное для следующей порции, и т.д.
Это соответствует тому, как наш трафик-менеджер постепенно уточняет свою оценку по мере поступления данных.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Полученное в итоге апостериорное распределение и является оптимальной оценкой того,
какие значения могут быть у параметров модели. Обычно апостериорное распределение имеет более узкую и вытянутую
форму по сравнению с априорным (пример - приведённый выше график биномиального распределения для 10000 визитов), отражая
рост уверенности в конкретном диапазоне значений параметра. Если данных много,
то форма апостериорного распределения практически не зависит от формы априорного. Если наоборот мало,
то апостериорное распределение будет близко к априорному.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Собственно, это вся суть байесовского подхода к анализу данных. Процесс получения апостериорного распределения
называют еще байесовским выводом (bayesian inference).
Осталось только выразить всё это в виде формул, чтобы можно было автоматически
рассчитывать параметры конверсионности, не прибегая к интуиции
трафик-менеджера.&lt;/p&gt;

&lt;h1 id=&#34;теорема-байеса&#34;&gt;Теорема Байеса&lt;/h1&gt;

&lt;p&gt;Я намеренно описал байесовский подход на практических примерах, а
не на основе теоремы Байеса, т.к. для полного понимания этой теоремы
нужно приличное знание тервера, которое есть не у всех потенциальных
читателей этой статьи. А если оно есть, то и теорема Байеса таким
читателям вероятно знакома. Но на всякий случай кратко изложу:&lt;/p&gt;

&lt;p&gt;$$\Pr(\theta|y)=\frac{\Pr(y|\theta)\Pr(\theta)}{\Pr(y)}$$&lt;/p&gt;

&lt;p&gt;Это теорема Байеса. На самом деле её в таком виде сформулировал не Байес, а Пьер Симон Лаплас.
Сам Байес понятия не имел ни о &amp;ldquo;теореме Байеса&amp;rdquo; ни &amp;ldquo;байесовских методах&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Томас Байес был священником и при своей жизни опубликовал единственную
работу по математике в поддержку матанализа, изобретенного Ньютоном.
 Эссе Байеса по условным вероятностям, которому
 сам автор видимо не придавал большого значения и не был уверен в правильности своих выводов,
  обнаружил после смерти Байеса (1761 год) его друг, Ричард Прайс. Прайс,
  тоже увлечённый математикой священник, настолько
 вдохновился этим эссе, что объявил его &lt;em&gt;теорией, подтверждающей существование Бога&lt;/em&gt;, основательно
 подредактировал спорные места, исправил ошибки, и представил публике.
 Тем временем Лаплас, который был настоящим &lt;del&gt;сварщиком&lt;/del&gt; математиком (и атеистом),
  независимо открыл и опубликовал примерно то же, о чём писал Байес,
 а также то, о чём Байес не писал, включая ту самую теорему, известную нам как &amp;ldquo;теорема Байеса&amp;rdquo;. Но видимо Прайс был
&lt;del&gt;хорошим продажником&lt;/del&gt; очень убедителен, именно Байеса с его подачи стали считать одним из основателей теории вероятностей, а самого Прайса
  за популяризацию идей Байеса приняли в Королевское Научное Общество.&lt;/p&gt;

&lt;p&gt;Этим всё не закончилось. В 1922 году Рональд Фишер (отец современной статистики, который в одно лицо придумал основную её часть,
 а заодно и современную генетику) заявил, что
 &amp;ldquo;&lt;em&gt;теория обратной вероятности&lt;/em&gt; (так он называл работы Байеса и Лапласа) &lt;em&gt;основана на ошибке и должна быть полностью отвергнута&lt;/em&gt;&amp;rdquo;.
В кругу Байеса сказали бы, что Фишер объявил эту теорию ересью.
 Авторитет Фишера был огромен, спорить с ним было трудно, и &amp;ldquo;теория обратной вероятности&amp;rdquo; была надолго отправлена в архив.&lt;/p&gt;

&lt;p&gt;Только в начале XXI века байесовские методы вновь вошли в моду и приобрели практическое
значение в связи с развитием вычислительной техники, позволившей решать
задачи байесовского вывода числовыми методами, а не аналитическими (аналитические
решения известны только для ограниченного круга задач). Тем не менее мейнстримовая
статистика продолжает пользоваться методами Фишера, а байесовские методы популярны в основном у
&lt;del&gt;хипстеров&lt;/del&gt; data scientist-ов.&lt;/p&gt;

&lt;p&gt;Теперь - что означают все эти буквы в формуле. Символом $\theta$ принято обозначать
параметры модели, которые мы хотим найти (в нашем случае истинную конверсионность,
в контексте Байесовских моделей будем называть её также &lt;em&gt;латентной конверсионностью&lt;/em&gt;).
$y$ это данные, наши наблюдаемые величины &lt;code&gt;визиты&lt;/code&gt; и &lt;code&gt;конверсии&lt;/code&gt;.
&lt;img src=&#39;bayes.png&#39; width=&#39;265&#39;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\Pr(\theta|y)$ это вероятность параметров при наблюдаемых данных - апостериорное
распределение, которое мы хотим получить&lt;/li&gt;
&lt;li&gt;$\Pr(y|\theta)$ это то, насколько хорошо параметры модели согласуются с
наблюдаемыми данными, или правдоподобие (likelihood)&lt;/li&gt;
&lt;li&gt;$\Pr(\theta)$ это вероятность иметь именно такие параметры или
&amp;ldquo;на что обычно похожи параметры&amp;rdquo; - априорное распределение&lt;/li&gt;
&lt;li&gt;$\Pr(y)$ это фактор, нужный для того, чтобы справа получилась корректная вероятность,
т.е. всё просуммировалось в единицу. Обычно все проблемы вызывает
именно этот компонент, т.к. для его вычисления требуется взять интеграл: $\Pr(y)=\int\Pr(y|\theta)\Pr(\theta)d\theta$,
который аналитически доступен далеко не всегда. Но в нашем
случае проблем не будет.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;бета-биномиальная-модель&#34;&gt;Бета-биномиальная модель&lt;/h1&gt;

&lt;p&gt;Перейдем наконец к делу. До того, чтобы сделать полноценную модель, остался
один шаг: выбрать априорное распределение. Для работы с биномиальным
распределением, описывающим генерацию данных, обычно используют бета распределение, которое
является сопряженным с биномиальным. &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%BF%D1%80%D1%8F%D0%B6%D1%91%D0%BD%D0%BD%D0%BE%D0%B5_%D0%B0%D0%BF%D1%80%D0%B8%D0%BE%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;Сопряженное&lt;/a&gt;
 означает, что при использовании бета
распределения как априорного, апостериорное тоже будет бета распределением,
 это удобно.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%91%D0%B5%D1%82%D0%B0-%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;Бета распределение&lt;/a&gt;
непрерывное (в отличие от дискретного биномиального),
определено на интервале $[0,1]$
и задаётся двумя параметрами, $\alpha$ и $\beta$. Формула плотности вероятности:
$$P(x)=\frac{x^{\alpha-1}(1-x)^{\beta-1}} {\mathrm{B}(\alpha,\beta)}$$
где $\mathrm{B}(\cdot,\cdot)$ это &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%91%D0%B5%D1%82%D0%B0-%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F&#34; target=&#34;_blank&#34;&gt;бета-функция&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Полный вывод бета-биномиальной модели тоже не привожу, его можно найти
его можно найти на &lt;a href=&#34;https://en.wikipedia.org/wiki/Conjugate_prior#Example&#34; target=&#34;_blank&#34;&gt;Википедии&lt;/a&gt; и практически в любом курсе по теории вероятностей.
В контексте работы с конверсией нам интересен конечный результат,
а он очень простой. Если априорное бета распределение было задано параметрами
$\alpha$ и $\beta$, и в наблюдаемых данных было $s$ успехов и $f$ неуспехов,
т.е. всего $s+f$ попыток, то апостериорное распределение будет
бета распределением с параметрами $\alpha&amp;rsquo;=\alpha+s$ и $\beta&amp;rsquo;=\beta+f$.
$$Posterior(x;s,f) = \frac{x^{\alpha-1+s}(1-x)^{\beta-1+f}} {\mathrm{B}(\alpha+s,\beta+f)} $$&lt;/p&gt;

&lt;p&gt;То есть всё, что надо сделать, чтобы получить апостериорное распределение в бета-биномиальной модели,
это сложить две пары чисел, априорные параметры и наблюдения.
Параметры $\alpha$ и $\beta$ можно интерпретировать, как количество успехов и
неуспехов, заложенное в модель в качестве априорной информации.&lt;/p&gt;

&lt;p&gt;Давайте посмотрим, как всё это работает на конкретных примерах. Допустим,
истинная конверсионность источника равна 2%.
В первом примере
предположим, что у нас нет никакой априорной информации, например мы измеряем
конверсионность на только что открывшемся сайте и до этого никогда не работали
с сайтами похожей тематики. В этом случае в качестве
параметров априорного распределения можно взять $\alpha=1, \, \beta=1$. Априорное
распределение будет просто горизонтальной линией, отражением факта, что мы
ничего не знаем о том, какая может быть конверсия. В байесовских
терминах это будет называться &lt;em&gt;неинформативное&lt;/em&gt; априорное распределение.&lt;/p&gt;

&lt;p&gt;Во втором примере предположим, что мы правильно угадали
априорное распределение, соответствующее источнику, и зададим его параметрами
$\alpha=1, \, \beta=49$, это означает 1 успех и 49 неуспехов, т.е.
конверсию 2%, равную конверсии источника.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;posteriors_1.png&#34; width=&#34;712&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;На графиках показаны апостериорные распределения, образовавшиеся
после разного числа наблюдений, т.е. визитов. Пунктирной линией
обозначено матожидание каждого распределения (для бета распределения
$\mathrm{E}[X]=\frac{\alpha}{\alpha + \beta}$).&lt;/p&gt;

&lt;p&gt;В первом примере видно, что при небольшом количестве визитов матожидание
промахивается мимо верного значения, но после примерно 600 визитов
большое количество наблюдаемых данных приводит распределение ближе к истине.
Во втором примере, естественно, матожидание с самого начала правильное
(все пунктирные линии совпали),
меняется только форма распределений по мере накопления наблюдений:
от размазанности вдоль оси X (большой неопределённости),
до острого пика, соответствующего высокой
уверенности в узком диапазоне значений конверсионности.&lt;/p&gt;

&lt;p&gt;Также заметно, что на втором примере распределения для небольшого количества визитов более узкие -
так влияет наличие априорной информации, оно снижает степень неопределённости.&lt;/p&gt;

&lt;p&gt;Визуально форма и положение распределений на первом примере кажутся более &amp;ldquo;правильными&amp;rdquo;, чем на втором,
но это оптический обман, связанный с несимметричностью распределений:
у них длинный хвост справа, поэтому матожидание не совпадает с визуальным
максимумом (эта точка называется &lt;em&gt;&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9C%D0%BE%D0%B4%D0%B0_(%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0)&#34; target=&#34;_blank&#34;&gt;мода&lt;/a&gt;&lt;/em&gt;) распределения.&lt;/p&gt;

&lt;p&gt;Построим еще два примера, на которых смоделируем ситуацию, когда истинная конверсионность
отличается от конверсионности, заданной априорным распределением (т.е. нам попался
нетипичный источник, конверсионность которого отличается от априорной &amp;ldquo;средней по больнице&amp;rdquo;).
Насколько большую ошибку внесёт априорное распределение, и насколько быстро
она исправится?&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;posteriors_2.png&#34; width=&#34;712&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;На первом графике априорное распределение соответствует конверсии 4%,
на втором конверсии 1%. Видно, что ближе к 600-1000 визитов ошибка становится
несущественной.&lt;/p&gt;

&lt;p&gt;Теперь оценим, как априорная информация влияет на сходимость. Построим
такой же график схождения к истинному значению как раньше (истинная конверсионность 1%), но
будем оценивать конверсионность не как отношение конверсий
к визитам, а как матожидание апостериорного распределения, при априорных параметрах
$\alpha=1,\,\beta=99$&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;obs_conv_prior.png&#34; width=&#34;495&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Видно, что стало лучше. Разброд и шатание, которые были раньше до 1000
визитов, ощутимо уменьшились. В целом скорость сходимости не изменилась
(Природу не обманешь), но вот дисперсия при небольшом количестве визитов
теперь находится в пределах разумного.&lt;/p&gt;

&lt;h2 id=&#34;видим-невидимое&#34;&gt;Видим невидимое!&lt;/h2&gt;

&lt;p&gt;Посмотрим, чему мы научились. У источника есть латентная конверсионность,
которую невозможно измерить напрямую. Можно только косвенным образом
оценить её по отношению кол-ва конверсий к кол-ву визитов, но эта оценка
при малом количестве данных работает очень грубо. Байесовский подход
позволил нам глубже заглянуть &amp;ldquo;внутрь&amp;rdquo; источника и увидеть его латентную
конверсионность. Мы видим её немного расплывчато, в виде распределения,
а не в виде точного значения, но всё равно это большой прогресс по сравнению с тем, что было
раньше.&lt;/p&gt;

&lt;p&gt;Более точная оценка конверсионности при малом количестве данных это не главное
из того, что даёт нам Байесовский подход. Основная
ценность в том, что у нас теперь есть
апостериорные распределения, отражающие степень уверенности в результатах измерений.
Для интернет аналитики на самом деле
важно не абсолютное значение конверсионности, а сопоставление источников
друг с другом, чтобы оставить лучшие и отключить худшие. И здесь наличие
апостериорных распределений не просто улучшает ситуацию, но в корне её меняет.&lt;/p&gt;

&lt;h1 id=&#34;сравнение-источников-друг-с-другом&#34;&gt;Сравнение источников друг с другом&lt;/h1&gt;

&lt;p&gt;Давайте посмотрим на источники трафика не со стороны точных значений
их конверсионности, а со стороны понимания, какой источник хуже, какой лучше,
а какой имеет потенциал, но пока себя не проявил.
Насколько вообще различима конверсионность источников при небольшом количестве визитов?
В порядке эксперимента, посмотрите на приведенные ниже диаграммы. На них
изображены апостериорные распределения конверсионности двух источников,
голубого и оранжевого. У какого источника конверсионность больше?&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;l_or_g.png&#34; width=&#34;650&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Если вы думаете, что на обоих конверсионность больше у оранжевого источника,
предлагаю посмотреть ещё на такую диаграмму:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;l_or_g2.png&#34; width=&#34;329&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;А здесь у кого больше?&lt;/p&gt;

&lt;p&gt;Правильный ответ: на первой диаграмме конверсионность источников
&lt;em&gt;статистически различима&lt;/em&gt;. У оранжевого источника она, очевидно, больше.
Для остальных диаграмм правильный ответ &amp;ndash; &lt;strong&gt;неизвестно&lt;/strong&gt;. На второй
диаграмме несколько больше вероятность того, что у оранжевого
конверсия выше, но дать уверенный ответ невозможно, т.к. диапазоны возможных
истинных значений сильно пересекаются. На третьей диаграмме
диапазон возможных истинных значений голубого источника настолько велик, что мы
вообще не можем ничего сказать.&lt;/p&gt;

&lt;p&gt;А теперь представьте цифры конверсионности в отчётах Метрики или Аналитикса.
Конечно все они будут отличаться друг от друга, потому что у этих распределений
разные матожидания, и пользователь будет пребывать в полной уверенности,
что видит явную разницу между конверсионностями (ведь цифры не могут врать).
Увы, могут (мы наглядно в этом убедились на графиках схождения к истинному значению),
 и для небольших источников врут гораздо чаще, чем показывают истину. Во многих
случаях, когда пользователь сравнивает конверсионность источников, он
видит всего лишь миражи, созданные случайным шумом, и делает на их основе ложные выводы.&lt;/p&gt;

&lt;p&gt;Но мы теперь вооружены апостериорными распределениями, и сможем отличить
мираж от реальности! В Байесовской статистике есть понятие &lt;strong&gt;правдоподобный
интервал&lt;/strong&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/Credible_interval&#34; target=&#34;_blank&#34;&gt;credible interval&lt;/a&gt;), которое похоже на хорошо известный
&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%B2%D0%B5%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%B2%D0%B0%D0%BB&#34; target=&#34;_blank&#34;&gt;доверительный интервал&lt;/a&gt;
из традиционной статистики. Проще всего построить его по квантилям апостериорного распределения,
например интервал 95% (т.е. содержащий 95% плотности распределения) будет находиться между квантилями 0.025 и 0.975.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;bci.png&#34; width=&#34;650&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;На диаграмме показан пример таких интервалов. Теперь легко можно
определить, можно ли сравнивать конверсионность источников: можно, если их правдоподобные
интервалы не пересекаются, и нельзя, если пересекаются. Размером правдоподобного интервала
можно регулировать степень нашей уверенности в различии, например интервал 90%
даст нам возможность сравнить друг с другом больше источников, чем интервал 95%,
но при этом будет больше вероятность ошибки. На практике разумно использовать
интервал 90% или 95%.&lt;/p&gt;

&lt;p&gt;Конечно, если потребуется визуально сравнить друг с другом 10 источников, то строить
ради этого 10 наложенных друг на друга диаграмм с распределениями будет неудобно. Для массового сравнения
хорошо подходит тип диаграммы, называемый &lt;strong&gt;forestplot&lt;/strong&gt;. Пример показан ниже.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;forestplot.png&#34; width=&#34;302&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Кружок в центре это матожидание, т.е. ожидаемая в среднем конверсионность по данным
апостериорного распределения, а &amp;ldquo;усы&amp;rdquo; это правдоподобный интервал. Соответственно, можно
сравнивать конверсионность источников, усы которых не пересекаются по вертикали.
Кружок можно рассматривать как текущий &lt;strong&gt;прогноз&lt;/strong&gt; конверсионности, т.е. значение,
к которому будет стремиться конверсионность источника по мере накопления данных.&lt;/p&gt;

&lt;h1 id=&#34;ранжирование-источников&#34;&gt;Ранжирование источников&lt;/h1&gt;

&lt;p&gt;Ранжирование (т.е. сортировка) источников это ещё одна задача, близкая к сравнению.
Наверное все пробовали отсортировать
отчёты Метрики и Аналитикса по конверсионности и все видели, какая бессмысленная ерунда
там всплывает наверх. И здесь тоже приходят на помощь правдоподобные интервалы. Принцип
простой: сортировку по убыванию (т.е. когда нас интересует топ лучших) надо делать по значению нижней,
пессимистической границы интервала, а сортировку по возрастанию (топ худших) наоборот по верхней оптимистической.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;top_best.png&#34; width=&#34;575&#34; /&gt;


&lt;/figure&gt;
На диаграмме показаны ранжированные по убыванию конверсионности источники реального сайта.
Источникам, находящимся вверху диаграммы, можно смело добавлять
трафик, не боясь израсходовать бюджет на псевдотоповые источники, у которых случайно сконвертировался
один визит из нескольких имеющихся.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;top_worst.png&#34; width=&#34;531&#34; /&gt;


&lt;/figure&gt;
 А здесь обратное ранжирование, по худшим источникам. Источники,
 оказавшиеся вверху, можно смело отключать. Для наглядности серыми
 кружками обозначена конверсионность, которая отображалась бы в обычном отчёте.
 Видно, что в топ вышли бы источники с нулем конверсий, которые
 пока просто не успели набрать статистически значимое количество визитов.
 Весь отчёт был бы забит такими &amp;ldquo;нулевыми&amp;rdquo; источниками, и найти среди
 них действительно плохие было бы затруднительно.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;maga.jpg&#34; width=&#34;539&#34; /&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;практические-рекомендации&#34;&gt;Практические рекомендации&lt;/h1&gt;

&lt;p&gt;Самая трудоёмкая часть работы при использовании Байесовского подхода это выбор
априорного распределения для каждой конкретной ситуации.&lt;/p&gt;

&lt;p&gt;В простейшем случае можно просто измерить среднюю конверсию источников
по заданной цели и взять её за основу. Для распределений конверсионности,
встречающихся на сайтах в реальной жизни, неплохо работает такой принцип:
принимаем параметр $\alpha$ (число успехов) равным единице, и задаём параметр $\beta$,
равный числу визитов, необходимому, чтобы получить одну конверсию, минус один успех.&lt;/p&gt;

&lt;p&gt;Например, средняя конверсия 2%, $\alpha=1$, тогда $\beta=50-1=49$. Если параметр
$\beta$ получается меньше чем 30-40, можно принять $\alpha=2$, и соответственно
удвоить значение $\beta$.&lt;/p&gt;

&lt;p&gt;Также надо учитывать релевантность источника цели. Представим, что проходит
рекламная кампания, под которую на сайте есть отдельный лэндинг, и на этом
лендинге определена цель. Понятно, что конверсии по ней будут только у источников,
участвующих в рекламной кампании, а у остальных источников будет нулевая конверсионность.
Если включить в расчёт априорного распределения для этой цели &lt;strong&gt;все источники&lt;/strong&gt;,
а не только участвующие в рекламной кампании, получится некорректный результат.&lt;/p&gt;

&lt;p&gt;Более продвинутые могут использовать расчет параметров по &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D1%80%D0%B0%D0%B2%D0%B4%D0%BE%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D1%8F&#34; target=&#34;_blank&#34;&gt;методу максимального
правдоподобия&lt;/a&gt;
(maximum likelihood estimation, MLE):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import beta
# Конверсионность имеющихся источников (в долях, не в процентах)
conversion_rate = [0.011, 0.008, 0.009, 0.012, 0.01, 0.0082, 0.0095]
# В a и b запишутся параметры alpha и beta для априорного распределения 
a, b, _, _ = beta.fit(conversion_rate, floc=0, fscale=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ещё более продвинутый способ это Байесовское моделирование
с помощью бета-биномиального распределения. Это очень
похоже на нашу модель для источников, только вместо
апостериорного распределения конверсионности каждого отдельного источника
моделируется общее распределение для всех источников одновременно.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pymc3 as pm
with pm.Model() as model:
    cr = pm.Beta(&#39;cr&#39;, alpha=1, beta=1)
    visits = pm.Lognormal(&#39;v&#39;, mu=np.log(100), sd=1)
    a = cr * visits
    b = (1-cr) * visits
    obs = pm.BetaBinomial(&#39;obs&#39;, a, b, n_visits, observed=n_conversions)
    result = pm.find_MAP()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Правда этому методу самому требуется априорное распределение,
 но здесь можно использовать или совсем приблизительные значения,
 посчитанные &amp;ldquo;на глаз&amp;rdquo;, как было описано выше, или неинформативный prior.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Сравнение и ранжирование источников это самые простые применения
Байесовских методов. Разумеется, этим их возможности не ограничиваются.
В &lt;a href=&#34;../conversion_opt&#34;&gt;следующей статье&lt;/a&gt; мы рассмотрим методику оптимального управления
источниками трафика, в основе которой лежат апостериорные распределения.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Эффективное сэмплирование распределений на Python</title>
      <link>https://suilin.ru/post/sampling_performance/</link>
      <pubDate>Mon, 10 Dec 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/sampling_performance/</guid>
      <description>

&lt;p&gt;&lt;style&gt;
th {
    text-align: right;
}
&lt;/style&gt;
В ходе работы над последним проектом мне понадобилось в промышленных
масштабах генерировать сэмплы из бета-распределения. Первое что пришло
в голову, это &lt;code&gt;scipy.stat&lt;/code&gt;, тем более там есть куча дополнительных
возможностей: и CDF, и квантили, и MLE, и всё, что душа пожелает. Но довольно
быстро я понял, что &lt;code&gt;scipy&lt;/code&gt; нетороплив, и генерация нескольких
миллиардов сэмплов затянется на часы, а то и на дни. Стал искать
альтернативные варианты и хочу теперь поделиться найденным.&lt;/p&gt;

&lt;h2 id=&#34;numpy-random&#34;&gt;numpy.random&lt;/h2&gt;

&lt;p&gt;Сначала выяснилось, что &lt;code&gt;numpy.random&lt;/code&gt; умеет генерировать не только
равномерные случайные числа, но и сэмплы из распределений. И делает
это бодрее, чем &lt;code&gt;scipy.stat&lt;/code&gt;, особенно на небольших выборках: по видимому,
у &lt;code&gt;scipy.stat&lt;/code&gt; большие накладные расходы на каждый вызов.&lt;/p&gt;

&lt;h2 id=&#34;mkl-random&#34;&gt;mkl_random&lt;/h2&gt;

&lt;p&gt;Совершенно случайно наткнулся на возможность генерации сэмплов в &lt;a href=&#34;https://software.intel.com/en-us/mkl&#34; target=&#34;_blank&#34;&gt;Intel MKL&lt;/a&gt;.
MKL это вообще такой подводный айсберг, в котором есть много всякого
хорошего, но мало кто об этом знает. Почему то Intel его не продвигает,
или просто не умеет продвигать?&lt;/p&gt;

&lt;p&gt;Ребята крайне серьезно подошли к вопросу генерации сэмплов, для каждого распределения есть &lt;a href=&#34;https://software.intel.com/en-us/mkl-vsperfdata&#34; target=&#34;_blank&#34;&gt;замеры
производительности&lt;/a&gt; при разных значениях параметров распределений и при
разных настройках базового генератора случайных чисел. Алгоритмически там
тоже всё круто, для бета распределения в зависимости от параметров распределения происходит
переключение аж между четырьмя различными алгоритмами генерации сэмплов:
у каждого алгоритма есть своя оптимальная область параметров, в которой
алгоритм даёт наилучшее быстродействие и качество.&lt;/p&gt;

&lt;p&gt;В Питоне генератор сэмплов доступен как библиотека &lt;a href=&#34;https://anaconda.org/anaconda/mkl_random&#34; target=&#34;_blank&#34;&gt;mkl_random&lt;/a&gt;,
причём есть еще &lt;a href=&#34;https://github.com/IntelPython/mkl_random&#34; target=&#34;_blank&#34;&gt;такая инкарнация&lt;/a&gt;, которая при установке
попытается притащить с собой весь дистрибутив Intel Python.
Потом выяснилось, что мне вообще ничего не надо устанавливать, mkl_random по видимому идёт в стандартной
поставке Conda. Ещё mkl_random прорастает в numpy в виде пакета &lt;code&gt;numpy.random_intel&lt;/code&gt;,
но происходит это всегда, или при особом положении звёзд, я так и не понял.&lt;/p&gt;

&lt;p&gt;В плане производительности всё превосходно, по сравнению с &lt;code&gt;numpy.random&lt;/code&gt; разница примерно на порядок.
Собственно, это лучший вариант для генерации сэмплов на CPU. Ещё одна приятная
возможность &amp;ndash; можно менять базовый алгоритм генерации случайных чисел,
есть быстрые генераторы, оптимизированные под процессоры с поддержкой SIMD.
Переключение генератора совмещено с seed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import mkl_random

# SIMD-oriented Fast Mersenne Twister (SFMT)
mkl_random.seed(None, &#39;SFMT19937&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cupy-random&#34;&gt;cupy.random&lt;/h2&gt;

&lt;p&gt;Генерация массива сэмплов это по своей природе очень хорошо параллелизуемая операция,
поэтому я стал искать реализацию для GPU. Реализация нашлась в лице пакета
&lt;a href=&#34;https://cupy.chainer.org&#34; target=&#34;_blank&#34;&gt;cupy&lt;/a&gt; , имплементирующего подмножество функций numpy на GPU.
Подмножество, кстати, весьма впечатляющее, и &lt;code&gt;numpy.random&lt;/code&gt; тоже в него входит
в виде реинкарнации &lt;code&gt;cupy.random&lt;/code&gt;. Хочу выразить респект разработчикам
Chainer/Cupy, которые не стали заново имплементировать функциональность numpy в виде собственного велосипеда с блэкджеком,
как это сделали Tensorflow и Pytorch, а сохранили совместимость с оригинальным numpy на уровне API.&lt;/p&gt;

&lt;p&gt;Скорость работы просто фантастическая. На GTX1080Ti
cupy генерирует более полумиллиарда сэмплов из бета распределения в секунду! Это больше, чем способны
переварить остальные компоненты моего проекта.
Апгрейдом до cupy вопрос скорости генерации сэмплов был полностью закрыт.&lt;/p&gt;

&lt;p&gt;Не обошлось без ложки дёгтя: в cupy некорректно работает обновление
seed после генерации сэмпла. Seed хранится
как 64-битный float (размером до 10&lt;sup&gt;308&lt;/sup&gt;), и выставляется на старте обычно в достаточно большое число.
После генерации сэмпла к нему добавляется целое,
которое этому float-у как слону дробина, и не меняет его ни на единый бит.
В итоге cupy каждый раз выдаёт один и тот же сэмпл. Пришлось выставлять seed
вручную.&lt;/p&gt;

&lt;h2 id=&#34;остальное&#34;&gt;Остальное&lt;/h2&gt;

&lt;p&gt;Из любопытства прогнал через бенчмарк также высокоуровневые библиотеки,
умеющие генерировать сэмплы из распределений: PyMC3, Pytorch и Tensorflow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyMC3&lt;/strong&gt; показал стабильно худшие результаты среди всего, что я тестировал.
Как это ни печально, библиотека, основная работа которой состоит в
сэмплировании, делает это хуже всех остальных. Видимо сказывается
возраст Theano, на котором основана вся функциональность PyMC3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PyTorch&lt;/strong&gt; при работе на CPU показал себя средне (на Бета распределении вообще провал),
 но при работе на GPU вполне конкурировал с cupy, иногда отставал, иногда обгонял.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tensorflow&lt;/strong&gt;, как выяснилось, не умеет сэмплировать на GPU. Если запускать
 всё с дефолтными настройками, то он будет делать вид, что работает на GPU,
 но на самом деле будет сэмплировать &lt;em&gt;медленнее&lt;/em&gt; по сравнению с честным
 запуском на CPU, потому что будет гонять данные туда-сюда. При сэмплировании
 на CPU показал себя хорошо, стабильное второе место после mkl_random, но
 большие накладные расходы на каждый вызов.&lt;/p&gt;

&lt;h1 id=&#34;результаты-тестирования&#34;&gt;Результаты тестирования&lt;/h1&gt;

&lt;p&gt;Я тестировал быстродействие сэмплеров на трех распределениях,
с параметрами, близким к используемым в моём проекте:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Бета распределение&lt;/strong&gt; с параметрами $\alpha=1$, $\beta=100$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распределение Пуассона&lt;/strong&gt; с параметром $\lambda=10$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Нормальное распределение&lt;/strong&gt; со стандартными параметрами $\mu=0$, $\sigma=1$,
тестировать с другими смысла нет, т.к. такие сэмплы получаются из стандартных простым
умножением.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Для cupy запускалось два варианта тестов, один с копированием результатов
с GPU на хост, второй без копирования (т.к. результаты могут обрабатываться
и на GPU). Вариант без копирования называется &lt;code&gt;cupy_nht&lt;/code&gt; (No Host Copy).
Разница между ними до 2х и более раз. GPU-тесты PyTorch запускались всегда
с копированием на хост.&lt;/p&gt;

&lt;p&gt;Тесты проводились с переменным размером батча (т.е. сколько сэмплов
генерируется за один вызов), варьирующимся от десяти до миллиона. Быстродействие
всех библиотек сильно зависит от этого размера.&lt;/p&gt;

&lt;p&gt;Результаты приводились к количеству сэмплов, генерируемому в секунду.
Числа в таблицах измеряются в миллионах сэмплов в секунду.&lt;/p&gt;

&lt;h2 id=&#34;бета-распределение&#34;&gt;Бета распределение&lt;/h2&gt;




&lt;figure&gt;

&lt;img src=&#34;beta.png&#34; width=&#34;534&#34; /&gt;


&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;batch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;scipy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;numpy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mkl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy_nht&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_cpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_gpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pymc3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tf_cpu&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;129&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;464&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1668&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;218.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5994&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;252.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;99.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16.0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;21544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;267.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;367.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;123.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;77426&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;254.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;302.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;816.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;129.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;278255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;258.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;456.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1005.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;149.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;266.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;551.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;968.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;159.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;распределение-пуассона&#34;&gt;Распределение Пуассона&lt;/h2&gt;




&lt;figure&gt;

&lt;img src=&#34;poisson.png&#34; width=&#34;534&#34; /&gt;


&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;batch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;scipy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;numpy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mkl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy_nht&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_cpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_gpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pymc3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tf_cpu&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;129&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;60.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;464&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;158.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1668&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;328.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5994&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;489.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;113.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;129.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;21544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;520.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;227.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;365.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;77426&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;507.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;169.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;344.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;732.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;278255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;512.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;209.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;294.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;983.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;531.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;229.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;282.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1346.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;84.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;нормальное-распределение&#34;&gt;Нормальное распределение&lt;/h2&gt;




&lt;figure&gt;

&lt;img src=&#34;normal.png&#34; width=&#34;534&#34; /&gt;


&lt;/figure&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;batch&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;scipy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;numpy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mkl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cupy_nht&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_cpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;torch_gpu&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;pymc3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tf_cpu&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;129&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;61.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;464&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;154.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1668&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;269.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;62.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5994&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;337.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;83.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26.5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;21544&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;364.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;114.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;362.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;83.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;77426&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;350.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;246.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;409.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;119.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;855.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;211.0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;278255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;356.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;596.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1389.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;122.6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1333.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;327.0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;371.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;921.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3521.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;126.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2049.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;366.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Тестирование проводилось на CPU i7-6850K (Broadwell E, 6 cores, 12 threads)
и GPU GTX1080Ti.&lt;/p&gt;

&lt;p&gt;Код, всех тестов доступен в GitHub: &lt;a href=&#34;https://gist.github.com/Arturus/dd5397e5cba3e4c05745811371406d83&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/Arturus/dd5397e5cba3e4c05745811371406d83&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Предсказание времени поста в Instagram</title>
      <link>https://suilin.ru/project/post_time/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/project/post_time/</guid>
      <description>

&lt;p&gt;Аудиторию блогера в Инстаграм можно разделить на активную (те, кто лайкают
посты) и пассивную (те, кто подписан, но не ставит лайки). Для рекламодателей
активная аудитория особенно интересна, поэтому её свойства (соцдем, интересы и т.п.) лучше
определять отдельно. Чтобы выделить активную аудиторию, очевидно, необходимо
получить лайки к постам. Проблема в том, что у крупных блогеров могут быть
миллионы лайков, а Инстаграм за один запрос отдаёт весьма ограниченное количество &lt;em&gt;последних&lt;/em&gt; лайков,
поэтому получение &lt;em&gt;всех&lt;/em&gt; лайков от постов &lt;em&gt;всех&lt;/em&gt; блогеров технически затруднительно.&lt;/p&gt;

&lt;p&gt;Разумная альтернатива это сэмплирование лайков. Если взять, например сэмпл в 10000 лайков,
то свойства аудитории, рассчитанные по этому сэмплу, будут не сильно
отличаться от свойств аудитории, посчитанных по миллиону лайков. Но для этого
сэмплирование должно быть &lt;em&gt;равномерным&lt;/em&gt;, то есть у всех &amp;ldquo;лайкеров&amp;rdquo; должны
быть примерно одинаковые шансы попасть в сэмпл. Равномерный сэмпл можно получить, если забирать последние лайки поста
 через некоторые интервалы времени.
Но здесь возникает другая проблема:
пост собирает основную часть лайков в первые несколько часов своей жизни.
Поэтому если не успеть вовремя обнаружить новый пост, основная масса лайков
уже не будет доступна, можно будет делать сэмпл только из &amp;ldquo;хвоста&amp;rdquo; лайкеров,
что приведёт к искажению статистики. Опоздав всего на час, мы пропустим 20-30% всех лайков.&lt;/p&gt;

&lt;p&gt;Самый простой вариант борьбы с опозданиями, это проверять, не появились ли новые посты, почаще,
 например раз в 10 минут. Но этот же вариант и самый неэкономичный
  по кол-ву запросов в Инстаграм. Блогер вряд ли будет делать посты, когда спит,
возможно у него есть любимое время для постинга, любимые дни недели, и т.п.
Если понять индивидуальное &amp;ldquo;расписание&amp;rdquo; каждого блоггера и подстроить под
него наши проверки, можно будет получить хорошую экономию запросов.&lt;/p&gt;

&lt;h2 id=&#34;как-постят-блоггеры&#34;&gt;Как постят блоггеры&lt;/h2&gt;

&lt;p&gt;Чтобы понять, есть ли у нас шансы выявить поведенческие паттерны блоггеров,
визуализируем некоторые имеющиеся данные.



&lt;figure&gt;

&lt;img src=&#34;image42.png&#34; alt=&#34;Распределение интервалов между постами, I&#34; width=&#34;422&#34; height=&#34;254&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Распределение интервалов между постами, I
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Хорошо видны пики 24-48-72 часа, соответствующие постингу раз в сутки/двое/трое в одно и то же время.
То есть у многих блогеров есть &amp;ldquo;любимое&amp;rdquo; время постинга, и интервал между их постами
получается кратным 24 часам.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;image47.png&#34; alt=&#34;Распределение интервалов между постами, II&#34; width=&#34;410&#34; height=&#34;255&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Распределение интервалов между постами, II
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;На меньшем масштабе заметны пики, соответствующие постингу раз в час или в несколько часов.
Возможно, это следы работы сервисов отложенного постинга.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;image48.png&#34; alt=&#34;Количество постов по времени суток, по всему Инстаграму.&#34; width=&#34;416&#34; height=&#34;258&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Количество постов по времени суток, по всему Инстаграму.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Хорошо заметны внутрисуточные подъемы и спады активности, связанные с естественными суточными ритмами
 &amp;ldquo;сон-бодрствование&amp;rdquo; и разной активностью в рабочее и нерабочее время.&lt;/p&gt;

&lt;p&gt;Аудитория Инстаграма интернациональна и распределена по всему миру по разным часовым поясам.
Чтобы лучше понять природу подъемов и спадов, визуализируем аудиторию из разных стран по отдельности:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;image1.png&#34; alt=&#34;Количество постов по времени суток, раздельно по странам.&#34; width=&#34;616&#34; height=&#34;353&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Количество постов по времени суток, раздельно по странам.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;На графиках отдельных стран суточная цикличность выражена еще более явно.
Также в некоторых странах заметно нарастание активности до начала рабочего дня и после его окончания.&lt;/p&gt;

&lt;h2 id=&#34;детализированные-паттерны-постинга&#34;&gt;Детализированные паттерны постинга&lt;/h2&gt;

&lt;p&gt;Визуальный анализ общей активности показывает, что посты распределяются
внутри суток неравномерно, есть определенные закономерности.
А сейчас от общего анализа перейдем к более подробной детализации, и посмотрим, как делают посты индивидуальные блоггеры.&lt;/p&gt;

&lt;h3 id=&#34;стабильные-блогеры&#34;&gt;Стабильные блогеры&lt;/h3&gt;

&lt;p&gt;Сначала посмотрим на &amp;ldquo;стабильных&amp;rdquo; блогеров, у которых минимальна
дисперсия интервалов между постами. Видно, что такие блогеры делают посты
в основном строго раз в 24 часа, с небольшими отклонениями:



&lt;figure&gt;

&lt;img src=&#34;image19.png&#34; width=&#34;840&#34; height=&#34;441&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;h3 id=&#34;нестабильные-блогеры&#34;&gt;Нестабильные блогеры&lt;/h3&gt;

&lt;p&gt;На следующем графике &amp;ndash; &amp;ldquo;нестабильные&amp;rdquo; блогеры, у которых высокая вариативность
 интервалов между постами. В основном это те, кто делает постинг пакетами по 2-3 поста,
  с большой паузой между пакетами. Видно, как интервал между постами циклически изменяется от секунд до нескольких суток:



&lt;figure&gt;

&lt;img src=&#34;image39.png&#34; width=&#34;840&#34; height=&#34;429&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;h3 id=&#34;быстрые-блоггеры&#34;&gt;Быстрые блоггеры&lt;/h3&gt;

&lt;p&gt;На этом графике блоггеры с минимальным интервалом между постами, т.е.
 создающие много постов в час. В основном это аккаунты магазинов,
 выкладывающие в Instagram свой товарный каталог. Видны регулярные паузы
  между постингами (единичные всплески вверх), по видимому они постят
 только в рабочее время, а в нерабочее и по выходным - отдыхают.



&lt;figure&gt;

&lt;img src=&#34;image30.png&#34; width=&#34;840&#34; height=&#34;429&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;h3 id=&#34;медленные-блоггеры&#34;&gt;Медленные блоггеры&lt;/h3&gt;

&lt;p&gt;На этом графике блоггеры с максимальным интервалом между постами, т.е. с редкими постами.
Каких либо явных закономерностей здесь не прослеживается:



&lt;figure&gt;

&lt;img src=&#34;image9.png&#34; width=&#34;840&#34; height=&#34;434&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Итак, мы увидели, что с одной стороны в posting patterns есть множество явных закономерностей,
 с другой стороны у разных блогеров эти закономерности разные, а у многих вообще отсутствуют.
Создать вручную набор правил, который одинаково хорошо подходил бы любому блогеру,
не представляется возможным. Решить эту задачу за разумное время можно только с помощью машинного обучения.&lt;/p&gt;

&lt;h2 id=&#34;пробуем-machine-learning&#34;&gt;Пробуем machine learning&lt;/h2&gt;

&lt;h3 id=&#34;немного-теории&#34;&gt;Немного теории&lt;/h3&gt;

&lt;p&gt;Первый вопрос, который надо задать при разработке machine learning модели &amp;ndash;
что мы, собственно, хотим предсказать? С первого взгляда кажется, что надо предсказывать время следующего поста.
То есть наша модель должна обучиться функции:
$$t_{i+1} = f(t_0, \dots, t_{i})$$&lt;/p&gt;

&lt;p&gt;где $t_0, \dots, t_{i}$ это история времени предыдущих постов, а $t_{i+1}$ это время следующего поста, которое мы хотим предсказать.&lt;/p&gt;

&lt;p&gt;Но на самом деле это плохая идея. Предсказывать точное время следующего поста это все равно,
 что предсказывать итог бросания монетки. Если монета честная, то мы знаем,
что в среднем итог бросков будет 50/50.
Но предсказать итог каждого конкретного броска невозможно. Та же ситуация с блогером:
 допустим, мы знаем, что он предпочитает делать посты по вечерам.
Но в конкретный день он может не сделать пост, потому что находится в поездке,
или сделать пост позже обычного, потому что у него были другие дела, или наоборот сделать сразу два поста.
Чтобы предсказывать точное время, нам  потребовалось бы знать множество фактов из жизни блогера,
которых у нас нет.&lt;/p&gt;

&lt;p&gt;Поэтому лучше предсказывать вероятность того, что блогер сделает пост в
определённом интервале времени. Вероятность единичных событий, происходящих на фиксированном отрезке времени,
в простейшем случае описывается распределением Пуассона:
$$\Pr(k)=\frac{\lambda^k}{k!}e^{\lambda}$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$k$ &amp;ndash; наблюдаемое количество событий за единицу времени (в нашем случае события это посты, а за единицу времени можно взять, например, сутки);&lt;/li&gt;
&lt;li&gt;$\lambda$ &amp;ndash; математическое ожидание количества событий за единицу времени, т.е. среднее количество событий.
Этот параметр еще называют интенсивностью.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Допустим, блогер делает в среднем 3 поста в сутки ($\lambda=3$).
Тогда, согласно распределению Пуассона, у нас получаются следующие вероятности увидеть $k$ постов за один конкретный день:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;вероятность увидеть 0 постов (т.е. ни одного):
$$\Pr(k=0)=\frac{2^0}{0!}e^{-3}=\frac{1}{1}e^{-3}\approx0.05$$&lt;/li&gt;
&lt;li&gt;вероятность увидеть 1 пост:
$$\Pr(k=1)=\frac{3^1}{1!}e^{-3}=\frac{3}{1}e^{-3}\approx0.15$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;остальные значения выведены на график:



&lt;figure&gt;

&lt;img src=&#34;image44.png&#34; width=&#34;505&#34; height=&#34;278&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Если бы блоггеры всегда делали посты с одинаковой интенсивностью, т.е.
 соблюдалось бы условие $\lambda=const$, то на этом можно было бы закончить наш анализ,
и даже не понадобилось бы машинное обучение.
Но в реальной жизни интенсивность все время изменяется.
Блогер может открыть для себя новую тематику, вдохновиться ею, и начать делать посты в несколько раз чаще, чем обычно.
Или наоборот, блогер может забросить свой аккаунт, переключившись на что-то другое, или уехать в отпуск, и вообще перестать делать посты.
В этом случае интенсивность будет стремиться к нулю. Таким образом, в реальной жизни это не константа, а функция от времени:&lt;/p&gt;

&lt;p&gt;$$\lambda=f(t)$$&lt;/p&gt;

&lt;p&gt;Наша задача &amp;ndash; найти эту функцию, тогда мы сможем оценить вероятность появления нового поста в произвольный момент времени,
и смоделировать поведение блогеров.&lt;/p&gt;

&lt;h2 id=&#34;от-теории-к-практике&#34;&gt;От теории к практике&lt;/h2&gt;

&lt;p&gt;Построим модель, которая обучится целевой функции:
$$\lambda=f(t, h)$$
где $h$ &amp;ndash; история предыдущих постов блогера, $t$ &amp;ndash; относительное время, прошедшее с момента последнего поста.&lt;/p&gt;

&lt;p&gt;Будем предсказывать интенсивность для следующих 24 часов после момента времени $t$.
Для обучения необходимо задать loss function, показывающую, насколько хорошо работают наши предсказания.
Будем использовать negative log likelihood:&lt;/p&gt;

&lt;p&gt;$$loss=-\log(Pr_{\lambda}(X=x\mid t))$$
 это вероятность наблюдения в течение 24 следующих часов после момента времени $t$
количества постов, равного $x$, для распределения Пуассона, характеризуемого параметром $\lambda$.
Количество постов  берется из реальных данных. Чем точнее мы предсказали значение $\lambda$,
 тем выше будет вычисленная по реальному количеству постов вероятность, и тем меньше будет loss.&lt;/p&gt;

&lt;p&gt;Для обучения будем использовать deep learning model, состоящую из &lt;a href=&#34;https://en.wikipedia.org/wiki/Recurrent_neural_network&#34; target=&#34;_blank&#34;&gt;Recurrent Neural Network&lt;/a&gt;
и нескольких fully connected layers.
На входы RNN подаётся история предыдущих постов блогера, на вход fully connected layers блока &amp;ndash; состояния на выходе из RNN и время $t$.&lt;/p&gt;

&lt;p&gt;Посмотрим, что получается в результате обучения, на примерах постов отдельных блоггеров.
Голубым обозначена предсказанная интенсивность, оранжевыми треугольниками и вертикальными линиями - моменты времени, когда происходили посты.
В идеале в те моменты, когда происходил постинг, предсказанная интенсивность
должна быть высокой, а в моменты, когда постов не было &amp;ndash; низкой:&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;image24.png&#34; width=&#34;834&#34; height=&#34;126&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Видно, что при повышении частоты постов предсказанное значение параметра $\lambda$,
 как и ожидалось, растет, а при отсутствии постов падает. Когда блогер перестал делать новые посты,
значение $\lambda$ упало практически до нуля. В момент, когда происходит новый пост после долгой паузы,
значение $\lambda$ скачкообразно повышается, т.к. модель видит, что блогер still alive, и начинает ожидать от него потока новых постов.&lt;/p&gt;

&lt;p&gt;Фактически, мы смоделировали самовозбуждающийся &lt;a href=&#34;https://arxiv.org/pdf/1708.06401.pdf&#34; target=&#34;_blank&#34;&gt;Hawkes process&lt;/a&gt;,
не используя при этом никакой сложной параметризованной математики &amp;ndash;
deep learning модель сама обучилась всем закономерностям!&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;image43.png&#34; width=&#34;840&#34; height=&#34;128&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;На втором примере модель, пронаблюдав историю постов в первые три месяца, выявила &amp;ldquo;любимые дни&amp;rdquo; блогера,
когда он делает посты. Соответственно, предсказанная интенсивность начинает повышаться заранее, в ожидании того,
что блогер скоро сделает пост. Даже когда постов нет, ожидаемая интенсивность все равно циклически повышается и понижается.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;image3.png&#34; width=&#34;840&#34; height=&#34;128&#34; /&gt;


&lt;/figure&gt;
На третьем примере моделью тоже выявлена недельная сезонность, но немного другого вида:
блогер обычно не делает посты по выходным. В течение рабочей недели примерно одинаковая высокая интенсивность,
по выходным интенсивность падает.&lt;/p&gt;

&lt;p&gt;Как видим, наша модель вполне способна выявить поведенческие паттерны блогера,
и предсказывать вероятность постов во времени в соответствии с этими паттернами.&lt;/p&gt;

&lt;h2 id=&#34;модель-для-реального-применения&#34;&gt;Модель для реального применения&lt;/h2&gt;

&lt;p&gt;Модель, предсказывающая интенсивность (т.е. параметр $\lambda$ для распределения Пуассона),
 хороша с теоретической точки зрения. Но она не дает прямого ответа на вопрос,
 который нас интересует на практике: когда надо проверять, не появился ли новый пост?
Чтобы получить ответ на этот вопрос, необходимо проинтегрировать функцию интенсивности:&lt;/p&gt;

&lt;p&gt;$$\Lambda=\int_{t_{0}}^{t&amp;rsquo;} \mathrm\lambda(t)\,\mathrm{d}t$$&lt;/p&gt;

&lt;p&gt;где $t_0$ &amp;ndash; текущее время, $t&amp;rsquo;$ &amp;ndash; предполагаемое время проверки.&lt;/p&gt;

&lt;p&gt;Сначала надо положить $t&amp;rsquo;=t_0$, и потом постепенно увеличивать $t&amp;rsquo;$,
 пока вычисленное значение интеграла  не станет больше некоторого заранее выбранного порога.
Понятно, что вычисление интеграла числовой аппроксимацией по отдельным точкам,
в которых модель рассчитала предсказания для $\lambda$, это неточная, неудобная и ресурсоемкая процедура.
Поэтому для реального использования лучше сделать другую модель, которая сразу будет предлагать время проверки.&lt;/p&gt;

&lt;p&gt;Как и с предыдущей моделью, нам надо определить, какая будет loss function.
Необходимо соблюсти одновременно два условия, противоречащие друг другу:
с одной стороны, надо делать проверки как можно реже, чтобы не генерировать большое кол-во запросов к Instagram.
 С другой стороны, надо минимизировать опоздания, т.е. обнаруживать пост в тот момент, когда он еще не накопил много лайков.
Для минимизации опозданий надо наоборот проверять как можно чаще. Наша loss function должна выражать баланс между этими двумя условиями.&lt;/p&gt;

&lt;p&gt;Для начала разберемся с тем, как оценивать опоздания. Рост количества лайков в посте происходит нелинейно,
вначале он очень быстрый, потом затухает, и к истечению двух дней с момента публикации поста рост практически прекращается.
Прирост лайков в разных постах можно визуализировать таким суммарным графиком:



&lt;figure&gt;

&lt;img src=&#34;image29.png&#34; width=&#34;509&#34; height=&#34;318&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Приблизительно смоделировать зависимость кол-ва лайков от времени можно формулой:
$$likes=1-e^{-\left(\frac{t}{\alpha}\right)^{\beta}}$$
где $t$ &amp;ndash; время в часах;
$\alpha, \beta$ &amp;ndash; подбираемые эмпирические коэффициенты. Для нашего случая $\alpha=4.2, \beta=0.7$.
Смоделированные значения показаны голубой линией на графике.
Используя эту формулу, мы можем оценить опоздание, как долю от общего кол-ва лайков, которую мы пропустили.
Таким образом, опоздание будет величиной в интервале $[0,1]$. Это первая часть нашей loss function.&lt;/p&gt;

&lt;p&gt;Второй частью будет оценка частоты проверок &amp;ndash; инвертированная длина предсказанного интервала между
 текущим временем и временем следующей проверки. Чем длинее эти интервалы, тем меньше будет проверок.
Итоговый loss складывается из потерь по лайкам $loss_{l}$ и потерь по опозданиям $loss_{f}$:&lt;/p&gt;

&lt;p&gt;$$loss =  loss_{l} + k \cdot loss_{f}$$&lt;/p&gt;

&lt;p&gt;где $k$ это коэффициент, регулирующий баланс между частотой проверок и размером опозданий.
Выставляется вручную, исходя из бизнес соображений: какой есть бюджет на проверки и насколько критичны опоздания.
При увеличении коэффициента частота проверок снижается и опоздания растут, при уменьшении наоборот.&lt;/p&gt;

&lt;p&gt;$$loss_{f} = \hat{t}^{-1}$$
$$loss_{l} = \sum_{i=1}^{n} loss_{p_i}$$
$$loss_{p_i} =
  \begin{cases}
    1-\exp\left(-\left(\frac{\hat{t}-t^{post}_i}{\alpha}\right)^{\beta}\right)       &amp;amp; \quad \text{if } \hat{t}-t^{post}_i &amp;gt; 0\\&lt;br /&gt;
    0  &amp;amp; \quad \text{otherwise}
  \end{cases}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\hat{t}$ &amp;ndash; предсказанный временной интервал от текущего времени до следующей проверки.&lt;/li&gt;
&lt;li&gt;$n$ &amp;ndash; количество будущих постов, т.е. постов, будут сделаны после текущего времени.&lt;/li&gt;
&lt;li&gt;$t^{post}_i$ &amp;ndash; временной интервал от текущего времени до $i$-го будущего постоа.&lt;/li&gt;
&lt;li&gt;$loss_{p_i}$ &amp;ndash; потери по лайкам для каждого будущего поста. Потери учитываются,
только если произошло опоздание, т.е. для поста выполнятся условие $\hat{t}-t^{post}_i &amp;gt; 0$, в противном
случае потери равны нулю.&lt;/li&gt;
&lt;li&gt;$\alpha, \beta$ &amp;ndash; коэффициенты для моделирования динамики лайков, о которых говорилось выше.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Во время обучения будем каждый раз выбирать случайным образом момент текущего времени внутри
истории постов аккаунта, и предсказывать время следующей проверки относительно этого текущего времени.
Таким образом, после достаточно длительного обучения, мы попадем почти в каждый интервал между постами,
и сделаем предсказания для множества разных мест в истории.&lt;/p&gt;

&lt;h2 id=&#34;результаты&#34;&gt;Результаты&lt;/h2&gt;

&lt;p&gt;Посмотрим, какие результаты дает обученная модель. Будем визуализировать на timeline одновременно и реальные посты, и точки проверок,
предсказанные нашей моделью. В идеале, если бы нам было известно, когда блогер сделает пост,
каждая проверка была бы сразу после появления нового поста, а в интервалах между постами проверок не было бы вообще,
или они были бы очень редкими. Но этот идеал, как уже обсуждалось в начале этой статьи, недостижим.
С другой стороны, можно вообще не применять никаких моделей, и просто проверять, например раз в час,
не появились ли у блогера новые посты. Тогда проверки будут равномерно распределены в интервалах между постами,
но будет много лишних проверок.&lt;/p&gt;

&lt;p&gt;Проверки, полученные с помощью нашей модели, должны быть где-то между этими двумя крайними случаями.
Т.е. на временных интервалах, где вероятность нового поста мала, проверки должны быть редкими,
а на интервалах, где велика вероятность обнаружить новый пост, проверки должны быть более частыми.&lt;/p&gt;

&lt;p&gt;Будем отображать проверки в виде светло-зеленых маленьких точек, а посты в виде более крупных точек,
окрашенных в цветовой шкале от синего до желтого, в зависимости от того,
какое количество лайков мы пропустили из-за опоздания:



&lt;figure&gt;

&lt;img src=&#34;image49.png&#34; width=&#34;542&#34; height=&#34;68&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Также надо помнить, что лайки нарастают очень быстро, при опоздании всего на 10 минут мы пропускаем 10% лайков,
 при опоздании на полчаса – 20% лайков, при опоздании на час – 30% лайков.&lt;/p&gt;

&lt;p&gt;Посмотрим на предсказания нашей модели для блогеров из тестовой выборки (т.е. блоггеров, которых модель не видела во время обучения):



&lt;figure&gt;

&lt;img src=&#34;image50.png&#34; width=&#34;840&#34; height=&#34;80&#34; /&gt;


&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;На первой диаграмме показана вся история одного блогера.
По оси Y - интервалы между проверками, т.е. чем выше зеленая точка, тем больше интервал.
Видно, что в начале истории модель адаптируется к поведению блогера,
 наблюдаются довольно сильные опоздания, и короткие интервалы между проверками.
Потом, по мере накопления информации о привычках блогера, проверки становятся более редкими и более точными.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;image15.png&#34; width=&#34;840&#34; height=&#34;80&#34; /&gt;


&lt;/figure&gt;
На второй диаграмме показан участок истории того же блогера, когда модель вышла на стабильный режим работы.
Видна явная суточная цикличность, проверки ночью делаются намного реже, проверки днем - чаще.
Время постов совпадает с периодами наиболее частых проверок, т.е. наша модель успешно обучилась.&lt;/p&gt;

&lt;p&gt;Рассмотрим ещё один аккаунт. На первом графике видно, как модель постепенно увеличивает интервал между проверками (зеленые точки уходят выше), если блогер не делает новых постов. Действительно, зачем часто проверять аккаунт, если он ничего не постит?



&lt;figure&gt;

&lt;img src=&#34;image28.png&#34; width=&#34;840&#34; height=&#34;80&#34; /&gt;


&lt;/figure&gt;



&lt;figure&gt;

&lt;img src=&#34;image52.png&#34; width=&#34;840&#34; height=&#34;80&#34; /&gt;


&lt;/figure&gt;
На втором графике видно, как модель адаптируется к изменяющемуся поведению блоггера. Сначала блогер делал один пост в день, и на диаграмме частоты проверок видно явное увеличение частоты в середине дня, когда вероятность поста максимальна. Потом блоггер начинает делать два поста в день, и хорошо видно, как проверки быстро адаптируются к новому поведению: вместо одной впадины, соответствующей уменьшению интервала между проверками в середине дня, мы начинаем наблюдать в правой половине графика &amp;ldquo;площадки&amp;rdquo;, соответствующие равномерно уменьшенному интервалу в течение всего дня.&lt;/p&gt;

&lt;p&gt;Успешная работа модели подтверждается не только визуально, но и цифрами. Если интервалы между проверками задаются нашей моделью, то для получения того же среднего процента пропущенных лайков (~15%) требуется сделать в 2-4 раза меньше проверок по сравнению с baseline. За baseline принимаются равномерные проверки раз в N минут. Если же наоборот зафиксировать кол-во проверок и сравнивать процент пропущенных лайков, то у модели он будет в 1.5-2 раза меньше, чем у baseline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Как добавить колонку к pd.DataFrame</title>
      <link>https://suilin.ru/post/pandas_column/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/post/pandas_column/</guid>
      <description>

&lt;h2 id=&#34;введение&#34;&gt;Введение&lt;/h2&gt;

&lt;p&gt;В Pandas существует по меньшей мере три официальных способа добавить колонку,
не включая экзотических:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Способ №1&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
df = pd.DataFrame(...)

df[&#39;column&#39;] = value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;У этого способа самый простой и очевидный синтаксис, поэтому по умолчанию
обычно используют именно его. Но наверняка каждый, кто работал
с Pandas, получал хотя бы раз в жизни такой неприятный warning при добавлении
колонки:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt;SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Этот warning говорит нам, что существует второй способ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Способ №2&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.loc[:, &#39;column&#39;] = value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Откуда же берется warning в первом способе? Он возникает, когда выполняется
несколько выборок идущих друг за другом, причем на вход следующей выборки
подаются результаты предыдущей выборки. В терминологии Pandas это называется
 &lt;em&gt;chained indexing&lt;/em&gt; и выглядит например так:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Выборка по строкам, потом по колонкам
df[df[&#39;a&#39;] &amp;gt; 5][[&#39;b&#39;, &#39;c&#39;]]

# Выборка по колонкам, потом по строкам
df[[&#39;b&#39;, &#39;c&#39;]][df[&#39;a&#39;] &amp;gt; 5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Если попытаться модифицировать результаты &lt;em&gt;chained indexing&lt;/em&gt; (добавление колонки это тоже
модификация), то Pandas не поймет, что мы хотим - добавить колонку в результаты
выборки, или добавить колонку в исходный фрейм? Оба примера, приведенные ниже, эквивалентны
с точки зрения Pandas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Добавить колонку &#39;b&#39; к исходному фрейму?
df[df[&#39;a&#39;] &amp;gt; 5][&#39;b&#39;] = 42 

# Или к результатам выборки?
df1 = df[df[&#39;a&#39;] &amp;gt; 5]
df1[&#39;b&#39;] = 42
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Чтобы выдать &lt;code&gt;SettingWithCopyWarning&lt;/code&gt;, Pandas запоминает источник данных для каждого
фрейма, &amp;lsquo;родительский&amp;rsquo; фрейм. Если такой источник существует, т.е. фрейм является
подмножеством данных родительского фрейма, то в момент модификации выдается warning.&lt;/p&gt;

&lt;p&gt;Второй способ позволяет нам более явным образом сообщить о своих намерениях, т.к. даёт
совместить выборку и присваивание в одном выражении.&lt;/p&gt;

&lt;p&gt;Более подробно о премудростях &lt;em&gt;chained indexing&lt;/em&gt; можно прочитать в
 &lt;a href=&#34;https://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy&#34; target=&#34;_blank&#34;&gt;документации Pandas&lt;/a&gt; или
в отличной &lt;a href=&#34;https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-part-4-c4216f84d388&#34; target=&#34;_blank&#34;&gt;статье на Medium&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Способ №3&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = df.assign(column=value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Третий способ не модифицирует исходный фрейм, что в зависимости от
ситуации может быть как плюсом (например при повторном выполнении ячейки
в Ipython Notebook), так и минусом, загромождая код присваиваниями.
 Кроме того, при
выполнении &lt;code&gt;assign()&lt;/code&gt; всегда происходит создание нового фрейма,
 что теоретически должно быть немного медленнее, чем предыдущие in-place способы.&lt;/p&gt;

&lt;p&gt;Наличие нескольких способов сделать одну и ту же простую задачу противоречит
известному принципу &lt;a href=&#34;https://www.python.org/dev/peps/pep-0020/&#34; target=&#34;_blank&#34;&gt;Zen of Python&lt;/a&gt; :&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There should be one—and preferably only one—obvious way to do it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;И как оказалось, проблема здесь не только в нарушении философского принципа.&lt;/p&gt;

&lt;h2 id=&#34;проблема&#34;&gt;Проблема&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;meme.jpeg#floatright&#34; width=&#34;350&#34; height=&#34;197&#34;/&gt;
Я давно замечал, что при активном добавлении колонок во фреймы код
начинает работать подозрительно медленно. Под активным я имею в виду
сотни и тысячи добавлений - такие задачи встречаются, когда данные
надо разбить на много мелких групп и работать с каждой отдельно.
Использование третьего способа, через &lt;code&gt;assign()&lt;/code&gt; обычно ускоряло такой код,
хотя теоретически он должен работать медленнее двух первых -
 я списывал это на то, что мне
просто показалось, и никогда не делал точных замеров.&lt;/p&gt;

&lt;p&gt;Но на последней задаче эта проблема проявилась особенно остро. Скрипт,
который должен был пропустить через себя примерно 100Gb данных, и
довольно бодро стартовавший с прогнозом времени выполнения 3 часа,
 был оставлен на ночь. К утру скрипт не выполнил и 20% работы и почти
  завис, потребляя при этом 100% CPU. В чём же дело?&lt;/p&gt;

&lt;p&gt;Запуск скрипта под cProfile выявил занятную картину: основную часть времени
процесс находится внутри метода &lt;code&gt;gc.collect()&lt;/code&gt;, при том, что я нигде не вызываю
сборщик мусора. Такое поведение было бы объяснимым для виртуальной
машины Java, работающей в условиях нехватки памяти, тогда бы сборщик мусора
активировался на каждый чих. Но Python?&lt;/p&gt;

&lt;p&gt;Пришлось поглубже залезть в трассировку вызовов&amp;hellip; и следы привели
к коду, добавляющему колонки в dataframe! Вот фрагмент кода метода
 &lt;code&gt;DataFrame._check_setitem_copy()&lt;/code&gt;, занимающегося проверкой при добавлении колонки,
 и выдающего тот самый &lt;code&gt;SettingWithCopyWarning&lt;/code&gt;, о котором говорилось выше :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if force or self._is_copy:
    value = config.get_option(&#39;mode.chained_assignment&#39;)
    if value is None:
        return
    # see if the copy is not actually referred; if so, then dissolve
    # the copy weakref
    try:
        gc.collect(2)
        if not gc.get_referents(self._is_copy()):
            self._is_copy = None
            return
    except Exception:
        pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;В поле &lt;code&gt;self._is_copy&lt;/code&gt; хранится weak reference на объект, являющийся
&amp;lsquo;родителем&amp;rsquo; текущего фрейма. Чтобы проверить, жив ли еще родитель,
авторы Pandas не нашли лучшего способа, чем просто запустить сборку
 мусора во всей виртуальной машине 😟&lt;/p&gt;

&lt;p&gt;На тестах, когда в памяти не очень много объектов,
 сборка мусора отрабатывает практически мгновенно и код не
  вызывает никаких нареканий.
 В моём же случае в памяти было закешировано около 10Gb данных, и сборщику
 мусора приходилось изрядно потрудиться, обходя все эти объекты при каждом
 добавлении колонки во фрейм.&lt;/p&gt;

&lt;h2 id=&#34;решение&#34;&gt;Решение&lt;/h2&gt;

&lt;p&gt;Решение было простым - раз блок кода со сборкой мусора исполняется только
при наличии &amp;lsquo;родителя&amp;rsquo;, надо сделать так, чтобы родителя не было. Я просто
добавил вызов &lt;code&gt;copy()&lt;/code&gt; перед тем местом, где добавляется колонка. После
&lt;code&gt;copy()&lt;/code&gt; фрейм считается &amp;lsquo;заново рождённым&amp;rsquo;, и не содержит ссылок на
источник данных:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.copy()
df[&#39;column&#39;] = value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Скрипт сразу заработал намного быстрее, и завершился всего за час 🎉&lt;/p&gt;

&lt;p&gt;Отмечу,
что тормоза были одинаковыми при использовании и первого и второго способа добавления
колонки, что неудивительно, т.к. оба они вызывают эту проверку.
А что же третий способ, &lt;code&gt;assign()&lt;/code&gt;? Посмотрим на его код, он очень
 простой (привожу только ветку для Python 3.6):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def assign(self, **kwargs):
  data = self.copy()
  for k, v in kwargs.items():
      data[k] = com._apply_if_callable(v, data)
  return data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Как видно, этот код делает ровно то, что я сделал вручную, ускоряя
свой скрипт: сначала копирует фрейм, а потом добавляет в него
колонки &lt;em&gt;дедовским способом&lt;/em&gt;. Именно поэтому использование &lt;code&gt;assign()&lt;/code&gt;,
вопреки логике, всегда ускоряло работу.&lt;/p&gt;

&lt;h2 id=&#34;выводы&#34;&gt;Выводы&lt;/h2&gt;

&lt;p&gt;Для пользователей Pandas вывод простой: надёжнее всего использовать
&lt;code&gt;assign()&lt;/code&gt;, и со стороны performance, и со стороны того, что он
ограждает пользователя от side effects, связанных с необратимым
изменением фрейма.
Автор &lt;a href=&#34;https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-part-4-c4216f84d388&#34; target=&#34;_blank&#34;&gt;статьи&lt;/a&gt;,
которую я рекомендовал выше, приходит к тем же
выводам. Всегда, когда надо присвоить что-то фрейму, перед присваиванием
лучше вызвать &lt;code&gt;df.copy()&lt;/code&gt;, чтобы избежать неоднозначностей. И, как показывает
мой пример, еще и получить прибавку к скорости!&lt;/p&gt;

&lt;p&gt;А разработчикам Pandas хорошо бы или найти способ отказаться от
 такой brute-force проверки, или хотя бы отразить её наличие в документации.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Определение пола и возраста по фото</title>
      <link>https://suilin.ru/project/gender_age/</link>
      <pubDate>Mon, 27 Aug 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/project/gender_age/</guid>
      <description>

&lt;h2 id=&#34;введение&#34;&gt;Введение&lt;/h2&gt;

&lt;p&gt;Для рекламодателей, использующих influencer marketing, важно понимать,
у какого блогера аудитория наиболее соответствует рекламируемым товарам и услугам.
Довольно бессмысленно рекламировать деловые костюмы девочкам-подросткам, так же как и
продвигать женскую косметику среди аудитории мужчин за 30.&lt;/p&gt;

&lt;p&gt;Но сам Instagram не предоставляет никакой соцдем информации по аудитории блогера,
поэтому рекламодателям приходится работать с блогерами исключительно на основе своих
предположений о составе их аудитории. Единственный способ подтвердить эти
предположения &amp;ndash; просмотреть выборку из фолловеров интересующего блогера и оценить на глаз их возраст и пол.
Это долгая, неинтересная, и немасштабируемая работа, к тому же не совсем
объективная, т.к. разные люди оценят возраст по разному.&lt;/p&gt;

&lt;p&gt;Но почему бы не поручить эту работу машине? Современные технологии Computer Vision
уже достаточно развиты, чтобы справиться с этой задачей без участия человека.&lt;/p&gt;

&lt;h2 id=&#34;постановка-задачи&#34;&gt;Постановка задачи&lt;/h2&gt;

&lt;p&gt;На входе есть аватары Instagram пользователей. Необходимо понять, есть ли на аватаре
люди, и сколько их. Если изображён один человек, предсказать его возраст и пол. При этом
масштаб изображения может быть разным: на аватаре может быть портрет или даже часть лица, может быть
человек в полный рост, может быть что-то промежуточное.
Фото может быть цветным, чёрно-белым, тонированным,
пропущенным через искажающие фильтры, с дорисованными частями и т.п.&lt;/p&gt;

&lt;p&gt;Естественным образом задача разделяется на две основные части:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Обнаружение на фотографии людей&lt;/li&gt;
&lt;li&gt;Определение пола и возраста.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;обнаружение-людей&#34;&gt;Обнаружение людей&lt;/h2&gt;

&lt;p&gt;Пол и возраст определяется прежде всего по лицу, поэтому обнаружением
человека считается наличие лица в кадре. Нога, рука или спина, хотя
и говорят о наличии человека на фото, для решаемой задачи не подходят.
Безусловно, можно обучить computer vision модель, которая будет отличать мужскую ногу от женской,
но на покрытие всех таких ситуаций ушло бы слишком много ресурсов,
при сравнительно небольшой отдаче &amp;ndash; всё таки лица изображены на аватарах гораздо чаще, чем ноги.&lt;/p&gt;

&lt;p&gt;Обнаружение лиц на фото это известная и хорошо проработанная задача computer vision,
поэтому здесь работа свелась к поиску подходящей модели и адаптации её
под требования проекта. Основными требованиями были приемлемая скорость работы
 (аудитория блогеров это сотни миллионов пользователей) и наличие уже
 обученного и готового к использованию варианта (чтобы не тратить время
 на разметку данных и обучение).&lt;/p&gt;

&lt;p&gt;Одним из дополнительных пожеланий было совмещение моделью
двух функций: собственно нахождения лиц на фото, и определения опорных
точек (face landmarks). Опорные точки это обычно центры глаз, кончик носа,
углы губ и другие топологические точки, положение которых на лице может быть
однозначно определено. Зачем они нужны?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C&#34; target=&#34;_blank&#34;&gt;Свёрточные сети&lt;/a&gt;
 (convolutional networks), используемые в моделях
компьютерного зрения, обладают свойством трансляционной инвариантности (translational invariance),
но не обладают (или обладают в ограниченном объеме) свойствами масштабной инвариантности (scale invariance)
и инвариантности к повороту (rotation invariance). Это означает, что если изображение
одного и того же лица смещается на фото в разные положения, то с точки зрения нейросети
это будет то же самое лицо (трансляционная инвариантность). Но если лицо поворачивается
или изменяется его масштаб, для нейросети это будут разные лица. Поэтому, чтобы
облегчить задачу для нейросети, лучше приводить все лица к единому масштабу и
единому вертикальному положению, а для этого нужна привязка к надёжным опорным точкам (например,
считать стандартным размером лица расстояние в 100 пикселей между горизонталью глаз и горизонталью губ,
и приводить все лица к этому масштабу)



&lt;figure&gt;

&lt;img src=&#34;invariance.jpg&#34; alt=&#34;Инвариантность для свёрточных сетей&#34; width=&#34;500&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Инвариантность для свёрточных сетей
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;С учетом перечисленных требований, задача свелась к выбору из двух моделей: MTCNN&lt;sup&gt;&lt;a href=&#34;#zhang2016&#34; id=&#34;zhang2016_t&#34;&gt;[1]&lt;/a&gt;&lt;/sup&gt;
 и детектора из библиотеки &lt;a href=&#34;http://dlib.net/&#34; target=&#34;_blank&#34;&gt;dlib&lt;/a&gt;:
  &lt;a href=&#34;http://blog.dlib.net/2016/10/easily-create-high-quality-object.html&#34; target=&#34;_blank&#34;&gt;CNN Face detector&lt;/a&gt; +
&lt;a href=&#34;http://blog.dlib.net/2017/09/fast-multiclass-object-detection-in.html&#34; target=&#34;_blank&#34;&gt;5-point landmark detector&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;mtcnn-vs-dlib&#34;&gt;MTCNN vs dlib&lt;/h2&gt;

&lt;p&gt;Обе модели продемонстрировали одинаково высокое качество обнаружения лиц,
ошибаясь крайне редко. С определением опорных точек ошибок больше,
особенно на лицах, сильно наклонённых, или повёрнутых ближе к положению &amp;ldquo;профиль&amp;rdquo;, чем к положению &amp;ldquo;анфас&amp;rdquo;.
Но и здесь нет явного лидера. MTCNN более корректно определяет границы лица (bounding box),
 лучше распараллеливает обработку, плюс имеет хороший запас в коде для будущего ускорения, поэтому
для дальнейшей работы была выбрана именно эта библиотека.
Рассматривалась также библиотека &lt;a href=&#34;https://github.com/1adrianb/face-alignment&#34; target=&#34;_blank&#34;&gt;face_alignment&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#bulat2017far&#34; id=&#34;bulat2017far_t&#34;&gt;[2]&lt;/a&gt;&lt;/sup&gt;, она даёт
даже избыточное дял данной задачи качество, но при этом медленно работает, и требует готовых bounding boxes.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;good_detect.jpg&#34; alt=&#34;Примеры определения границ лица и опорных точек библиотеками MTCNN (зелёный прямоугольник, зелёные точки), dlib (красный прямоугольник, фиолетовые точки), face-alignment (белые миниатюрные точки)&#34; width=&#34;668&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Примеры определения границ лица и опорных точек библиотеками MTCNN (зелёный прямоугольник, зелёные точки), dlib (красный прямоугольник, фиолетовые точки), face-alignment (белые миниатюрные точки)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;




&lt;figure&gt;

&lt;img src=&#34;bad_detect.jpg&#34; alt=&#34;Примеры некорректной работы детектора. Первое фото - захват искусственного изображения вместо лица. Второе - некорректная работа dlib (фиолетовые точки) на лице, частично повернутом в анфас&#34; width=&#34;262&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Примеры некорректной работы детектора. Первое фото - захват искусственного изображения вместо лица. Второе - некорректная работа dlib (фиолетовые точки) на лице, частично повернутом в анфас
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;архитектура-mtcnn&#34;&gt;Архитектура MTCNN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#39;mtcnn_nets.png&#39; width=&#34;600&#34;/&gt;&lt;/p&gt;

&lt;p&gt;MTCNN состоит из трех CNN-&amp;ldquo;стадий&amp;rdquo; P-Net, R-Net и O-Net, каждая из которых
уточняет результаты предыдущей ступени, и предварительной стадии построения
&amp;ldquo;пирамиды изображений&amp;rdquo;. Пирамида представляет из себя просто набор уменьшенных копий
входного изображения. MTCNN заранее не знает, в каком масштабе будут лица
на фото, а свёрточные сети, как уже говорилось выше, не инвариантны
к масштабированию. Поэтому приходится готовить несколько
версий входного изображения в разных масштабах, и искать лица на каждой версии.&lt;/p&gt;

&lt;p&gt;Задача стадий - определить границы (bounding box) всех лиц на изображении.
Первая стадий имеет самую простую архитектуру и работает очень быстро,
но при этом генерирует много ошибок первого рода (false positives). Задача
следующих стадий, более сложных и мощных &amp;ndash; выбрать из предложенных
bounding boxes наиболее похожие на правду, уточнить их координаты, и передать дальше.
Последняя ступень также определяет координаты опорных точек внутри
результирующих bounding boxes. В конце каждой стадии сильно пересекающиеся друг с другом bounding boxes
сливаются в один bounding box с помощью алгоритма NMS (non-max suppression):&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;mtcnn_stages.jpg&#34; alt=&#34;Стадии работы MTCNN&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Стадии работы MTCNN
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Трёхстадийная архитектура позволяет MTCNN работать быстро, т.к.
всю черновую работу делает простая первая ступень, а следующие занимаются
только уточнением результатов. Как показали замеры, основное время уходит на построение
пирамиды изображений, а не на собственно работу свёрточных сетей.
Переход на более быстрые алгоритмы уменьшения изображений позволяет
ещё в разы поднять производительность.&lt;/p&gt;

&lt;h3 id=&#34;нормализация-лиц&#34;&gt;Нормализация лиц&lt;/h3&gt;

&lt;p&gt;Перед тем, как отдавать найденные лица в детектор пола/возраста, их необходимо
&lt;em&gt;нормализовать&lt;/em&gt;, т.е. привести лица разного масштаба, по разному повёрнутые и наклонённые,
к одному стандартному виду &amp;ldquo;как на паспорт&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Существует довольно много методик нормализации, от продвинутых, натягивающих
лицо как скин на 3D модель и манипулирующих этой моделью в пространстве,
чтобы она смотрела прямо в объектив и заполняла весь кадр, находясь в его центре, до простых, ограничивающихся приведением лиц
к близкому масштабу. Какую из них выбрать?&lt;/p&gt;

&lt;p&gt;Я исходил из принципа минимального вмешательства: нормализация должна выдавать
только естественную форму лица, которая встречается в природе. Если нормализация
изменяет пропорции раздельно по осям X и Y, изменяет параллельность линий,
или тем более натягивает лицо на 3D сетку, то она может принести больше
вреда, чем пользы. Это подтверждают исследования&lt;sup&gt;&lt;a href=&#34;#ludwiczuk2017&#34; id=&#34;ludwiczuk2017_t&#34;&gt;[3]&lt;/a&gt;&lt;/sup&gt;
результатами которых я активно пользовался при работе над этим проектом.&lt;/p&gt;

&lt;p&gt;Принципу сохранения естественных пропорций лица соответствует
 &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;преобразование подобия&lt;/a&gt; (similarity transform) ,
т.е. набор действий ограничивается сочетанием сдвига, вращения и масштабирования. Это
преобразование является частным случаем аффинного преобразования и описывается
c помощью следующей &lt;a href=&#34;https://en.wikipedia.org/wiki/Transformation_matrix&#34; target=&#34;_blank&#34;&gt;матрицы перехода&lt;/a&gt; :
  $$ \mathbf{A} =
   \begin{bmatrix}
    a_0 &amp;amp; b_0 &amp;amp; a_1 \\&lt;br /&gt;
    b_0 &amp;amp; a_0 &amp;amp; b_1 \\&lt;br /&gt;
    0  &amp;amp; 0  &amp;amp; 1
   \end{bmatrix} $$&lt;/p&gt;

&lt;p&gt;$$\begin{bmatrix}x&amp;rsquo;\\y&amp;rsquo;\\1\end{bmatrix}=\mathbf{A}\begin{bmatrix}x\\y\\1\end{bmatrix}$$&lt;/p&gt;

&lt;p&gt;или в скалярной форме:
 $$ x&amp;rsquo; = a_0x - b_0y + a_1 = sx\cos(\theta) - sy\sin(\theta) + a_1 $$
 $$ y&amp;rsquo; = b_0x + a_0y + b1 = sx\sin(\theta) + sy\cos(\theta) + b_1 $$&lt;/p&gt;

&lt;p&gt;где $(x,y)$ и ($x&amp;rsquo;,y&amp;rsquo;)$ &amp;ndash; координаты исходной и результирующей точек изображения,
$\theta$ &amp;ndash; угол поворота, $s$ &amp;ndash; коэффициент масштабирования.&lt;/p&gt;

&lt;p&gt;Чтобы найти коэффициенты для матрицы перехода, используются опорные точки.
Есть координаты пяти опорных точек  $(\mathbf{s}_1 \dots \mathbf{s}_5),\: \mathbf{s}_i = [x_i, y_i, 1]^\top$ &amp;ldquo;эталонного лица&amp;rdquo;,
 имеющего правильный масштаб,
вертикальную ориентацию и расположенного в центре кадра. И есть координаты
опорных точек на фото $(\mathbf{a_1} \dots \mathbf{a_5})$, которые нашёл MTCNN. Задача &amp;ndash; найти такие коэффициенты
 для $\mathbf{A}$, чтобы после преобразования точки c фото
по возможности совпали с эталонными точками, т.е. минимизировать
расстояние между эталоном и преобразованными точками с фото:
  $$\operatorname*{arg\,min}_\mathbf{A} \frac{1}{n}\sum_{i=1}^n \|\mathbf{s}_i - \mathbf{Aa}_i\|$$
Эта задача решается с помощью метода Umeyama&lt;sup&gt;&lt;a href=&#34;#umeyama1991&#34; id=&#34;umeyama1991_t&#34;&gt;[4]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;normalized.jpg&#34; alt=&#34;Примеры нормализации лиц, слева исходное фото, справа результат.&#34; width=&#34;250&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Примеры нормализации лиц, слева исходное фото, справа результат.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;определение-пола-и-возраста&#34;&gt;Определение пола и возраста&lt;/h2&gt;

&lt;h3 id=&#34;выбор-модели&#34;&gt;Выбор модели&lt;/h3&gt;

&lt;p&gt;Модель для определения пола и возраста подбиралась из примерно тех же критериев,
что и модель для обнаружения лиц:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Приемлемая скорость работы (и для обучения, и для предсказаний).&lt;/li&gt;
&lt;li&gt;Наличие pretrained модели, чтобы не обучать всё с нуля.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Рассматривались три архитектуры: ResNet&lt;sup&gt;&lt;a href=&#34;#he2016&#34; id=&#34;he2016_t&#34;&gt;[5]&lt;/a&gt;&lt;/sup&gt;, NASNet&lt;sup&gt;&lt;a href=&#34;#zoph2017learning&#34; id=&#34;zoph2017learning_t&#34;&gt;[6]&lt;/a&gt;&lt;/sup&gt;, MobileNet&lt;sup&gt;&lt;a href=&#34;#howard2017mobilenets&#34; id=&#34;howard2017mobilenets_t&#34;&gt;[7]&lt;/a&gt;&lt;/sup&gt;.
За baseline был взят хорошо известный ResNet-50.&lt;/p&gt;

&lt;p&gt;NASNet-A-Mobile-224, сконструированный с помощью &amp;ldquo;искусственного интеллекта&amp;rdquo;, содержит примерно в 5 раз параметров,
чем ResNet-50. Но на практике он обучался не в 5 раз быстрее, а даже медленнее, чем ResNet-50.&lt;/p&gt;

&lt;p&gt;Аналогично mobilenet-1.0-224, несмотря на то, что содержит в разы меньше параметров,
чем ResNet-50, на практике обучался со скоростью, сопоставимой с ResNet-50,
показывая при этом худшие результаты предсказаний. Видимо, эта архитектура имеет смысл
именно для мобильных устройств, а не для стационарных GPU.&lt;/p&gt;

&lt;p&gt;В итоге победил ResNet-50, как архитектура, оптимальным образом использующая
стационарный GPU (обучение шло на видеокартах GTX 1080TI).&lt;/p&gt;

&lt;h3 id=&#34;архитектура&#34;&gt;Архитектура&lt;/h3&gt;

&lt;p&gt;На выходе надо получить два предсказания, пол и возраст.
Можно обучить две отдельных модели, но эффективнее использовать &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-task_learning&#34; target=&#34;_blank&#34;&gt;multitask
learning&lt;/a&gt; и получить одну модель, выдающую одновременно два прогноза.
Тем более пол и возраст с точки зрения здравого смысла не являются независимыми переменными, женщины и мужчины
взрослеют и стареют по разному:
$$P_{gender}(D) \neq P_{gender}(D|age)$$
$$P_{age}(D) \neq P_{age}(D|gender)$$
таким образом знание моделью пола будет помогать предсказывать возраст и наоборот.&lt;/p&gt;

&lt;p&gt;Предсказание возраста это регрессия, для регрессионных задач в качестве целевой функции
обычно используют MSE (среднеквадратичную ошибку от предсказываемой переменной). Но в данном случае прямое применение
MSE не оправдано. Во-первых возраст не может быть отрицательным. Во-вторых, MSE предполагает линейную шкалу.
В реальности разница в 5 лет между
2-x и 7-летними детьми это гораздо больше, чем разница в 5 лет между 60-летним и 65-летним человеком,
т.е. человеческий возраст имеет скорее логарифмическую шкалу. Поэтому на входе
в модель возраст трансформировался из исходного возраста $age$:
$$a=\log(age + \gamma)$$
где $\gamma$ это эмпирическая сглаживающая константа, чтобы разница между
новорожденным и взрослым не устремилась в бесконечность. Модель внутри себя
везде использует логарифмический возраст $a$, при выдаче результатов пользователю
он переводится обратно в линейную шкалу:
$$\widehat{age}=\exp(\hat{a}) - \gamma$$&lt;/p&gt;

&lt;p&gt;Так как определение возраста по фото это сложная задача, с которой даже люди
далеко не всегда справляются, хотелось получить на выходе также оценку неопределённости
(uncertainty estimation). Для этого можно предсказывать не точечную
 оценку возраста, а распределение вероятностей, каким он мог бы быть. Для простоты было принято, что ошибка определения
возраста имеет Гауссовское распределение, т.е.
$$\hat{a} \sim \mathcal{N}(a, \sigma^2)$$
где $\hat{a}$ это предсказанный возраст, $a$ - истинный возраст,
$\sigma^2$ &amp;ndash; дисперсия, отражающая степень неуверенности в прогнозе.&lt;/p&gt;

&lt;p&gt;Тогда за целевую функцию можно принять то, насколько хорошо предсказанное распределение
соответствует истинному возрасту, т.е. функцию правдоподобия (likelihood):
$$\mathcal{L}(\hat{a}, \hat{\sigma} | a) = \prod_{i=1}^{n}\frac{1}{\hat{\sigma}_i\sqrt{2\pi}}\exp\left(-\frac{(a_i-\hat{a}_i)^2}{2\hat{\sigma}_i^2}\right)$$
Использование произведения по всем примерам из батча может привести к проблемам с точностью вычислений с плавающей точкой,
поэтому на практике используют негативную логарифмическую функцию правдоподобия (negative log-likelihood):
$$\ell=\log(\mathcal{L})=-\frac{n}{2}\log(2\pi)-\sum_{i=1}^{n}\log\hat{\sigma}_i-\sum_{i=1}^{n}\frac{(a_i-\hat{a}_i)^2}{2\hat{\sigma}_i^2}$$
$$L_{age} = -\ell$$&lt;/p&gt;

&lt;p&gt;Определение пола является бинарной классификацией, в качестве целевой функции в модели используется
стандартная для таких задач перекрестная энтропия:
$$L_{gender} = -{(y\log(p) + (1 - y)\log(1 - p))}$$
где $y$ это бинарная метка класса, например 1 соответствует мужчине, 0 женщине,
а $p$ это предсказанная вероятность принадлежать классу с меткой 1, в данном случае
вероятность быть мужчиной.&lt;/p&gt;

&lt;p&gt;Результирующая целевая функция является просто суммой функций по полу и по возрасту:
$$L=L_{age} + kL_{gender}$$
Абсолютные значения целевых функций находятся в разных шкалах, например
значение $L_{age}$ зависит от того, в каких единицах измеряется возраст.
Чтобы привести их к более-менее одному масштабу, нужен выравнивающий коэффициент
$k$, значение которого подбирается эмпирически.&lt;/p&gt;

&lt;h2 id=&#34;обучающая-выборка&#34;&gt;Обучающая выборка&lt;/h2&gt;

&lt;p&gt;В идеале обучающая выборка должна быть из того же распределения,
что и данные, на которых потом модель будет делать предсказания.
Это означает ручную разметку фотографий из Instagram, т.к. достоверную информацию
о возрасте владельцев аккаунтов взять негде.&lt;/p&gt;

&lt;p&gt;Ручная разметка пола это в принципе посильная задача, а вот с возрастом
всё не так просто. Люди определяют возраст на глаз крайне субъективно,
это означает большую дисперсию и затруднительность контроля результатов.
Для того, чтобы получить от работников надёжный результат, обычно одна и та же
задача даётся трём-пяти людям, и за верный результат принимается большинство голосов.
Работник, часто дающий ошибочные результаты, заменяется. Для возраста
такая схема работать не будет, т.к. чтобы получить надёжную картину
максимума распределения возрастов для каждого фото и отсеять выбросы,
пришлось бы давать оценить каждое фото 10-20 людям, что было бы слишком затратно.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;age_guess.png&#34; alt=&#34;Распределение разницы между средним оценочным возрастом (по &amp;gt; 10 оценкам от разных людей) и реальным возрастом. Источник: AgeGuess database, J. A. Barthold Jones et al, 2018, arXiv:1803.10063&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Распределение разницы между средним оценочным возрастом (по &amp;gt; 10 оценкам от разных людей) и реальным возрастом. Источник: AgeGuess database, J. A. Barthold Jones et al, 2018, arXiv:1803.10063
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Оценка возраста людьми может сильно расходиться с реальным возрастом,
см. приведённый рисунок. Кроме того, оценка возраста зависит еще и от национальности и культурного контекста
оценщика, т.е. пришлось бы набирать распределённую по разным точкам мира команду.&lt;/p&gt;

&lt;p&gt;Поэтому были рассмотрены другие источники. Стандартный dataset,
 используемый в академических кругах для подобных задач, это &lt;a href=&#34;https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/&#34; target=&#34;_blank&#34;&gt;IMDB-WIKI&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#rothe2016dex&#34; id=&#34;rothe2016dex_t&#34;&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Однако, качество разметки этого dataset-а, особенно в данных из IMDB, крайне низкое,
и неприемлемо для проекта, который будет использоваться в production.&lt;/p&gt;

&lt;p&gt;Остальные доступные datasets: (&lt;a href=&#34;https://talhassner.github.io/home/projects/Adience/Adience-data.html&#34; target=&#34;_blank&#34;&gt;Adience&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#eidinger2014age&#34; id=&#34;eidinger2014age_t&#34;&gt;[9]&lt;/a&gt;&lt;/sup&gt;,
 &lt;a href=&#34;https://susanqq.github.io/UTKFace/&#34; target=&#34;_blank&#34;&gt;UTKFace&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#zhifei2017cvpr&#34; id=&#34;zhifei2017cvpr_t&#34;&gt;[10]&lt;/a&gt;&lt;/sup&gt;) слишком малы для полноценного обучения.&lt;/p&gt;

&lt;p&gt;В результате самым продуктивным оказался самостоятельный автоматизированный сбор данных
из социальных сетей и Интернет-сайтов, с последующей модерацией.
Но найти хорошие источники размеченных фотографий для возрастов &amp;lt;17 так и не удалось,
поэтому для этой возрастной категории были выкачаны фото из Инстаграм,
содержащие тэги, указывающие на возраст (обычно такие тэги бывают в фото с дней рождения).
Пол в этих фото размечался вручную, релевантность содержимого фото (что на ней изображены именно дети)
и разметки возраста (возраст с тэга совпадает с визуальным возрастом) контролировались командой модераторов.&lt;/p&gt;

&lt;p&gt;Работа по получению и разметке обучающих данных была самым долгим этапом проекта
и заняла около 4-х месяцев.&lt;/p&gt;

&lt;h2 id=&#34;подготовка-данных&#34;&gt;Подготовка данных&lt;/h2&gt;

&lt;p&gt;В обучающей выборке выравнивалось количество мужчин и женщин
из каждой страны (например в арабских странах и в Индии женщины представлены в онлайне слабо,
и без выравнивания модель могла просто не научиться с ними работать).&lt;/p&gt;

&lt;p&gt;Распределение количества
фото по странам по возможности приводилось в соответствие с распределением
кол-ва аккаунтов по странам в Instagram. При этом странам с преобладающим азиатским или негроидным населением
давался больший вес, чтобы в обучающей выборке не доминировала европеоидная раса
и у модели было достаточно данных, чтобы научиться работать с азиатскими
и негроидными типами лиц.&lt;/p&gt;

&lt;p&gt;Также убирались перекосы по возрастам, чтобы распределение возрастов
было похожим на распределение, заявленное для Instagram в открытых источниках.&lt;/p&gt;

&lt;p&gt;В итоговую выборку попало около 3 млн. фото взрослых и около 300 тыс.
фото детей и подростков.&lt;/p&gt;

&lt;h3 id=&#34;аугментация-данных&#34;&gt;Аугментация данных&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.algorithmia.com/introduction-to-dataset-augmentation-and-expansion/&#34; target=&#34;_blank&#34;&gt;Аугментация&lt;/a&gt;
проводилась по тому же принципу, что и нормализация лиц:
на выходе должны получаться только такие фото, которые могут встретиться
в естественных условиях. Зашумленных, нерезких, тонированных и частично обрезанных
фото было и так достаточно в обучающих данных, при этом качество фото
на аватарах в Instagram в среднем достаточно высокое. Поэтому разновидности
аугментации, &amp;ldquo;ухудшающие&amp;rdquo; фото, не применялись.&lt;/p&gt;

&lt;p&gt;Аугментация свелась к вертикальному зеркалированию (flip), случайному повороту на небольшой угол
и случайному кропу. MTCNN не абсолютно точно и одинаково определяет опорные
 точки на всех фото, от лица к лицу возможны вариации,
два последних вида аугментации как раз учат нейросеть справляться с такими отклонениями.&lt;/p&gt;

&lt;h2 id=&#34;обучение&#34;&gt;Обучение&lt;/h2&gt;

&lt;p&gt;Использовалась модель ResNet-50, предварительно обученная на данных Imagenet.
У неё убирался последний слой, отвечающий за классификацию ImageNet,
и вместо него добавлялся полносвязный (fully connected, FC) слой, генерирующий предсказания для модели
(3 выходных значения: пол, матожидание и дисперсия возраста).&lt;/p&gt;

&lt;p&gt;Обучение проводилось в два этапа, сначала обучался только добавленный
FC слой, затем, когда ошибка переставала уменьшаться, в обучение включалась
вся модель. Transfer learning, т.е. обучение только последнего слоя,
сам по себе давал посредственные результаты: точность определения пола
была не выше 85%. Это объяснимо, т.к. человеческие лица представляют собой
довольно узкий и специфический домен, сильно отличающийся от данных ImageNet, к тому же в ImageNet
не существует классов &amp;ldquo;человек&amp;rdquo; или &amp;ldquo;лицо&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Использовался оптимизатор Nesterov Momentum и cosine learning rate decay с рестартами&lt;sup&gt;&lt;a href=&#34;#loshchilov2016sgdr&#34; id=&#34;loshchilov2016sgdr_t&#34;&gt;[11]&lt;/a&gt;&lt;/sup&gt;
Максимальный и минимальный learning rate подбирался с помощью техники &lt;em&gt;LR range test&lt;/em&gt; &lt;sup&gt;&lt;a href=&#34;#smith2017cyclical&#34; id=&#34;smith2017cyclical_t&#34;&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;lr_test.png&#34; alt=&#34;Один из результатов LR range test, ось Y - loss, ось X - learning rate, логарифмическая шкала&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Один из результатов LR range test, ось Y - loss, ось X - learning rate, логарифмическая шкала
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Один полный прогон обучения занимал около пяти дней.&lt;/p&gt;

&lt;h2 id=&#34;результаты&#34;&gt;Результаты&lt;/h2&gt;

&lt;p&gt;Результаты работы обученной модели сравнивались с результатами 4-х крупных коммерческих
систем Computer Vision: &lt;a href=&#34;https://aws.amazon.com/rekognition/&#34; target=&#34;_blank&#34;&gt;AWS Rekognition&lt;/a&gt;,
 &lt;a href=&#34;https://azure.microsoft.com/services/cognitive-services/face/&#34; target=&#34;_blank&#34;&gt;Microsoft Azure&lt;/a&gt;, &lt;a href=&#34;https://www.faceplusplus.com/attributes/&#34; target=&#34;_blank&#34;&gt;Face++&lt;/a&gt;,
  &lt;a href=&#34;https://clarifai.com/models/demographics-image-recognition-model-c0c0ac362b03416da06ab3fa36fb58e3&#34; target=&#34;_blank&#34;&gt;Clarifai&lt;/a&gt;.
Чтобы сравнение было объективным, замеры проводились не только на собственной
тестовой выборке, но и на дополнительных datasets: &lt;a href=&#34;https://talhassner.github.io/home/projects/Adience/Adience-data.html&#34; target=&#34;_blank&#34;&gt;Adience&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#eidinger2014age&#34; id=&#34;eidinger2014age_t&#34;&gt;[9]&lt;/a&gt;&lt;/sup&gt;
 и &lt;a href=&#34;https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/&#34; target=&#34;_blank&#34;&gt;IMDB-WIKI&lt;/a&gt;&lt;sup&gt;&lt;a href=&#34;#rothe2016dex&#34; id=&#34;rothe2016dex_t&#34;&gt;[8]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Для возраста точность оценивалась по ошибке MAPE (Mean absolute percentage error),
показывающей отклонение в процентах определенного моделью возраста от реального.&lt;/p&gt;

&lt;p&gt;$$MAPE=\frac{100\%}{n}\sum_{i=1}^{n}\left|\frac{age_i - \widehat{age}_i}{age_i}\right|$$&lt;/p&gt;

&lt;p&gt;где $age$ &amp;ndash; истинный возраст, $\widehat{age}$ - предсказанный возраст&lt;/p&gt;

&lt;h3 id=&#34;собственный-dataset&#34;&gt;Собственный dataset&lt;/h3&gt;

&lt;p&gt;Распределение возрастов соответствует естественному распределению в социальных сетях,
откуда были собраны данные.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Gender accuracy, %&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Age MAPE, %&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ours&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;99.3&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;14.4&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Face++&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92.2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;59.3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Clarifai&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;47.7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Azure&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;96.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;34.2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;AWS Rekognition&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91.1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;38.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;imdb-wiki&#34;&gt;IMDB-Wiki&lt;/h3&gt;

&lt;p&gt;Из IMDB-WIKI были отброшены данные IMDB, как содержащие огромное количество
неточностей, в данных WIKI был дополнительно вручную исправлен пол там, где
были явные ошибки.
Для тестов были взяты фото людей в диапазоне возрастов 13-44 года (актуальный диапазон для Instagram).
Также были отброшены фотографии, сделанные до 2005 года, т.к. стилистика этих фото (косметика, причёски) отличается от современной,
и фото такой давности редко встречаются в соцсетях.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Gender accuracy, %&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Age MAPE, %&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ours&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;98.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;14.9&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Face++&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Clarifai&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;91.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;35.9&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Azure&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;89.4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;24.6&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;AWS Rekognition&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;94.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;43.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;adience&#34;&gt;Adience&lt;/h3&gt;

&lt;p&gt;Возраст в Adience указан в виде диапазона а не точного значения, поэтому для него использовались
другие метрики точности:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Age accuracy&lt;/em&gt; &amp;ndash; процент попаданий предсказанного возраста в правильный диапазон.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Age accuracy one-off&lt;/em&gt; &amp;ndash; процент попаданий или в правильный диапазон или в два соседних с ним.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Gender accuracy, %&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Age acc., %&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Age acc. one-off, %&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ours&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;97.7&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;19.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Face++&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;86.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;14.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;65.1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Clarifai&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;84.6&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;25.7&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;78.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Azure&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;94.8&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;36.9&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;strong&gt;89.8&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;AWS Rekognition&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;88.9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;21.3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;В этом тесте точность определения возраста у модели уступает некоторым коммерческим сервисам (на первом месте - Azure).
Объясняется это тем, что Adience это академический dataset, в котором все возраста от 0 до 80 лет присутствуют
в примерно равной пропорции. Модель же обучалась под распределение возрастов,
наблюдающееся в реальной жизни в Instagram и социальных сетях, которое весьма далеко от равномерного (доминирует возраст 18-30).
Соответственно, на равномерном распределении точность модели хуже, т.к. при прочих равных предпочтение отдаётся возрастам в диапазоне 18-30.&lt;/p&gt;

&lt;p&gt;Если бы целью было показать хороший результат именно на Adience,
надо было бы обучить модель на выборке с равномерным сэмплированием по всем возрастам.&lt;/p&gt;

&lt;h2 id=&#34;на-что-смотрит-модель&#34;&gt;На что смотрит модель?&lt;/h2&gt;

&lt;p&gt;Было бы интересно понять, какие области лица играют главную роль при
определении пола/возраста. Большинство традиционных методы выявления областей
внимания для этой модели, к сожалению, не выдают наглядных результатов, т.к.
в результате нормализации лицо занимает практически весь кадр, и вся его
площадь является активной областью. Самые наглядная визуализация получилась
 при использовании библиотеки SHAP&lt;sup&gt;&lt;a href=&#34;#lundberg2017shap&#34; id=&#34;lundberg2017shap_t&#34;&gt;[13]&lt;/a&gt;&lt;/sup&gt; (метод DeepExplainer)
 


&lt;figure&gt;

&lt;img src=&#34;shap.jpg&#34; alt=&#34;Активные зоны, влияющие на определение пола на мужском и женском лицах&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Активные зоны, влияющие на определение пола на мужском и женском лицах
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
 Видно, что модель прежде всего обращает внимание на зоны, где возможна растительность
 на лице: область над губой, щёки. Для щек также вероятно важна структура кожи,
 более грубая у мужчин. Для женщин важны разрез глаз (и по видимому наличие там косметики)
 и форма подбородка. Для мужчин - форма и &amp;ldquo;кустистость&amp;rdquo; бровей.&lt;/p&gt;

&lt;p&gt;Из собственного практического опыта работы с этой моделью &amp;ndash; она смотрит примерно на те же
 признаки, что и человек, никакого сверхзнания у неё нет. Если показать
 модели фото мальчика, хорошо загримированного под девочку, модель выдаст
 ответ &amp;ldquo;девочка&amp;rdquo;, и наоборот. Трансгендеры и лица, не до конца определившиеся
с выбором визуального пола, вызовут у модели затруднения при определении биологическго пола,
такие же, как и у людей.&lt;/p&gt;

&lt;h2 id=&#34;эволюция-в-результате-обучения&#34;&gt;Эволюция в результате обучения&lt;/h2&gt;

&lt;p&gt;Ещё один вопрос, на который было интересно ответить &amp;ndash; насколько
модель далеко ушла в своей эволюции от изначальной модели, обученной
на изображениях ImageNet? Поскольку выразить &amp;ldquo;далеко&amp;rdquo; или &amp;ldquo;не очень&amp;rdquo;
в виде числовой оценки затруднительно, лучше получить ответ в виде
визуализации. Я использовал визуализацию каналов ResNet с помощью
библиотеки &lt;a href=&#34;https://github.com/tensorflow/lucid&#34; target=&#34;_blank&#34;&gt;Lucid&lt;/a&gt;.
 Суть этой визуализации в том, что с помощью оптимизации подбирается такое входное изображение,
которое максимизирует ответ от канала. Содержимое этого изображения
будет указывать, на какие паттерны во входном изображении реагирует данный канал.&lt;/p&gt;

&lt;p&gt;Если сравнить визуализации одних и тех же каналов в исходной сети
и в сети после обучения на лицах, будет видно, как поменялось &amp;ldquo;восприятие&amp;rdquo; сетью изображений.
Будем визуализировать избранные каналы в блоках ResNet от первых блоков,
самых примитивных, обрабатывающих контуры и границы
до последних, обрабатывающих целые визуальные объекты.
Верхний ряд каждой визуализции это каналы модели, обученной распознаванию пола и возраста,
нижний ряд - те же самые каналы исходной модели, обученной на ImageNet.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;net_block1_6.jpg&#34; alt=&#34;Визуализация паттернов в первом блоке ResNet, 6-й канал&#34; width=&#34;654&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов в первом блоке ResNet, 6-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;



&lt;figure&gt;

&lt;img src=&#34;net_block1_7.jpg&#34; alt=&#34;Визуализация паттернов в первом блоке ResNet, 7-й канал&#34; width=&#34;654&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов в первом блоке ResNet, 7-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
В первом блоке обрабатываются самые простые паттерны. Видно что паттерны
для исходной и нашей модели не сильно отличаются. Тем не менее уже заметно,
что паттерны для ImageNet (нижние ряды) имеют более сложную структуру.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;net_block2_5.jpg&#34; alt=&#34;Визуализация паттернов во втором блоке ResNet, 5-й канал&#34; width=&#34;654&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов во втором блоке ResNet, 5-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
Во втором блоке паттерны усложнились, но всё еще похожи друг на друга. Заметно,
что в паттернах ImageNet больше цветовое разнообразие, а паттерны нашей
модели окрашены в цвета, близкие к телесным.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;net_block3_4.jpg&#34; alt=&#34;Визуализация паттернов во третьем блоке ResNet, 4-й канал&#34; width=&#34;850&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов во третьем блоке ResNet, 4-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
 


&lt;figure&gt;

&lt;img src=&#34;net_block3_8.jpg&#34; alt=&#34;Визуализация паттернов во третьем блоке ResNet, 8-й канал&#34; width=&#34;850&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов во третьем блоке ResNet, 8-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
В третьем блоке паттерны продолжают усложняться, и схожесть между ними остаётся только на самом общем уровне.
Паттерны ImageNet имеют гораздо более проработанную структуру, которая
начинает соответствовать объектам из реального мира.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;net_block4_4.jpg&#34; alt=&#34;Визуализация паттернов в четвёртом блоке ResNet, 4-й канал&#34; width=&#34;394&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Визуализация паттернов в четвёртом блоке ResNet, 4-й канал
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
 В последнем блоке паттерны окончательно перестали был похожими друг на друга.
 В паттернах нашей модели видна структура, соответствующая человеческим губам.
 в паттерах Imagenet &amp;ndash; что то растительное.&lt;/p&gt;

&lt;p&gt;Видно, что эволюция в верхних слоях зашла довольно далеко,
при этом паттерны нашей модели в целом проще исходных, т.е. произошла некоторая
деградация. Возможно, что ResNet-50 избыточен для данной задачи,
и можно было использовать более простую сеть.&lt;/p&gt;

&lt;h1 id=&#34;интерактивное-демо&#34;&gt;Интерактивное демо&lt;/h1&gt;

&lt;p&gt;В статье не публикуются образцы предсказаний модели, т.к. любые предсказания
можно сгенерировать самостоятельно, с помощью интерактивного
демо, находящегося по адресу &lt;a href=&#34;https://ag-demo.suilin.ru/&#34; target=&#34;_blank&#34;&gt;https://ag-demo.suilin.ru/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;В демо можно загружать любые фото, где есть лицо одного человека. Поддерживается
 работа как с компьютеров, так и со смартфонов (можно определять пол и возраст для селфи).&lt;/p&gt;

&lt;h1 id=&#34;резюме&#34;&gt;Резюме&lt;/h1&gt;

&lt;p&gt;Задача распознавания пола и возраста в промышленных масштабах оказалась
вполне решаемой. При этом модель угадывает возраст примерно на уровне
человека, часто даже точнее.&lt;/p&gt;

&lt;h2 id=&#34;возможные-улучшения&#34;&gt;Возможные улучшения&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Сделать более выравненную по возрастам обучающую выборку, чтобы не страдало качество вне основного диапазона возрастов&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Попробовать ускорить обучение с помощью &lt;em&gt;Super-Convergence&lt;/em&gt; &lt;sup&gt;&lt;a href=&#34;#smith2017superconvergence&#34; id=&#34;smith2017superconvergence_t&#34;&gt;[14]&lt;/a&gt;&lt;/sup&gt;.
Если ускорить обучение (рекорд скорости обучения ResNet-50 &amp;ndash; &lt;a href=&#34;https://www.fast.ai/2018/08/10/fastai-diu-imagenet&#34; target=&#34;_blank&#34;&gt;18 минут&lt;/a&gt;), появится
возможность обучаться на большем количестве данных. В результате можно увеличить размер обучающей выборки и активнее использовать аугментацию,
например применить алгоритмы AutoAugment&lt;sup&gt;&lt;a href=&#34;#cubuk2018autoaugment&#34; id=&#34;cubuk2018autoaugment_t&#34;&gt;[15]&lt;/a&gt;&lt;/sup&gt;, Mixup&lt;sup&gt;&lt;a href=&#34;#zhang2017mixup&#34; id=&#34;zhang2017mixup_t&#34;&gt;[16]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Попробовать более современные, чем ResNet, архитектуры: AmoebaNet&lt;sup&gt;&lt;a href=&#34;#real2018regularized&#34; id=&#34;real2018regularized_t&#34;&gt;[17]&lt;/a&gt;&lt;/sup&gt;,
DenseNet&lt;sup&gt;&lt;a href=&#34;#huang2017densely&#34; id=&#34;huang2017densely_t&#34;&gt;[18]&lt;/a&gt;&lt;/sup&gt;, WideResNet&lt;sup&gt;&lt;a href=&#34;#zagoruyko2016wide&#34; id=&#34;zagoruyko2016wide_t&#34;&gt;[19]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Работа с лицами это достаточно узкая и специфичная задача, возможно оптимальнее
будет не использовать архитектуру общего назначения, а создать custom архитектуру,
нацеленную именно на обработку лиц. Можно использовать методики
автоматического создания и оптимизации архитектур:
ENAS&lt;sup&gt;&lt;a href=&#34;#pham2018efficient&#34; id=&#34;pham2018efficient_t&#34;&gt;[20]&lt;/a&gt;&lt;/sup&gt;, DARTS&lt;sup&gt;&lt;a href=&#34;#liu2018darts&#34; id=&#34;liu2018darts_t&#34;&gt;[21]&lt;/a&gt;&lt;/sup&gt;,
Auto-Keras&lt;sup&gt;&lt;a href=&#34;#jin2018efficient&#34; id=&#34;jin2018efficient_t&#34;&gt;[22]&lt;/a&gt;&lt;/sup&gt;,  AdaNet&lt;sup&gt;&lt;a href=&#34;#cortes2017adanet&#34; id=&#34;cortes2017adanet_t&#34;&gt;[23]&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr style=&#34;margin-top: 3em;&#34;&gt;
&lt;h2&gt;Источники&lt;/h2&gt;
&lt;ol&gt;
  

  
  &lt;li id=&#34;zhang2016&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Joint face detection and alignment using multitask cascaded convolutional networks&lt;/span&gt;

    [&lt;a href=&#34;http://dx.doi.org/10.1109/LSP.2016.2603342&#34;&gt;link&lt;/a&gt;] &lt;a href=&#34;#zhang2016_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       K. Zhang,
     
       Z. Zhang,
     
       Z. Li,
     
       Y. Qiao,
     
    2016.
    IEEE Signal Processing Letters.
    23.10 (pp. 1499-1503) DOI:&lt;a href=&#34;http://doi.org/10.1109/lsp.2016.2603342&#34;&gt;10.1109/lsp.2016.2603342&lt;/a&gt;
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;bulat2017far&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;How far are we from solving the 2D &amp;amp; 3D face alignment problem? (And a dataset of 230,000 3D facial landmarks)&lt;/span&gt;

     &lt;a href=&#34;#bulat2017far_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       A. Bulat,
     
       G. Tzimiropoulos,
     
    2017.
    International conference on computer vision.
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;ludwiczuk2017&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Demystifying face recognition&lt;/span&gt;

    [&lt;a href=&#34;http://blcv.pl/static/tag/face-recogition&#34;&gt;link&lt;/a&gt;] &lt;a href=&#34;#ludwiczuk2017_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       B. Ludwiczuk,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;umeyama1991&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Least-squares estimation of transformation parameters between two point patterns&lt;/span&gt;

    [&lt;a href=&#34;http://edge.cs.drexel.edu/Dmitriy/Matching_and_Metrics/Umeyama/um.pdf&#34;&gt;PDF&lt;/a&gt;] &lt;a href=&#34;#umeyama1991_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       S. Umeyama,
     
    1991.
    IEEE Transactions on Pattern Analysis and Machine Intelligence.
    13.4 (pp. 376-380) DOI:&lt;a href=&#34;http://doi.org/10.1109/34.88573&#34;&gt;10.1109/34.88573&lt;/a&gt;
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;he2016&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Deep residual learning for image recognition&lt;/span&gt;

    [&lt;a href=&#34;http://dx.doi.org/10.1109/CVPR.2016.90&#34;&gt;link&lt;/a&gt;] &lt;a href=&#34;#he2016_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       K. He,
     
       X. Zhang,
     
       S. Ren,
     
       J. Sun,
     
    2016.
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
     DOI:&lt;a href=&#34;http://doi.org/10.1109/cvpr.2016.90&#34;&gt;10.1109/cvpr.2016.90&lt;/a&gt;
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;zoph2017learning&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Learning transferable architectures for scalable image recognition&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1707.07012&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#zoph2017learning_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       B. Zoph,
     
       V. Vasudevan,
     
       J. Shlens,
     
       Q. Le,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;howard2017mobilenets&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;MobileNets: Efficient convolutional neural networks for mobile vision applications&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1704.04861&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#howard2017mobilenets_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       A. Howard,
     
       M. Zhu,
     
       B. Chen,
     
       D. Kalenichenko,
     
       W. Wang,
     
       T. Weyand,
     
       M. Andreetto,
     
       H. Adam,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;rothe2016dex&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Deep expectation of real and apparent age from a single image without facial landmarks&lt;/span&gt;

    [&lt;a href=&#34;https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_01229.pdf&#34;&gt;PDF&lt;/a&gt;] &lt;a href=&#34;#rothe2016dex_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       R. Rothe,
     
       R. Timofte,
     
       L. Gool,
     
    2016.
    International Journal of Computer Vision (IJCV).
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;eidinger2014age&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Age and gender estimation of unfiltered faces&lt;/span&gt;

    [&lt;a href=&#34;https://www.openu.ac.il/home/hassner/Adience/EidingerEnbarHassner_tifs.pdf&#34;&gt;PDF&lt;/a&gt;] &lt;a href=&#34;#eidinger2014age_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       E. Eidinger,
     
       R. Enbar,
     
       T. Hassner,
     
    2014.
    IEEE Transactions on Information Forensics and Security.
    9. (pp. 2170-2179) DOI:&lt;a href=&#34;http://doi.org/10.1109/TIFS.2014.2359646&#34;&gt;10.1109/TIFS.2014.2359646&lt;/a&gt;
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;zhifei2017cvpr&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Age progression/regression by conditional adversarial autoencoder&lt;/span&gt;

    [&lt;a href=&#34;https://arxiv.org/abs/1702.08423&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#zhifei2017cvpr_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       S. Zhang,
     
       H. Qi,
     
    2017.
    IEEE conference on computer vision and pattern recognition (cvpr).
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;loshchilov2016sgdr&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;SGDR: Stochastic gradient descent with warm restarts&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1608.03983&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#loshchilov2016sgdr_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       I. Loshchilov,
     
       F. Hutter,
     
    2016.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;smith2017cyclical&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Cyclical learning rates for training neural networks&lt;/span&gt;

    [&lt;a href=&#34;http://dx.doi.org/10.1109/WACV.2017.58&#34;&gt;link&lt;/a&gt;] &lt;a href=&#34;#smith2017cyclical_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       L. Smith,
     
    2017.
    IEEE Winter Conference on Applications of Computer Vision (WACV).
     DOI:&lt;a href=&#34;http://doi.org/10.1109/wacv.2017.58&#34;&gt;10.1109/wacv.2017.58&lt;/a&gt;
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;lundberg2017shap&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;A unified approach to interpreting model predictions&lt;/span&gt;

    [&lt;a href=&#34;http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf&#34;&gt;PDF&lt;/a&gt;] &lt;a href=&#34;#lundberg2017shap_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       S. Lundberg,
     
       S. Lee,
     
    2017.
    Advances in neural information processing systems 30.
     (pp. 4765-4774)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;smith2017superconvergence&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Super-convergence: Very fast training of neural networks using large learning rates&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1708.07120&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#smith2017superconvergence_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       L. Smith,
     
       N. Topin,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;cubuk2018autoaugment&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;AutoAugment: Learning augmentation policies from data&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1805.09501&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#cubuk2018autoaugment_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       E. Cubuk,
     
       B. Zoph,
     
       D. Mane,
     
       V. Vasudevan,
     
       Q. Le,
     
    2018.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;zhang2017mixup&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Mixup: Beyond empirical risk minimization&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1710.09412&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#zhang2017mixup_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       H. Zhang,
     
       M. Cisse,
     
       Y. Dauphin,
     
       D. Lopez-Paz,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;real2018regularized&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Regularized evolution for image classifier architecture search&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1802.01548&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#real2018regularized_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       E. Real,
     
       A. Aggarwal,
     
       Y. Huang,
     
       Q. Le,
     
    2018.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;huang2017densely&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Densely connected convolutional networks&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1608.06993&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#huang2017densely_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       G. Huang,
     
       Z. Liu,
     
       L. Maaten,
     
       K. Weinberger,
     
    2017.
    CVPR.
     (pp. 2261-2269)
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;zagoruyko2016wide&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Wide residual networks&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1605.07146&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#zagoruyko2016wide_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       S. Zagoruyko,
     
       N. Komodakis,
     
    2016.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;pham2018efficient&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Efficient neural architecture search via parameter sharing&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1802.03268&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#pham2018efficient_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       H. Pham,
     
       M. Guan,
     
       B. Zoph,
     
       Q. Le,
     
       J. Dean,
     
    2018.
    
    abs/1802.03268.
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;liu2018darts&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Darts: Differentiable architecture search&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1806.09055&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#liu2018darts_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       H. Liu,
     
       K. Simonyan,
     
       Y. Yang,
     
    2018.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;jin2018efficient&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;Auto-keras: Efficient neural architecture search with network morphism&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1806.10282&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#jin2018efficient_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       H. Jin,
     
       Q. Song,
     
       X. Hu,
     
    2018.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  

  
  &lt;li id=&#34;cortes2017adanet&#34;&gt;

    &lt;span class=&#34;bib-title&#34;&gt;AdaNet: Adaptive structural learning of artificial neural networks&lt;/span&gt;

    [&lt;a href=&#34;http://arxiv.org/abs/1607.01097&#34;&gt;arXiv&lt;/a&gt;] &lt;a href=&#34;#cortes2017adanet_t&#34;&gt;^&lt;/a&gt;
    &lt;div class=&#34;bib-authors&#34;&gt;
     
       C. Cortes,
     
       X. Gonzalvo,
     
       V. Kuznetsov,
     
       M. Mohri,
     
       S. Yang,
     
    2017.
    
    
    &lt;/div&gt;

  &lt;/li&gt;
  
  
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>TopicTensor: тематики в Instagram</title>
      <link>https://suilin.ru/project/topic_tensor/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/project/topic_tensor/</guid>
      <description>

&lt;h2 id=&#34;проблема-подбора-блогеров&#34;&gt;Проблема подбора блогеров&lt;/h2&gt;

&lt;p&gt;У рекламодателей, использующих продвижение товаров и услуг через посты инфлюэнсеров, есть большая проблема,
 которая называется таргетирование. Если рекламодатель даёт рекламу, например,
 через AdWords, он может сфокусироваться по ключевым словам на сколь угодно узкий тематический сегмент аудитории,
  точно соответствующий тем товарам и услугами, которые он продвигает на рынок.
 В социальных сетях такой возможности нет: в Instagram миллионы потенциальных инфлюэнсеров,
  у которых можно разместить рекламу. Каким образом выбрать из них тех, тематика которых подходит рекламодателю?
Или рекламодатель должен сам активно пользоваться Инстаграмом и априори знать тех блогеров,
которые ему подойдут (т.е. только крупных и только тех, на кого он сам подписан).
Или просто давать рекламу наугад, надеясь что она дойдет до хотя бы небольшого процента целевой аудитории.
Понятно, что оба варианта не очень хороши, поэтому для нишевых средних и мелких рекламодателей Инстаграм малоэффективен.&lt;/p&gt;

&lt;p&gt;Если решить эту проблему, реклама в Инстаграме доступной и удобной для всех, а не только для крупных брендов.&lt;/p&gt;

&lt;h2 id=&#34;тематический-рубрикатор-спасибо-не-надо&#34;&gt;Тематический рубрикатор? Спасибо, не надо.&lt;/h2&gt;

&lt;p&gt;Самый очевидный путь разделения блогеров на тематические сегменты это
 тематический рубрикатор. Для этого сначала вручную создаётся дерево тематик.
 Затем каждый блогер вручную или с помощью машинного обучения соотносится с рубриками этого дерева.&lt;/p&gt;

&lt;p&gt;Этим путём идёт большинство рекламных систем, включая такие крупные, как Facebook Ads.
  Но на самом деле это неэффективное решение.
   Количество потенциально интересных рекламодателям тематик очень велико.
 Не будет ошибкой сказать, что оно потенциально бесконечно: чем более в узкой нише работает рекламодатель,
  тем более мелкое дробление на тематики ему требуется.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Например, есть тематика &lt;strong&gt;Еда&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Очевидно, что ресторану нужна не просто &lt;strong&gt;Еда&lt;/strong&gt;, а &lt;strong&gt;Еда : Рестораны&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Если это ресторан тайской кухни, то нужна еще более узкая тематика &lt;strong&gt;Еда : Рестораны : Тайская кухня&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Если этот ресторан находится в Лондоне, неплохо бы добавить и тематику &lt;strong&gt;Еда : Рестораны : Лондон&lt;/strong&gt;, и так далее.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Но дерево тематик не может расти бесконечно:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;C ним станет банально неудобно работать рекламодателю.
Размер рубрикатора, который без усилий может воспринять и удержать в голове обычный человек &amp;ndash; не больше 100 пунктов.&lt;/li&gt;
&lt;li&gt;Чем больше рубрик, тем сложнее отнести блогера к конкретным рубрикам и тем больший объем работы для этого нужен.
Сделать эту работу вручную (блогеров &amp;ndash; несколько миллионов) вообще невозможно.
Можно использовать машинное обучение, но всё равно сначала надо составить обучающую выборку,
т.е. образцы правильной разметки блогеров тематиками. Её составляют люди, люди делают ошибки,
люди субъективны в своих взглядах и оценках (разные асессоры отнесут одного и того же блогера к разным рубрикам).
Если учесть, что многие блогеры работают одновременно в нескольких тематиках, проблема еще больше усложняется.&lt;/li&gt;
&lt;li&gt;Придётся тратить много усилий на поддержание рубрикатора в актуальном состоянии
(постоянно появляются новые тренды, открываются новые тематики и отмирают старые),
и на обновление обучающей выборки.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Получается, что маленькое дерево тематик будет слишком негибким и давать слишком грубое деление,
а с большим деревом будет неудобно работать и рекламодателю, и его создателям. Необходимо другое решение.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;topic_tree.jpg&#34; width=&#34;560&#34;/&gt;&lt;/p&gt;

&lt;h2 id=&#34;тематику-задаёт-сам-рекламодатель&#34;&gt;Тематику задаёт сам рекламодатель&lt;/h2&gt;

&lt;p&gt;Что, если тематику можно будет задавать набором ключевых слов, как в AdWords?
Тогда рекламодатель сам сможет настроить для себя любую, сколь угодно узкую тематику,
 и ему при этом не надо будет ползать по огромному рубрикатору.
 Но в AdWords ключевые слова соответствуют тому, что явным образом ищут потенциальные клиенты,
  а чему должны соответствовать ключевые слова в Instagram? Хэштэгам? Но здесь всё не так просто.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Блогер, пишущий про автомобили, совсем не обязательно будет употреблять хэштэг &lt;code&gt;#car&lt;/code&gt;,
он может использовать &lt;code&gt;#auto&lt;/code&gt;, &lt;code&gt;#fastcars&lt;/code&gt;, &lt;code&gt;#wheels&lt;/code&gt;, &lt;code&gt;#drive&lt;/code&gt; или названия брендов &lt;code&gt;#bmw&lt;/code&gt;,  &lt;code&gt;#audi&lt;/code&gt; и т.п.
Как рекламодатель должен догадаться, какие конкретно тэги используют блогеры?
В принципе эта же проблема есть и в AdWords: рекламодателю надо приложить немалые усилия,
чтобы охватить все возможные ключевые слова в его тематике.&lt;/li&gt;
&lt;li&gt;Блогер может случайно употребить хэштэг, например &lt;code&gt;#car&lt;/code&gt;, если он купил новый авто,
или просто увидел и сфотографировал интересный автомобиль на улице.
Это совсем не значит, что он пишет на автомобильную тематику.&lt;/li&gt;
&lt;li&gt;Блогеры часто используют популярные тэги, не имеющие никакого отношения к тематике поста,
просто чтобы попасть в поисковую выдачу по тэгу (hashtag spam).
Например тэгом &lt;code&gt;#cat&lt;/code&gt; может быть помечено и фото бородатого хипстера, и пейзаж с закатом,
и селфи себя любимой в новом наряде и в окружении подруг.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Поэтому подбор блогеров просто по наличию у них тэгов, заданных рекламодателем, будет работать плохо.
Нужны более интеллектуальные способы решения этой задачи.&lt;/p&gt;

&lt;h2 id=&#34;тематическое-моделирование-теория&#34;&gt;Тематическое моделирование, теория&lt;/h2&gt;

&lt;p&gt;В современных методиках обработки естественного языка
есть область, называемая &lt;em&gt;тематическое моделирование&lt;/em&gt; (topic modeling). Проще всего
объяснить применение тематического моделирования к нашей проблеме на простом примере.&lt;/p&gt;

&lt;p&gt;Представим очень примитивную социальную сеть, в которой у людей есть всего два основных интереса
  - интерес к еде (&lt;em&gt;Food&lt;/em&gt;), и интерес к Японии (&lt;em&gt;Japan&lt;/em&gt;).
Если “силу” интереса сопоставить с числом от 0 до 1, то любые хэштэги, используемые блогерами,
можно поместить на 2D диаграмму.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;tags2d.png&#34; alt=&#34;Пример задания тематик на двумерном пространстве&#34; width=&#34;600&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Пример задания тематик на двумерном пространстве
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Видно, что любой хэштэг можно описать парой чисел, соответствующих координатам X и Y в тематическом пространстве.
Тэги, относящиеся к одной тематике, группируются в кластера, т.е. у них близкие координаты.
Пользуясь этой диаграммой, можно вычислить релевантность поста тематикам, рассчитав по всем тэгам,
 входящим в пост, его “усредненные” координаты в тематическом пространстве (т.е. получив
  &lt;a href=&#34;https://en.wikipedia.org/wiki/Centroid&#34; target=&#34;_blank&#34;&gt;центроид&lt;/a&gt;). Координаты центроида по осям X и Y
будут соответствовать релевантности поста тематикам &lt;em&gt;Food&lt;/em&gt; и &lt;em&gt;Japan&lt;/em&gt;, чем координата ближе к 1,
тем выше релевантность. Таким же образом рассчитав центроид всех постов блогера,
можно понять, каким тематикам в целом релевантен контент у блогера.&lt;/p&gt;

&lt;p&gt;В реальном тематическом моделировании конечно используется не две тематики, а десятки и сотни,
соответственно тэги существуют в high-dimensional пространстве. Более математическое определение:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Существует множество документов $D$ (в нашем случае это посты), множество
слов $W$ (в нашем случае тэги) и множество тематик $T$, размер которого задан заранее.&lt;/li&gt;
&lt;li&gt;Содержимое документов можно представить как набор пар документ-слово:
$(d, w), d \in D, w \in W_d$&lt;/li&gt;
&lt;li&gt;Каждая тематика $t \in T$ описывается неизвестным распределением $p(w|t)$ на множестве слов $w \in W$&lt;/li&gt;
&lt;li&gt;Каждый документ $d\in D$ описывается неизвестным распределением $p(t|d)$ на множестве тем $t\in T$&lt;/li&gt;
&lt;li&gt;Предполагается, что распределение слов в документах зависит только от тематики: $p(w|t,d)=p(w|t)$&lt;/li&gt;
&lt;li&gt;В процессе построения тематической модели алгоритм находит матрицу &amp;ldquo;слово&amp;ndash;тематика&amp;rdquo;
$\mathbf{\Phi} =||p(w|t)||$ и матрицу &amp;ldquo;тематика&amp;ndash;документ&amp;rdquo; $\mathbf{\Theta} =||p(t|d)||$ по содержимому коллекции $D$.
Нас интересует первая матрица.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Тематическое моделирование эквивалентно &lt;a href=&#34;https://en.wikipedia.org/wiki/Non-negative_matrix_factorization&#34; target=&#34;_blank&#34;&gt;неотрицательному
матричному разложению&lt;/a&gt;
(NMF, Non-negative matrix factorization). На входе есть разреженная матрица
&amp;ldquo;cлово-документ&amp;rdquo; $\mathbf{S} \in \mathbb{R}^{W \times D}$, описывающая
 вероятность встретить слово $w$ в документе $d$. Вычисляется её аппроксимация
в виде произведения низкоранговых матриц
 $\mathbf{\Phi} \in \mathbb{R}^{W \times T}$ и $\mathbf{\Theta} \in \mathbb{R}^{T \times D}$.
$$\mathbf{S} \approx \mathbf{\Phi}\mathbf{\Theta}$$
Более подробная информация по тематическому моделированию и его алгоритмам есть в &lt;a href=&#34;https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5&#34; target=&#34;_blank&#34;&gt;Википедии&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;тематическое-моделирование-практика&#34;&gt;Тематическое моделирование, практика&lt;/h2&gt;

&lt;p&gt;На практике тематическое моделирование показало посредственные
результаты. В таблице представлены результаты моделирования по 15 тематикам с помощью
библиотеки &lt;a href=&#34;https://github.com/bigartm/bigartm&#34; target=&#34;_blank&#34;&gt;BigARTM&lt;/a&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Тематика&lt;/th&gt;
&lt;th&gt;Топ тэги&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;sky, clouds, sea, spring, baby, ocean, nyc, flower, landscape, drinks&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;beer, vintage, chill, school, rainbow, yoga, rock, evening, chicago, relaxing&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;sweet, chocolate, dance, rain, nike, natural, anime, old, wcw, reflection&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;foodporn, breakfast, delicious, foodie, handmade, gold, instafood, garden, healthy, vegan&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;architecture, california, lights, portrait, newyork, wine, blonde, familytime, losangeles, thanksgiving&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;nature, travel, autumn, london, fall, trees, tree, photoshoot, city, cake&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;flowers, design, inspiration, artist, goals, illustration, pizza, ink, glasses, money&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;winter, snow, catsofinstagram, sexy, cats, cold, quote, fire, disney, festival&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;work, mountains, paris, football, nails, video, florida, diy, free, japan&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;dog, puppy, wedding, dogsofinstagram, dogs, roadtrip, painting, trip, thankful, pet&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;coffee, quotes, river, yum, moon, streetart, sleepy, music, adidas, positivevibes&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;style, fashion, party, home, model, music, dress, goodvibes, couple, tired&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;fitness, motivation, gym, workout, drawing, dinner, fit, sketch, health, fresh&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;beach, lake, usa, shopping, hiking, fashion, kids, park, freedom, sand&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;makeup, cat, yummy, eyes, snapchat, homemade, tattoo, kitty, lips, mom&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Видно, что какая то разумная структура прослеживается, но тематики далеки от совершенства.
Увеличение количества тематик до 150 даёт сравнительно небольшое улучшение.&lt;/p&gt;

&lt;p&gt;Возможно, причина в том, что тематическое моделирование рассчитано на работу с документами, содержащими
сотни и тысячи слов. В нашем случае у большинства постов есть всего 2-3 тэга.&lt;/p&gt;

&lt;p&gt;У BigARTM есть большое количество гиперпараметров и возможных способов их применения
(в начале обучения, в конце, ко всем тематикам, к отдельным тематикам и т.п.).
Возможно, при некоторых настройках результат был бы лучше,
но TopicTensor это коммерческий проект, подразумевающий лимиты времени на реализацию.
С тематическим моделированием был риск потратить всё проектное время на подбор гиперпараметров,
и так и не получить удовлетворительный результат на выходе. Другие библиотеки ( &lt;a href=&#34;https://radimrehurek.com/gensim&#34; target=&#34;_blank&#34;&gt;Gensim&lt;/a&gt;,
 &lt;a href=&#34;http://mallet.cs.umass.edu/topics.php&#34; target=&#34;_blank&#34;&gt;Mallet&lt;/a&gt;)
тоже показали весьма скромные результаты.&lt;/p&gt;

&lt;p&gt;Поэтому был выбран другой,
более простой и в то же время более мощный способ моделирования.
$ \newcommand{\sim}[2]{\operatorname{sim}(#1,#2)} $&lt;/p&gt;

&lt;h2 id=&#34;модель-topictensor&#34;&gt;Модель TopicTensor&lt;/h2&gt;

&lt;p&gt;Основной плюс тематического моделирования это интерпретируемость полученных
результатов. Для любого слова/тэга на выходе получается набор весов, показывающих,
насколько близко это слово к каждой тематике из всего набора.&lt;/p&gt;

&lt;p&gt;Но этот же плюс накладывает серьезные ограничения на модель, вынуждая её
укладываться строго в фиксированное количество тематик, не больше и не меньше.
В реальной жизни количество тематик большой социальной сети практически бесконечно.
Поэтому, если убрать требование по интерпретируемости тематик (и их фиксированному количеству),
обучение станет более эффективным.&lt;/p&gt;

&lt;p&gt;В результате получается модель, близкая по смыслу к хорошо известной модели &lt;a href=&#34;https://en.wikipedia.org/wiki/Word2vec&#34; target=&#34;_blank&#34;&gt;Word2Vec&lt;/a&gt;.
Каждый тэг представлен в виде вектора в $N$-мерном пространстве: $w \in \mathbb{R}^N$.
Степень схожести (т.е. насколько близки тематики) между тэгами $w$ и $w&amp;rsquo;$ может вычисляться как скалярное произведение (dot product):
$$\sim{w}{w&amp;rsquo;}=w \cdot w&amp;rsquo;$$
как Евклидово расстояние:
 $$\sim{w}{w&amp;rsquo;}=\|w-w&amp;rsquo;\|$$
как cosine similarity:
$$\sim{w}{w&amp;rsquo;}=\cos(\theta )=\frac{w \cdot w&amp;rsquo;}{\|w \|\|w&amp;rsquo; \|}$$&lt;/p&gt;

&lt;p&gt;Задача модели в ходе обучения - найти такие представления тэгов, которые будут полезны
для одного из предсказаний:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;На основе одного тэга предсказать, какие ещё тэги будут включены в пост (архитектура Skip-gram)&lt;/li&gt;
&lt;li&gt;На основе всех тэгов поста, кроме одного, предсказать недостающий тэг (архитектура CBOW, &amp;ldquo;мешок слов&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;Взять два случайных тэга из поста, и на основе первого предсказать второй&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Все эти предсказания сводятся к тому, что есть целевой тэг $w_t$ , который надо предсказать,
и контекст $c$, представленный одним или несколькими тэгами, входящими в пост. Модель
должна максимизировать вероятность тэга в зависимости от контекста, это можно представить
в виде softmax критерия:&lt;/p&gt;

&lt;p&gt;$$P(w_t|c) = \operatorname{softmax}(\sim{w_t}{c})$$
$$P(w_t|c) = \frac{\exp(\sim{w_t}{c})}{\sum_{w&amp;rsquo; \in W}\exp(\sim{w&amp;rsquo;}{c})}$$&lt;/p&gt;

&lt;p&gt;Но вычислять softmax по всему множеству тэгов $W$ дорого (в обучении может участвовать миллион тэгов и более),
поэтому вместо него используются альтернативные способы. Они сводятся к тому, что есть позитивный
пример $(w_t,c)$, который надо предсказать, и случайно выбранные негативные примеры
$(w_1^{-}, c), (w_2^{-}, c),\dots,(w_n^{-}, c)$ являющиеся образцом того, как &lt;em&gt;не надо&lt;/em&gt; предсказывать.
Негативные примеры должны сэмплироваться из того же распределения частот тэгов, что и в обучающих данных.&lt;/p&gt;

&lt;p&gt;Loss функция по набору примеров может иметь вид бинарной классификации (Negative sampling в классическом Word2Vec)
$$L = \log(\sigma(\sim{w_t}{c})) +  \sum_i\log(\sigma(-\sim{w_i^-}{c}))$$
$$\sigma(x) = \frac{1}{1+e^{-x}}$$
или работать как ranking loss, попарно сравнивая &amp;ldquo;совместимость&amp;rdquo; с контекстом позитивного и негативного примеров:
$$L = \sum_{i} l(\sim{w_t}{c}, \sim{w_i^-}{c})$$
гдe $l(\cdot, \cdot)$ это ranking функция, в качестве которой часто используют
max margin loss:
$$l=\max(0,\mu+\sim{w_i^-}{c}−\sim{w_t}{c})$$&lt;/p&gt;

&lt;p&gt;Модель TopicTensor также эквивалентна матричной
факторизации, только вместо матрицы &amp;ldquo;документ-слово&amp;rdquo; (как в тематическом моделировании)
здесь факторизуется матрица &amp;ldquo;контекст-тэг&amp;rdquo;, которая в при некоторых типах предсказаний
превращается в матрицу взаимной встречаемости тэгов &amp;ldquo;тэг-тэг&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;практическая-реализация-topictensor&#34;&gt;Практическая реализация TopicTensor&lt;/h2&gt;

&lt;p&gt;Были рассмотрены несколько возможных способов реализации модели: код на &lt;a href=&#34;https://www.tensorflow.org/&#34; target=&#34;_blank&#34;&gt;Tensorflow&lt;/a&gt; ,
код на &lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34;&gt;PyTorch&lt;/a&gt;, библиотека &lt;a href=&#34;https://radimrehurek.com/gensim/&#34; target=&#34;_blank&#34;&gt;Gensim&lt;/a&gt;,
 библиотека &lt;a href=&#34;https://github.com/facebookresearch/StarSpace&#34; target=&#34;_blank&#34;&gt;StarSpace&lt;/a&gt;.
Выбран последний вариант, как
требующий минимальных усилий на доработку (вся необходимая функциональность уже есть),
дающий высокое качество, и практически линейно распараллеливающийся на любое количестве
ядер (для ускорения обучения использовалась 32 и 64-ядерные машины).&lt;/p&gt;

&lt;p&gt;StarSpace по умолчанию использует loss функцию &lt;em&gt;max margin ranking loss&lt;/em&gt;
и &lt;em&gt;cosine distance&lt;/em&gt; как метрику близости векторов. Последующие эксперименты
с гиперпараметрами показали, что эти установки по умолчанию являются оптимальными.&lt;/p&gt;

&lt;h3 id=&#34;подбор-гиперпараметров&#34;&gt;Подбор гиперпараметров&lt;/h3&gt;

&lt;p&gt;Перед финальным
обучением проводился подбор гиперпараметров, с целью найти баланс между качеством
и приемлемым временем обучения.
Качество замерялось так: бралась выборка постов, которые модель не видела в ходе
обучения. Для каждого тэга из поста (всего в посте $n$ тэгов) находились наиболее близкие ему
тэги-кандидаты по критерию cosine similarity из множества всех тэгов $W$:
$$candidates_i=\operatorname{top_n}(\sim{w_t}{w&amp;rsquo;}, \forall w&amp;rsquo; \in W)$$
$$i \in 1 \dots n$$
Подсчитывалось, сколько из этих кандидатов совпало с реальными тэгами в посте (число совпадений $n^{+}$).
$$quality=\frac{\sum n^+}{\sum n}$$
Качество &amp;ndash; процент правильно угаданных тэгов по всем постам из выборки.
Такая оценка качества наиболее близка к использованию модели в реальной жизни,
когда пользователь будет задавать в большинстве случаев один стартовый тэг и подбирать по нему остальные
тэги, блогеров, и т.д.&lt;/p&gt;

&lt;p&gt;Эта оценка также подразумевает, что наиболее оптимально обучать модель по skip-gram
критерию (по одному тэгу предсказывать остальные). Это подтвердилось на
практике: skip-gram обучение показало наилучшее качество, хотя оказалось и самым медленным.&lt;/p&gt;

&lt;p&gt;Подбирались следующие гиперпараметры:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Размерность векторов&lt;/li&gt;
&lt;li&gt;Кол-во эпох для обучения&lt;/li&gt;
&lt;li&gt;Кол-во негативных примеров&lt;/li&gt;
&lt;li&gt;Learning rate&lt;/li&gt;
&lt;li&gt;Undersampling и oversampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Последний гиперпараметр связан с тем, что на постах с малым количеством тэгов обучение
происходит быстрее, чем на постах с большим количеством тэгов. StarSpace за один проход
случайным образом выбирает из поста только один целевой тэг. Таким образом за 20 эпох
каждый тэг из поста, содержащего 2 целевых тэга, побудет целевым в среднем 10 раз, а каждый
тэг из поста, содержащего 20 тэгов &amp;ndash; в среднем только один раз. На коротких списках
тэгов модель переобучится, а на длинных наоборот недообучится. Чтобы избежать этого,
&amp;ldquo;коротким&amp;rdquo; постам надо делать undersampling, а &amp;ldquo;длинным&amp;rdquo; &amp;ndash; oversampling.&lt;/p&gt;

&lt;h3 id=&#34;подготовка-данных&#34;&gt;Подготовка данных&lt;/h3&gt;

&lt;p&gt;Тэги были нормализованы: приведены к lowercase, убраны диакритические знаки (за исключением случаев,
где знак влияет на смысл слова)&lt;/p&gt;

&lt;p&gt;Для обучения отобраны тэги, встречающиеся в обучающей выборке
не менее N раз у разных блогеров, чтобы обеспечить
разнообразие контекстов их использования (в зависимости от языка N
варьировалось от 20 до 500).&lt;/p&gt;

&lt;p&gt;Для каждого языка была сделана выборка из топ 1000 самых распространенных тэгов, и в этой
выборке внесены в blacklist общеупотребительные слова,
 не несущие тематической нагрузки (me, you, together, etc), числительные,
названия цветов (красный, желтый, и т.п.), и некоторые тэги, особенно любимые спамерами.&lt;/p&gt;

&lt;p&gt;Тэги каждого блогера перевзвешены в соответствии с частотой их употребления этим блогером.
У большинства блогеров есть &amp;ldquo;любимые&amp;rdquo; тэги и сочетания, используемые почти в каждом посте, и если не понизить их вес,
активно пишущий блогер может перекосить глобальную статистику по употреблению любимых им тэгов и модель обучится под вкусы
этого блогера.&lt;/p&gt;

&lt;p&gt;В финальную обучающую выборку попало около 8 млрд тэгов из 1 млрд постов.
Обучение шло более трех недель на 32-ядерном сервере.&lt;/p&gt;

&lt;h2 id=&#34;результаты&#34;&gt;Результаты&lt;/h2&gt;

&lt;p&gt;Полученные embeddings показали отличное разделение тематик, хорошую
способность к генерализации и устойчивость к спам-тэгам.&lt;/p&gt;

&lt;p&gt;Демо-выборка top 10К тэгов (только английский язык) &lt;a href=&#34;http://projector.tensorflow.org/?config=https://gist.githubusercontent.com/Arturus/5dc4d72432e0fc2a5d6f543178a39f1f/raw/790392fe844d86eb46a5ac07622ac6715f8c67de/sample.json&#34; target=&#34;_blank&#34;&gt;доступна для просмотра&lt;/a&gt;
в Embedding Projector. Перейдя по ссылке, надо переключиться в режим t-SNE
 (tab в левой нижней части) и подождать примерно 500 итераций, пока
 не построится проекция в 3D. Просматривать лучше в режиме &lt;code&gt;Color By = logcnt&lt;/code&gt;.
Если не хочется ждать, в правом нижнем
 углу есть раздел &lt;code&gt;Bookmarks&lt;/code&gt;, в нём выбрать &lt;code&gt;Default&lt;/code&gt;, тогда сразу загрузится
 уже рассчитанная проекция.&lt;/p&gt;

&lt;h3 id=&#34;примеры-формирования-тематик&#34;&gt;Примеры формирования тематик&lt;/h3&gt;

&lt;p&gt;Начнём с самого простого. Зададим тематику одним тэгом и найдем топ 50
релевантных тэгов.



&lt;figure&gt;

&lt;img src=&#34;bmw.png&#34; alt=&#34;Тематика, заданная тэгом `#bmw`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгом &lt;code&gt;#bmw&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
&lt;img src=&#34;relevance.png&#34; width=&#34;400&#34;/&gt;
Тэги окрашены в соответствии с релевантностью. Размер тэга пропорционален его популярности.&lt;/p&gt;

&lt;p&gt;Как видно, TopicTensor прекрасно справился с формированием тематики ‘BMW’
и нашел много релевантных тэгов, о существовании которых большинство даже не подозревает.&lt;/p&gt;

&lt;p&gt;Усложним задачу, и сформируем тематику из нескольких немецких автобрендов
 (найдем тэги, наиболее близкие к сумме векторов входных тэгов):



&lt;figure&gt;

&lt;img src=&#34;cars.png&#34; alt=&#34;Тематика, заданная тэгами `#bmw`, `#audi`, `#mercedes`, `#vw`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгами &lt;code&gt;#bmw&lt;/code&gt;, &lt;code&gt;#audi&lt;/code&gt;, &lt;code&gt;#mercedes&lt;/code&gt;, &lt;code&gt;#vw&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;На этом примере видна способность TopicTesnor к обобщению (generalisation):
TopicTensor понял, что мы имеем в виду автомобили в целом (тэги &lt;code&gt;#car&lt;/code&gt;, &lt;code&gt;#cars&lt;/code&gt;).
Также понял, что в тематике надо отдать предпочтение немецким автомобилям (тэги, обведенные красным),
и сам добавил “недостающие” тэги:
&lt;code&gt;#porsche&lt;/code&gt; (тоже немецкий автобренд), и варианты написания тэгов, которых не было на входе:
 &lt;code&gt;#mercedesbenz&lt;/code&gt;, &lt;code&gt;#benz&lt;/code&gt; и &lt;code&gt;#volkswagen&lt;/code&gt;&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;apple.png&#34; alt=&#34;Тематика, заданная тэгом `#apple`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгом &lt;code&gt;#apple&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Усложним задачу еще больше, и создадим тематику на основе неоднозначного тэга
 &lt;code&gt;#apple&lt;/code&gt;, который может обозначать как бренд, так и просто фрукт.
  Видно, что тематика бренда доминирует, тем не менее фруктовая тема тоже присутствует
 в виде тэгов &lt;code&gt;#fruit&lt;/code&gt;, &lt;code&gt;#apples&lt;/code&gt; и &lt;code&gt;#pear&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Попробуем выделить чистую “фруктовую” тематику, для этого добавим несколько тэгов, относящихся
  к бренду apple, с отрицательным веcом.
Соответственно, будем искать тэги, наиболее близкие к взвешенной сумме векторов входных тэгов (по умолчанию
вес равен единице):
$$target = \sum_i w_i \cdot tag_i $$&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;apple_fruit.png&#34; alt=&#34;Тематика, заданная тэгами  `#apple`, `#iphone:-1`, `#macbook:-0.05` `#macintosh:-0.005`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгами  &lt;code&gt;#apple&lt;/code&gt;, &lt;code&gt;#iphone:-1&lt;/code&gt;, &lt;code&gt;#macbook:-0.05&lt;/code&gt; &lt;code&gt;#macintosh:-0.005&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Видно, что отрицательные веса убрали тематику бренда, и осталась
 только тематика фрукта.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;mirror.png&#34; alt=&#34;Тематика, заданная тэгом  `#mirror`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгом  &lt;code&gt;#mirror&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
TopicTensor в курсе, что
одно и то же понятие может быть выражено различными словами на разных языках,
как это видно на примере с &lt;code&gt;#mirror&lt;/code&gt;. К английским
&lt;strong&gt;mirror&lt;/strong&gt; и &lt;strong&gt;reflection&lt;/strong&gt; подобрались: &lt;strong&gt;зеркало&lt;/strong&gt; и &lt;strong&gt;отражение&lt;/strong&gt; на русском,
&lt;strong&gt;espejo&lt;/strong&gt; и &lt;strong&gt;reflejo&lt;/strong&gt; на испанском, &lt;strong&gt;espelho&lt;/strong&gt; и &lt;strong&gt;reflexo&lt;/strong&gt; на португальском,
&lt;strong&gt;specchio&lt;/strong&gt; и &lt;strong&gt;riflesso&lt;/strong&gt; на итальянском, &lt;strong&gt;spiegel&lt;/strong&gt; и &lt;strong&gt;spiegelung&lt;/strong&gt; на немецком.&lt;/p&gt;

&lt;p&gt;


&lt;figure&gt;

&lt;img src=&#34;boobs.png&#34; alt=&#34;Тематика, заданная тэгом  `#boobs`&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тематика, заданная тэгом  &lt;code&gt;#boobs&lt;/code&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
На последнем примере видно, что casual тематики работают так же хорошо,
 как и брендовые.&lt;/p&gt;

&lt;h3 id=&#34;подбор-блоггеров&#34;&gt;Подбор блоггеров&lt;/h3&gt;

&lt;p&gt;Для каждого блогера анализируются его посты и суммируются вектора всех входящих в них тэгов.
$$\beta=\sum_i^{|posts|}\sum_j^{|tags_i|} w_{ij}$$
где $|posts|$ это кол-во постов, $|tags_i|$ это кол-во тэгов в $i$-том посте.
Результирующий вектор $\beta$ это и есть &lt;em&gt;тематика блогера&lt;/em&gt;.
Затем находятся
блогеры, тематический вектор которых наиболее близок к тематическому
вектору, заданному пользователем. Список сортируется по релевантности
и выдается пользователю.&lt;/p&gt;

&lt;p&gt;Дополнительно учитывается популярность блогера и количество тэгов в его постах,
т.к. в противном случае в топ вышли бы блогеры, у которых есть один пост
с одним тэгом, заданным пользователем на входе. Финальный score, по которому
сортируются блогеры, рассчитывается так:
$$score_i = {\sim{input}{\beta_i} \over \log(likes)^\lambda \cdot \log(followers)^\phi \cdot \log(tags)^\tau}$$
где $\lambda, \phi, \tau$ - эмпирически подобранные коэффициенты, лежащие в интервале $0\dots1$&lt;/p&gt;

&lt;p&gt;Расчет cosine distance по всему массиву блогеров (в подборе участвует несколько миллионов аккаунтов)
занимает значительное время. Для ускорения подбора была
использована библиотека &lt;a href=&#34;https://github.com/nmslib/nmslib&#34; target=&#34;_blank&#34;&gt;NMSLIB&lt;/a&gt; (Non-Metric Space Library), позволившая
сократить время поиска на порядок. NMSLIB заранее строит индексы по
координатам векторов в пространстве, что позволяет намного быстрее вычислять
топ близких векторов, рассчитывая cosine distance только для тех кандидатов,
для которых это имеет смысл.&lt;/p&gt;

&lt;h3 id=&#34;демо-сайт&#34;&gt;Демо сайт&lt;/h3&gt;

&lt;p&gt;Демонстрационный сайт с ограниченным количеством тэгов и блогеров доступен
по адресу &lt;a href=&#34;http://tt-demo.suilin.ru/&#34; target=&#34;_blank&#34;&gt;http://tt-demo.suilin.ru/&lt;/a&gt;. На сайте можно самостоятельно
поэкспериментировать с формированием тематик и подбором блогеров по
сформированной тематике.&lt;/p&gt;

&lt;h3 id=&#34;тематические-lookalikes&#34;&gt;Тематические lookalikes&lt;/h3&gt;

&lt;p&gt;Вектора $\beta$, рассчитанные для подбора блогеров, можно использовать и для
сопоставления блогеров друг с другом. Фактически lookalikes это тот же подбор блогеров,
но вместо вектора тэгов на вход подается тематический вектор $\beta$ блогера, заданного
пользователем. На выходе получается список блогеров, тематика которых
близка тематике заданного блогера, в порядке релевантности.&lt;/p&gt;

&lt;h3 id=&#34;фиксированные-тематики&#34;&gt;Фиксированные тематики&lt;/h3&gt;

&lt;p&gt;В TopicTensor, как уже говорилось, нет явным образом заданных тематик.
Тем не соотнесение постов и блогеров с фиксированным набором тематики бывает необходимо,
для упрощения поиска, или для ранжирования блогеров внутри отдельных тематик.
Возникает задача по экстрагированию фиксированных тематик из векторного пространства тэгов.&lt;/p&gt;

&lt;p&gt;Для решения этой задачи было выбрано unsupervised обучение, чтобы
избежать субъективности при определении возможных тематик, и чтобы
сэкономить ресурсы, т.к. просмотр сотен тысяч тегов (даже 10% из них) и присвоение им тематик
&amp;ndash; это большая ручная работа.&lt;/p&gt;

&lt;p&gt;Самый очевидный способ экстракции тематик это кластеризация векторного
представления тэгов, один кластер = одна тематика. Кластеризация проводилась в два
этапа, т.к. алгоритмов, способных эффективно искать кластера в 200D
пространстве, пока не существует.&lt;/p&gt;

&lt;p&gt;На первом этапе проводилось уменьшение размерности с помощью технологии
&lt;a href=&#34;https://github.com/lmcinnes/umap&#34; target=&#34;_blank&#34;&gt;UMAP&lt;/a&gt;. Эта технология является в некотором смысле
улучшенным t-SNE (хотя основана на совершенно других принципах),
быстрее работает, и лучше сохраняет исходную
топологию данных. Размерность уменьшалась до 5D, в качестве метрики расстояния использовался cosine distance,
остальные гиперпараметры подбирались по результатам кластеризации (второго этапа).&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;clusters3d.png&#34; alt=&#34;Пример кластеризации тэгов в 3D пространстве. Разные кластера помечены разными цветами (цвета не уникальны и могут повторяться для разных кластеров).&#34; width=&#34;600&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Пример кластеризации тэгов в 3D пространстве. Разные кластера помечены разными цветами (цвета не уникальны и могут повторяться для разных кластеров).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;На втором этапе проводилась кластеризация алгоритмом &lt;a href=&#34;https://github.com/scikit-learn-contrib/hdbscan&#34; target=&#34;_blank&#34;&gt;HDBSCAN&lt;/a&gt; .
Результаты кластеризации (только по английскому языку)
можно увидеть &lt;a href=&#34;https://github.com/Arturus/clusters_new/blob/master/index.md&#34; target=&#34;_blank&#34;&gt;в GitHub&lt;/a&gt;.
Кластеризация выделила около 500 тематик (параметрами UMAP и кластеризации
можно регулировать кол-во тематик в широких пределах), при этом в кластера
попало 70%-80% тэгов. Визуальная
проверка показала хорошую когерентность тематик и отсутствие заметной корреляции
между кластерами. Тем не менее, для практического применения
кластера нуждаются в доработке: собрать из них дерево, убрать бесполезные
(например &lt;a href=&#34;https://raw.githubusercontent.com/Arturus/clusters_new/master/jack.txt&#34; target=&#34;_blank&#34;&gt;кластер личных имён&lt;/a&gt;,
 &lt;a href=&#34;https://raw.githubusercontent.com/Arturus/clusters_new/master/help.txt&#34; target=&#34;_blank&#34;&gt;кластер негативных эмоций&lt;/a&gt;,
 &lt;a href=&#34;https://raw.githubusercontent.com/Arturus/clusters_new/master/people.txt&#34; target=&#34;_blank&#34;&gt;кластер общеупотребительных слов&lt;/a&gt;),
  объединить некоторые кластера в одну тематику.&lt;/p&gt;

&lt;h2 id=&#34;возможные-улучшения&#34;&gt;Возможные улучшения&lt;/h2&gt;

&lt;p&gt;Основной недостаток TopicTensor &amp;ndash; покрытие, далёкое от 100%. Далеко не все блогеры
используют тэги, и далеко не все, кто их используют, пишут в них что-то осмысленное.
Есть два основных способа расширить покрытие:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Анализ содержимого фото. Тематика блога чётко определяется по фотографиям
(собственно, они её и задают), поэтому computer vision модель, обученная
выдавать по фотографии её тематический вектор, могла бы частично заменить тэги.&lt;/li&gt;
&lt;li&gt;Если считать, что у блогеров со схожей аудиторией должна быть схожая
тематика, можно выводить тематику блогеров, не использующих тэги,
через audience lookalikes, если существуют блогеры с похожей
аудиторией и с тэгами.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Определение места жительства Instagram пользователей</title>
      <link>https://suilin.ru/project/geo/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0300</pubDate>
      
      <guid>https://suilin.ru/project/geo/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;world_crop2.jpg&#34;&gt;
Для рекламодателей одним из самых важных параметров блогера и его
аудитории является географическое положение (geolocation), соответствующее
месту жительства.
Чтобы рекламная кампания была эффективной, она обычно должна быть таргетирована
 на конкретную страну или даже город.&lt;/p&gt;

&lt;h2 id=&#34;геотэги&#34;&gt;Геотэги&lt;/h2&gt;

&lt;p&gt;Посты в Instagram могут содержать информацию о месте, в котором был сделан пост, называемую &lt;em&gt;geotags&lt;/em&gt;.
 Это самый простой и очевидный способ определения geolocation. В заголовке страницы приведена карта,
  окраска которой соответствует количеству постов, сделанных в каждой точке мира (построено
  по данным геотэгов из более чем 4 млрд. постов).
Видно, что наибольшая интенсивность
    постов соответствует местам с большой плотностью населения &amp;ndash; крупные города с пригородами,
 зоны вдоль крупных трасс.&lt;/p&gt;

&lt;p&gt;Но геотэги, несмотря на свою очевидную полезность, не являются надёжным источником информации
 о месте &lt;strong&gt;проживания&lt;/strong&gt; владельца аккаунта:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Часто люди используют geotagging только когда едут в командировку/отпуск/путешествие или находятся в интересных с их точки зрения местах.
То есть у блогера, живущего в Лондоне, геотэги могут быть из каких угодно мест мира, кроме самого Лондона.&lt;/li&gt;
&lt;li&gt;Далеко не все аккаунты включают geotagging постов. У большинства постов (более 85%) геотэги не проставлены.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Поэтому, если определять geolocation только по геотэгам, у результатов будет маленькое покрытие и низкая точность.
 Чтобы улучшить качество определения, необходимо использовать дополнительную информацию.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;sources.jpg&#34; width=&#34;450&#34;/&gt;&lt;/p&gt;

&lt;h2 id=&#34;дополнительные-источники-информации&#34;&gt;Дополнительные источники информации&lt;/h2&gt;

&lt;p&gt;Cуществует много дополнительных, не столь очевидных и надёжных, как геотэги, источников информации о местоположении блогера.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;В первую очередь это текст из раздела ‘bio’ аккаунта: в нем могут быть указаны город блогера или локальные географические названия,
сайт или email в национальных доменах, emoji, соответствующие флагам стран, и т.п.&lt;/li&gt;
&lt;li&gt;Такая же информация + текстовые тэги может содержаться в тексте постов и комментариев.&lt;/li&gt;
&lt;li&gt;Кроме этого, можно использовать информацию о языке и audience geolocation &amp;ndash; очевидно,
что например японская аудитория скорее будет у блогера из Японии, чем у блогера из Мексики.
Также маловероятно, что мексиканский блогер будет делать посты на японском языке.&lt;/li&gt;
&lt;li&gt;Также, приблизительную геоинформацию можно извлекать из фотографий в постах (когда нет геотэгов).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Вся эта информация извлекается с помощью набора правил,
использующих как и сложные закодированные вручную эвристики, так и
 machine learning модели для named entity recognition.
Рассказ о методиках извлечения может потянуть
на отдельный проект, поэтому здесь я сфокусируюсь только
 на использовании уже извлеченной геоинформации.&lt;/p&gt;

&lt;h2 id=&#34;ensemble-learning&#34;&gt;Ensemble learning&lt;/h2&gt;

&lt;p&gt;Итак, есть несколько источников геоинформации.
Каждый отдельно взятый источник не очень точен и достоверен,
но совместное их использование может дать намного более точный результат. Классический подход,
используемый для работы с такими источниками, это &lt;em&gt;ensemble learning&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Ensemble learning is a machine learning paradigm where multiple learners
are trained to solve the same problem. In contrast to ordinary machine
learning approaches which try to learn &lt;em&gt;one&lt;/em&gt; hypothesis from training data,
 ensemble methods try to construct a set of hypotheses and combine them to use.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;В нашем конкретном случае используется разновидность ensemble learning, называемая
&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Ensemble_learning#Stacking&#34; target=&#34;_blank&#34;&gt;stacking&lt;/a&gt;&lt;/em&gt;.
&lt;img src=&#34;stacking.png&#34; width=&#34;400&#34;/&gt;
Смысл этого метода в том, что есть два уровня моделей:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First level &amp;ndash; это модели, извлекающие геоинформацию из данных Instagram, о которых мы говорили
в предыдущем разделе (&lt;em&gt;base classifiers&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Second level (&lt;em&gt;meta-classifier&lt;/em&gt;) использует предсказания, сгенерированные моделями на первом уровне,
для принятия окончательного решения.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;hands.jpg&#34; width=600/&gt;&lt;/p&gt;

&lt;h2 id=&#34;weighted-majority-voting&#34;&gt;Weighted majority voting&lt;/h2&gt;

&lt;p&gt;Простейший способ создания meta classifier &amp;ndash; это использование &lt;em&gt;weighted majority voting&lt;/em&gt;.
 Есть $N$ базовых классификаторов, каждому из них присваивается вес $w_i$, соответствующий
 вероятности правильной классификации $\hat{p}_i$ (в данном случае правильному определению страны блогера).
 Вероятности рассчитываются по обучающей выборке.
$$w_i=\log\left(\frac{\hat{p}_i}{1−\hat{p}_i}\right),\quad i=1,\dotsc,N$$&lt;/p&gt;

&lt;p&gt;Классификаторы, которые часто ошибаются, получают меньший вес, классификаторы, у которых высокая точность &amp;ndash; больший вес.
На этом &amp;ldquo;обучение&amp;rdquo; заканчивается.&lt;/p&gt;

&lt;p&gt;Использование: для интересующего нас аккаунта каждым из базовых классификаторов назначаются
 метки классов (в нашем случае это предполагаемые страны), получаем набор меток
 $l_1,\dotsc,l_N$ . Значения, которые могут принимать метки,
  это множество всех стран размером $M$: $(c_1,\dotsc,c_M)$.
 Подсчитывается рейтинг каждого класса суммированием весов всех отданных
 за него голосов.&lt;/p&gt;

&lt;p&gt;$$R(k)=\sum_{l_i=c_k}w_i,\quad k=1,\dotsc,M$$&lt;/p&gt;

&lt;p&gt;Результирующий класс (страна) &amp;ndash; тот, который набрал наибольший рейтинг,
 т.е. тот за которого было отдано больше голосов с большим весом:
$$k^*=\operatorname*{arg} \operatorname*{max}_{k=1}^M R(k)$$&lt;/p&gt;

&lt;p&gt;Первая модель для определения geolocation использовала именно этот способ,
 как самый простой в реализации. Но у него есть недостатки:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Предсказания base classifiers могут быть неверными, например из bio может быть извлечена страна Япония,
из геотэгов &amp;ndash; Бразилия и Германия, при этом блогер на самом деле живет в Англии.
Поэтому meta classifier должен не просто предсказывать страну, но еще
и выдавать вероятность правильного ответа. Если полученная вероятность
ниже некоторого порога, надо считать, что для определения страны у нас недостаточно информации.
Но majority voting не выдает значение вероятности,
можно только примерно оценить верхнюю и нижнюю границы.&lt;/li&gt;
&lt;li&gt;Majority voting не умеет работать с дополнительной информацией, не являющейся
меткой класса, например с языком аккаунта.&lt;/li&gt;
&lt;li&gt;Majority voting исходит из предположений, что предсказания base classifiers
не коррелируют друг с другом, и что вероятность
корректной классификации одинакова для любой страны. В реальной жизни
ни то ни другое не выполняется: надо учитывать и возможные взаимодействия
между классификаторами и индивидуальные особенности стран.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;catboost.jpeg&#34; width=&#34;400&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;gradient-boosting&#34;&gt;Gradient boosting&lt;/h2&gt;

&lt;p&gt;Учитывая все недостатки majority voting, была разработана более совершенная модель,
 использующая &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34; target=&#34;_blank&#34;&gt;градиентный бустинг&lt;/a&gt;&lt;/em&gt;.
  В данном проекте использовалась
 библиотека &lt;a href=&#34;https://tech.yandex.ru/catboost/&#34; target=&#34;_blank&#34;&gt;Yandex CatBoost&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Градиентный бустинг это один из самых лучших известных на сегодняшний день алгоритмов машинного обучения
для табличных данных. Поэтому качество полученной модели превзошло все ожидания.&lt;/p&gt;

&lt;h3 id=&#34;формирование-обучающих-данных&#34;&gt;Формирование обучающих данных&lt;/h3&gt;

&lt;p&gt;Чтобы обучить модель, надо сначала каким то образом получить разметку Instagram
аккаунтов в виде истинного места проживания владельца. Получить эти данные напрямую
от владельцев затруднительно, поэтому был использован косвенный источник: Twitter.&lt;/p&gt;

&lt;p&gt;Владелец Twitter аккаунта может указать свое место жительства (location), эта информация общедоступна.
 Если у этого же человека есть Instagram account, то можно использовать геоинформацию из Twitter,
 в качестве ground truth. Конечно геоданные из Twitter не являются абсолютно точными, т.к. люди не всегда
 указывают настоящее место жительства, или переезжают и забывают обновить location.
 По приблизительным оценкам, примерно у 1-3% Twitter аккаунтов информация
о location является неверной или устаревшей. Но, как показала практика, это не помешало обучению.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;location.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;Место жительства из Твиттера представляет собой строку произвольного формата. Иногда там указана
страна, иногда город, иногда штат, иногда просто афоризм или шутка, не имеющие отношения к географии.
Для обучения необходимо преобразовать эти данные к коду страны.
Задача получения географических координат из адреса произвольного формата называется
 &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Geocoding&#34; target=&#34;_blank&#34;&gt;геокодирование&lt;/a&gt;&lt;/em&gt;. В этом проекте для геокодирования была использована
 система &lt;a href=&#34;https://wiki.openstreetmap.org/wiki/Nominatim&#34; target=&#34;_blank&#34;&gt;Nominatim&lt;/a&gt;, созданная сообществом Open Street Maps.&lt;/p&gt;

&lt;p&gt;Задача геокодирования часто не имеет однозначного решения. Например, строка Moscow может обозначать столицу России,
 а может город в USA. Georgia может оказаться как страной Грузией, так и штатом в USA.&lt;/p&gt;

&lt;p&gt;Чтобы убрать неоднозначности и повысить точность, дополнительно использовался анализ поисковой выдачи Google.
В выдаче по строке с адресом проверялось наличие сниппета &amp;ldquo;географическое место&amp;rdquo;, и если сниппет найден,
в нём анализировалась ссылка на информацию в &lt;a href=&#34;https://www.wikidata.org&#34; target=&#34;_blank&#34;&gt;WikiData&lt;/a&gt;.
Если страна из WikiData совпадала со страной, которую
выдал Nominatim, результат геокодирования принимался, в противном случае назначалась дополнительная ручная
проверка. Идея здесь в том, что Google, на основе собственного анализа популярности поисковых запросов,
знает, какой город люди обычно имеют в виду, когда пишут &amp;ldquo;Moscow&amp;rdquo;. Как показали результаты, такая перепроверка
оказалась полезной, и значительно повысила точность геокодирования по сравнению с использованием чистого Nominatim.&lt;/p&gt;

&lt;h3 id=&#34;модель&#34;&gt;Модель&lt;/h3&gt;

&lt;p&gt;Самый очевидный способ моделирования это multiclass classification, где каждый класс
соответствует отдельной стране. Но такая модель не очень хорошо работает на практике.
Проблема в том, что распределение стран по количеству аккаунтов в них
очень неравномерное. Есть несколько доминирующих стран, в которых находится
основное количество аккаунтов, и длинный хвост из стран небольшого размера,
или в которых Instagram непопулярен. В результате для стран из &amp;ldquo;хвоста&amp;rdquo;
качество определения страны будет посредственным. Модели просто не хватит
данных для обучения в этих странах.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;acc_country.jpg&#34; alt=&#34;Зависимость точности (ось Y) от кол-ва аккаунтов страны в обучающей выборке (ось X, логарифмическая шкала), для одной из ранних моделей.&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Зависимость точности (ось Y) от кол-ва аккаунтов страны в обучающей выборке (ось X, логарифмическая шкала), для одной из ранних моделей.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;На рис. 1 видно, что с уменьшением количества аккаунтов на страну точность понижается.
Чтобы избежать этого эффекта, был выбран другой способ моделирования классов,
близкий по смыслу к majority voting, где модель вообще не отличает страны друг от друга,
а принимает во внимание только &amp;ldquo;силу&amp;rdquo; голосов за каждую страну.&lt;/p&gt;

&lt;p&gt;В данных каждого аккаунта базовые классификаторы обычно распознают не более
3-4 разных стран. Из этих стран и надо выбрать правильный ответ (или отсутствие ответа).
Поэтому для работы модели достаточно всего 4-x классов, каждый из которых
соответствует одной уникальной стране во входных данных, плюс один негативный класс
&amp;ldquo;Другое&amp;rdquo;, чтобы модель могла просигнализировать, что ни одна страна из предложенных не подходит.
Страны ранжируются в порядке популярности, т.е. самая часто встречающаяся
страна из предложенных принимается за класс №1, вторая по популярности за класс №2 и т.п.
В то же время, чтобы учесть индивидуальные особенности отдельных стран,
идентификаторы предложенных стран (country codes) тоже подаются на вход модели.&lt;/p&gt;

&lt;p&gt;Таким образом модель совмещает в себе лучшее и от majority voting (способность
обучаться на любых странах, даже если эта страна встретилась в обучающей выборке всего единожды)
и от multiclass c классами-странами (способность учитывать особенности
отдельной страны). Эксперименты показали, что точность такой модели для
мелких стран заметно выросла, без ухудшения точности для крупных.&lt;/p&gt;

&lt;h3 id=&#34;корректировка-весов-обучающих-примеров&#34;&gt;Корректировка весов обучающих примеров&lt;/h3&gt;




&lt;figure&gt;

&lt;img src=&#34;country_diff.png&#34; alt=&#34;Распределение долей top 18 стран в Twitter (по данным из обучающей выборки) и Instagram (по данным из геотэгов)&#34; width=&#34;600&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Распределение долей top 18 стран в Twitter (по данным из обучающей выборки) и Instagram (по данным из геотэгов)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Распределение стран в Twitter значительно отличается от распределения
стран в Instagram, это видно на рис. 2. Например, популярность Instagram
в России и в Иране значительно превосходит популярность Twitter. Если
использовать обучающие данные as is, модель запомнит распределение стран
из Twitter и будет подгонять свои ответы под это распределение. Вероятность
того, модель выдаст в ответе Иран, будет так же мала, как доля Ирана в Twitter.&lt;/p&gt;

&lt;p&gt;Чтобы избежать этого, при обучении
дополнительно задавались веса для каждого обучающего примера, равные
соотношению долей соответствующей примеру страны в Instagram и Twitter.&lt;/p&gt;

&lt;h3 id=&#34;результаты-обучения&#34;&gt;Результаты обучения&lt;/h3&gt;

&lt;p&gt;Была получена точность определения страны &lt;strong&gt;97%&lt;/strong&gt; при покрытии &lt;strong&gt;86%&lt;/strong&gt; (т.е. для 14% аккаунтов имеющейся информации
оказалось недостаточно для надежного принятия решения и выданная вероятность была ниже заданного порога).
Если вообще не использовать пороговую вероятность, и учитывать предсказания для любых аккаунтов,
   где есть хотя бы какая нибудь геоинформация, получится точность 93.4%.
 Цифры точности измерялись на тестовой выборке, т.е. на аккаунтах, которые модель ни разу не видела в процессе
 обучения и настройки гиперпараметров.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;acc_cover.png&#34; alt=&#34;Взаимозависимость между точностью и покрытием при разных значениях порога вероятности.&#34; width=&#34;500&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Взаимозависимость между точностью и покрытием при разных значениях порога вероятности.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;




&lt;figure&gt;

&lt;img src=&#34;features_unnorm.png&#34; alt=&#34;Важность входных features для модели.&#34; width=&#34;500&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Важность входных features для модели.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Как видно из рис. 4, наиболее важны country_id (идентификатор страны, позволяет модели
 учитывать индивидуальные особенности отдельных стран), геотэги из постов, язык блогера
и геоданные аудитории. Но эта диаграмма не совсем корректно отражает реальную важность,
потому что далеко не все features присутствуют в каждом аккаунте.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;features_norm.png&#34; alt=&#34;Важность входных features для модели. Features нормализованы по частоте.&#34; width=&#34;500&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Важность входных features для модели. Features нормализованы по частоте.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Если нормализовать важность features по частоте их появления в аккаунтах,
то будут лидировать features c самой точной геоинформацией:
телефонные номера (код страны), geotags, emoji (флаги стран).&lt;/p&gt;

&lt;p&gt;Можно заглянуть еще глубже, и проанализировать модель с использованием &lt;a href=&#34;https://github.com/slundberg/shap&#34; target=&#34;_blank&#34;&gt;SHAP framework&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.



&lt;figure&gt;

&lt;img src=&#34;shap_all.png&#34; alt=&#34;Feature importancе c использованием SHAP.&#34; width=&#34;400&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Feature importancе c использованием SHAP.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Чем дальше точки на рис. 6 отклонены от середины, тем больше влияет
на результат данная фича. Голубые точки это нулевые значения, обычно
соответствующие отсутствию feature во входных данных, красные &amp;ndash; ненулевые,
т.е. в данном контексте непустые.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;features_detail.png&#34; alt=&#34;Детализация Feature importancе (SHAP) для отдельных аккаунтов.&#34; width=&#34;700&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
  
  &lt;p&gt;
    Детализация Feature importancе (SHAP) для отдельных аккаунтов.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;На рис. 7 также видно, что наиболее “активными” features являются country_id, геотэги из постов, язык и emoji.&lt;/p&gt;

&lt;h2 id=&#34;образец-работы-модели&#34;&gt;Образец работы модели&lt;/h2&gt;

&lt;p&gt;Чтобы можно было посмотреть на реальные результаты работы модели, сформирована
демо-выборка из 2000 аккаунтов (по тестовым данным, которые модель не
 видела в процессе обучения). Выборка представляет собой CSV файл, состоящий из 5 колонок:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;instagram&lt;/strong&gt; &amp;ndash; Instagram аккаунт, для которого определялась страна&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;twitter&lt;/strong&gt; &amp;ndash; соответствующий Twitter account, источник &amp;ldquo;правды&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;prediction&lt;/strong&gt; &amp;ndash; предсказанная страна&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;truth&lt;/strong&gt; &amp;ndash; страна из Twitter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probability&lt;/strong&gt; &amp;ndash; вероятность, интерпретируемая как уверенность в предсказании&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Файл можно загрузить &lt;a href=&#34;geo_sample.csv.gz&#34; target=&#34;_blank&#34;&gt;здесь&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;В выборку включены результаты, где вероятность &amp;gt; 90%. Как видно по приведенным данным,
качество связки Instagram &amp;lt;&amp;ndash;&amp;gt; Twitter не идеальное, попадаются ошибки,
которые ухудшают замеренную результирующую точность. Поэтому реальная
точность модели возможно еще выше, чем замеренные 97%&lt;/p&gt;

&lt;h2 id=&#34;визуализации-геоданных&#34;&gt;Визуализации геоданных&lt;/h2&gt;

&lt;p&gt;В завершение &amp;ndash; пара дополнительных визуализаций географических данных Instagram,
полученных с помощью описанной выше модели.&lt;/p&gt;

&lt;h3 id=&#34;визуализация-1&#34;&gt;Визуализация 1&lt;/h3&gt;

&lt;p&gt;Подсчитаем распределение количества аккаунтов Instagram по странам мира.
Абсолютное количество, конечно, невозможно подсчитать, потому что для этого нужны были бы данные по всем аккаунтам,
 существующим в Instagram. Вместо этого можно взять достаточно большой сэмпл и посчитать,
  какой процент от всех аккаунтов принадлежит каждой стране.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;map_percent.png&#34; alt=&#34;Тройка лидеров по количеству аккаунтов это _USA, Brazil, Indonesia_. Меньше всего аккаунтов в странах _Африки_, _North Korea_, и _Greenland_.&#34; width=&#34;800&#34; /&gt;



&lt;figcaption data-pre=&#34;Рис. &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    Тройка лидеров по количеству аккаунтов это &lt;em&gt;USA, Brazil, Indonesia&lt;/em&gt;. Меньше всего аккаунтов в странах &lt;em&gt;Африки&lt;/em&gt;, &lt;em&gt;North Korea&lt;/em&gt;, и &lt;em&gt;Greenland&lt;/em&gt;.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;визуализация-2&#34;&gt;Визуализация 2&lt;/h3&gt;

&lt;p&gt;Предыдущая визуализация интересна, но очевидно, что чем больше население страны,
тем больше в ней будет Instagram accounts. Чтобы убрать зависимость от населенности,
можно подсчитать, какой процент от общего количества Instagram аккаунтов
 приходится &lt;em&gt;на одного жителя&lt;/em&gt; страны. Таким образом мы получим популярность Instagram среди населения.
&lt;img src=&#34;map_normed.png&#34; width=800/&gt;&lt;/p&gt;

&lt;p&gt;Видна совершенно другая картина.
Пятерка стран, где жители больше всего любят Instagram: &lt;em&gt;Cyprus, United Arab Emirates,
 Iceland, Qatar, Malaysia&lt;/em&gt;. Эти страны окрашены светло-желтым, поэтому их не очень хорошо заметно на карте.
 Среди крупных стран Instagram более всего популярен в &lt;em&gt;Brazil, Australia, USA&lt;/em&gt;.
Менее всего Instagram популярен в Африке и Азии.&lt;/p&gt;

&lt;h2 id=&#34;заключение&#34;&gt;Заключение&lt;/h2&gt;

&lt;p&gt;Проект передан в production, и успешно использовался
в компании Deep.Social, радикально улучшив качество определения места жительства
блогеров и их аудиторий, и убрав больше количество претензий клиентов к неточным данным.
В настоящий момент проект также используется в компаниях, купивших у Deep.Social права на
её продукт.&lt;/p&gt;

&lt;h4 id=&#34;возможные-улучшения&#34;&gt;Возможные улучшения:&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;Сейчас учитываемые геотэги просто суммируются, порядок их следования никак не учитывается
(bag of geotags). В то же время, в геотэгах часто прослеживается явная темпоральная структура, соответствующая
географическим перемещениям владельца аккаунта. Если учитывать эту структуру,
то есть работать с геотэгами, как с временным рядом, возможно будет
повысить точность. Например модель сможет различать ситуации, когда
владелец аккаунта поехал в командировку и когда он переехал на новое место жительства.&lt;/li&gt;
&lt;li&gt;Информация из фотографий сейчас не используется, т.к. обучение computer vision
классификатора занимает много времени, и проект не уложился бы в плановые сроки.
Если начать использовать эту информацию, будет дополнительный источник
данных, особенно актуальный для аккаунтов, которые не используют геотэги и не указывают
дополнительной информации в bio. Вырастет покрытие.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf&#34; target=&#34;_blank&#34;&gt;Zhi-Hua Zhou, Ensemble Learning&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Scott Lundberg, Su-In Lee (2017). A Unified Approach to Interpreting Model Predictions. &lt;a href=&#34;https://arxiv.org/abs/1705.07874&#34; target=&#34;_blank&#34;&gt;arXiv:1705.07874&lt;/a&gt; [cs.AI]
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
