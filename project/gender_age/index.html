<!DOCTYPE html>
<html lang="ru">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.0.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Артур Суилин">

  
  
  
  
    
  
  <meta name="description" content="Принципы работы модели, архитектура, подготовка данных, обучение. Результаты работы модели превосходят результаты всех коммерческих поставщиков аналогичной услуги по состоянию на июль 2018 года (Microsoft, Amazon, etc)">

  
  <link rel="alternate" hreflang="ru" href="https://suilin.ru/project/gender_age/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#086377">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Fira+Sans+Condensed:400,400italic,700|PT+Mono|Fira+Sans|PT+Serif:400,400italic,700">
  

  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/custom.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-38480657-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://suilin.ru/index.xml" type="application/rss+xml" title="Артур Суилин">
  <link rel="feed" href="https://suilin.ru/index.xml" type="application/rss+xml" title="Артур Суилин">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://suilin.ru/project/gender_age/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@asuilin">
  <meta property="twitter:creator" content="@asuilin">
  
  <meta property="og:site_name" content="Артур Суилин">
  <meta property="og:url" content="https://suilin.ru/project/gender_age/">
  <meta property="og:title" content="Определение пола и возраста по фото | Артур Суилин">
  <meta property="og:description" content="Принципы работы модели, архитектура, подготовка данных, обучение. Результаты работы модели превосходят результаты всех коммерческих поставщиков аналогичной услуги по состоянию на июль 2018 года (Microsoft, Amazon, etc)">
  
  
    
  <meta property="og:image" content="https://suilin.ru/img/project/gender_age/montage.jpg">
  <meta property="og:locale" content="ru">
  
  <meta property="article:published_time" content="2018-08-27T00:00:00&#43;03:00">
  
  <meta property="article:modified_time" content="2018-08-27T00:00:00&#43;03:00">
  

  

  <meta property="og:type" content="article"/>
<meta name="yandex-verification" content="5753f991e1c3d96c" />
<meta name="google-site-verification" content="5sd_7MZqiS3rR-1B3f-FqUkrWEd92TYr9FNEGkFE14I" />

<script type="text/javascript" >
  (function (d, w, c) {
    (w[c] = w[c] || []).push(function() {
      try {
        w.yaCounter21550249 = new Ya.Metrika2({
          id:21550249,
          clickmap:true,
          trackLinks:true,
          accurateTrackBounce:true,
          webvisor:true
        });
      } catch(e) { }
    });

    var n = d.getElementsByTagName("script")[0],
      s = d.createElement("script"),
      f = function () { n.parentNode.insertBefore(s, n); };
    s.type = "text/javascript";
    s.async = true;
    s.src = "https://mc.yandex.ru/metrika/tag.js";

    if (w.opera == "[object Opera]") {
      d.addEventListener("DOMContentLoaded", f, false);
    } else { f(); }
  })(document, window, "yandex_metrika_callbacks2");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/21550249" style="position:absolute; left:-9999px;" alt="" /></div></noscript>



  <title>Определение пола и возраста по фото | Артур Суилин</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Артур Суилин</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Переключить навигацию">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        

        <li class="nav-item">
          <a class="nav-link" href="/">
            
            <span>Главная</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/post/">
            
            <span>Блог</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/project/">
            
            <span>Проекты</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Контакты</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article article-project" itemscope itemtype="http://schema.org/Article">

  









<div class="article-header">
  
  
  <img src="/img/project/gender_age/montage.jpg" class="article-banner" itemprop="image" alt="">
  

  <span class="article-header-caption">Аватары пользователей Instagram</span>
</div>




  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Определение пола и возраста по фото</h1>

  
  <p class="page-subtitle">Проект был выполнен для компании Deep.Social</p>
  

  
    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Артур Суилин">
  </span>
  

  <span class="article-date">
    
    <meta content="2018-08-27 00:00:00 &#43;0300 MSK" itemprop="datePublished">
    <time datetime="2018-08-27 00:00:00 &#43;0300 MSK" itemprop="dateModified">
      27.08.2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Артур Суилин">
  </span>

  

  
  

  

  
  

  

</div>

  
</div>



  <div class="article-container">
    

    <div class="article-style" itemprop="articleBody">
      

<h2 id="введение">Введение</h2>

<p>Для рекламодателей, использующих influencer marketing, важно понимать,
у какого блогера аудитория наиболее соответствует рекламируемым товарам и услугам.
Довольно бессмысленно рекламировать деловые костюмы девочкам-подросткам, так же как и
продвигать женскую косметику среди аудитории мужчин за 30.</p>

<p>Но сам Instagram не предоставляет никакой соцдем информации по аудитории блогера,
поэтому рекламодателям приходится работать с блогерами исключительно на основе своих
предположений о составе их аудитории. Единственный способ подтвердить эти
предположения &ndash; просмотреть выборку из фолловеров интересующего блогера и оценить на глаз их возраст и пол.
Это долгая, неинтересная, и немасштабируемая работа, к тому же не совсем
объективная, т.к. разные люди оценят возраст по разному.</p>

<p>Но почему бы не поручить эту работу машине? Современные технологии Computer Vision
уже достаточно развиты, чтобы справиться с этой задачей без участия человека.</p>

<h2 id="постановка-задачи">Постановка задачи</h2>

<p>На входе есть аватары Instagram пользователей. Необходимо понять, есть ли на аватаре
люди, и сколько их. Если изображён один человек, предсказать его возраст и пол. При этом
масштаб изображения может быть разным: на аватаре может быть портрет или даже часть лица, может быть
человек в полный рост, может быть что-то промежуточное.
Фото может быть цветным, чёрно-белым, тонированным,
пропущенным через искажающие фильтры, с дорисованными частями и т.п.</p>

<p>Естественным образом задача разделяется на две основные части:</p>

<ol>
<li>Обнаружение на фотографии людей</li>
<li>Определение пола и возраста.</li>
</ol>

<h2 id="обнаружение-людей">Обнаружение людей</h2>

<p>Пол и возраст определяется прежде всего по лицу, поэтому обнаружением
человека считается наличие лица в кадре. Нога, рука или спина, хотя
и говорят о наличии человека на фото, для решаемой задачи не подходят.
Безусловно, можно обучить computer vision модель, которая будет отличать мужскую ногу от женской,
но на покрытие всех таких ситуаций ушло бы слишком много ресурсов,
при сравнительно небольшой отдаче &ndash; всё таки лица изображены на аватарах гораздо чаще, чем ноги.</p>

<p>Обнаружение лиц на фото это известная и хорошо проработанная задача computer vision,
поэтому здесь работа свелась к поиску подходящей модели и адаптации её
под требования проекта. Основными требованиями были приемлемая скорость работы
 (аудитория блогеров это сотни миллионов пользователей) и наличие уже
 обученного и готового к использованию варианта (чтобы не тратить время
 на разметку данных и обучение).</p>

<p>Одним из дополнительных пожеланий было совмещение моделью
двух функций: собственно нахождения лиц на фото, и определения опорных
точек (face landmarks). Опорные точки это обычно центры глаз, кончик носа,
углы губ и другие топологические точки, положение которых на лице может быть
однозначно определено. Зачем они нужны?</p>

<p><a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C" target="_blank">Свёрточные сети</a>
 (convolutional networks), используемые в моделях
компьютерного зрения, обладают свойством трансляционной инвариантности (translational invariance),
но не обладают (или обладают в ограниченном объеме) свойствами масштабной инвариантности (scale invariance)
и инвариантности к повороту (rotation invariance). Это означает, что если изображение
одного и того же лица смещается на фото в разные положения, то с точки зрения нейросети
это будет то же самое лицо (трансляционная инвариантность). Но если лицо поворачивается
или изменяется его масштаб, для нейросети это будут разные лица. Поэтому, чтобы
облегчить задачу для нейросети, лучше приводить все лица к единому масштабу и
единому вертикальному положению, а для этого нужна привязка к надёжным опорным точкам (например,
считать стандартным размером лица расстояние в 100 пикселей между горизонталью глаз и горизонталью губ,
и приводить все лица к этому масштабу)



<figure>

<img src="invariance.jpg" alt="Инвариантность для свёрточных сетей" width="500" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Инвариантность для свёрточных сетей
    
    
    
  </p> 
</figcaption>

</figure></p>

<p>С учетом перечисленных требований, задача свелась к выбору из двух моделей: MTCNN<sup><a href="#zhang2016" id="zhang2016_t">[1]</a></sup>
 и детектора из библиотеки <a href="http://dlib.net/" target="_blank">dlib</a>:
  <a href="http://blog.dlib.net/2016/10/easily-create-high-quality-object.html" target="_blank">CNN Face detector</a> +
<a href="http://blog.dlib.net/2017/09/fast-multiclass-object-detection-in.html" target="_blank">5-point landmark detector</a>.</p>

<h2 id="mtcnn-vs-dlib">MTCNN vs dlib</h2>

<p>Обе модели продемонстрировали одинаково высокое качество обнаружения лиц,
ошибаясь крайне редко. С определением опорных точек ошибок больше,
особенно на лицах, сильно наклонённых, или повёрнутых ближе к положению &ldquo;профиль&rdquo;, чем к положению &ldquo;анфас&rdquo;.
Но и здесь нет явного лидера. MTCNN более корректно определяет границы лица (bounding box),
 лучше распараллеливает обработку, плюс имеет хороший запас в коде для будущего ускорения, поэтому
для дальнейшей работы была выбрана именно эта библиотека.
Рассматривалась также библиотека <a href="https://github.com/1adrianb/face-alignment" target="_blank">face_alignment</a><sup><a href="#bulat2017far" id="bulat2017far_t">[2]</a></sup>, она даёт
даже избыточное дял данной задачи качество, но при этом медленно работает, и требует готовых bounding boxes.</p>




<figure>

<img src="good_detect.jpg" alt="Примеры определения границ лица и опорных точек библиотеками MTCNN (зелёный прямоугольник, зелёные точки), dlib (красный прямоугольник, фиолетовые точки), face-alignment (белые миниатюрные точки)" width="668" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Примеры определения границ лица и опорных точек библиотеками MTCNN (зелёный прямоугольник, зелёные точки), dlib (красный прямоугольник, фиолетовые точки), face-alignment (белые миниатюрные точки)
    
    
    
  </p> 
</figcaption>

</figure>




<figure>

<img src="bad_detect.jpg" alt="Примеры некорректной работы детектора. Первое фото - захват искусственного изображения вместо лица. Второе - некорректная работа dlib (фиолетовые точки) на лице, частично повернутом в анфас" width="262" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Примеры некорректной работы детектора. Первое фото - захват искусственного изображения вместо лица. Второе - некорректная работа dlib (фиолетовые точки) на лице, частично повернутом в анфас
    
    
    
  </p> 
</figcaption>

</figure>

<h3 id="архитектура-mtcnn">Архитектура MTCNN</h3>

<p><img src='mtcnn_nets.png' width="600"/></p>

<p>MTCNN состоит из трех CNN-&ldquo;стадий&rdquo; P-Net, R-Net и O-Net, каждая из которых
уточняет результаты предыдущей ступени, и предварительной стадии построения
&ldquo;пирамиды изображений&rdquo;. Пирамида представляет из себя просто набор уменьшенных копий
входного изображения. MTCNN заранее не знает, в каком масштабе будут лица
на фото, а свёрточные сети, как уже говорилось выше, не инвариантны
к масштабированию. Поэтому приходится готовить несколько
версий входного изображения в разных масштабах, и искать лица на каждой версии.</p>

<p>Задача стадий - определить границы (bounding box) всех лиц на изображении.
Первая стадий имеет самую простую архитектуру и работает очень быстро,
но при этом генерирует много ошибок первого рода (false positives). Задача
следующих стадий, более сложных и мощных &ndash; выбрать из предложенных
bounding boxes наиболее похожие на правду, уточнить их координаты, и передать дальше.
Последняя ступень также определяет координаты опорных точек внутри
результирующих bounding boxes. В конце каждой стадии сильно пересекающиеся друг с другом bounding boxes
сливаются в один bounding box с помощью алгоритма NMS (non-max suppression):</p>




<figure>

<img src="mtcnn_stages.jpg" alt="Стадии работы MTCNN" width="400" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Стадии работы MTCNN
    
    
    
  </p> 
</figcaption>

</figure>

<p>Трёхстадийная архитектура позволяет MTCNN работать быстро, т.к.
всю черновую работу делает простая первая ступень, а следующие занимаются
только уточнением результатов. Как показали замеры, основное время уходит на построение
пирамиды изображений, а не на собственно работу свёрточных сетей.
Переход на более быстрые алгоритмы уменьшения изображений позволяет
ещё в разы поднять производительность.</p>

<h3 id="нормализация-лиц">Нормализация лиц</h3>

<p>Перед тем, как отдавать найденные лица в детектор пола/возраста, их необходимо
<em>нормализовать</em>, т.е. привести лица разного масштаба, по разному повёрнутые и наклонённые,
к одному стандартному виду &ldquo;как на паспорт&rdquo;.</p>

<p>Существует довольно много методик нормализации, от продвинутых, натягивающих
лицо как скин на 3D модель и манипулирующих этой моделью в пространстве,
чтобы она смотрела прямо в объектив и заполняла весь кадр, находясь в его центре, до простых, ограничивающихся приведением лиц
к близкому масштабу. Какую из них выбрать?</p>

<p>Я исходил из принципа минимального вмешательства: нормализация должна выдавать
только естественную форму лица, которая встречается в природе. Если нормализация
изменяет пропорции раздельно по осям X и Y, изменяет параллельность линий,
или тем более натягивает лицо на 3D сетку, то она может принести больше
вреда, чем пользы. Это подтверждают исследования<sup><a href="#ludwiczuk2017" id="ludwiczuk2017_t">[3]</a></sup>
результатами которых я активно пользовался при работе над этим проектом.</p>

<p>Принципу сохранения естественных пропорций лица соответствует
 <a href="https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B4%D0%BE%D0%B1%D0%B8%D0%B5" target="_blank">преобразование подобия</a> (similarity transform) ,
т.е. набор действий ограничивается сочетанием сдвига, вращения и масштабирования. Это
преобразование является частным случаем аффинного преобразования и описывается
c помощью следующей <a href="https://en.wikipedia.org/wiki/Transformation_matrix" target="_blank">матрицы перехода</a> :
  $$ \mathbf{A} =
   \begin{bmatrix}
    a_0 &amp; b_0 &amp; a_1 \\<br />
    b_0 &amp; a_0 &amp; b_1 \\<br />
    0  &amp; 0  &amp; 1
   \end{bmatrix} $$</p>

<p>$$\begin{bmatrix}x&rsquo;\\y&rsquo;\\1\end{bmatrix}=\mathbf{A}\begin{bmatrix}x\\y\\1\end{bmatrix}$$</p>

<p>или в скалярной форме:
 $$ x&rsquo; = a_0x - b_0y + a_1 = sx\cos(\theta) - sy\sin(\theta) + a_1 $$
 $$ y&rsquo; = b_0x + a_0y + b1 = sx\sin(\theta) + sy\cos(\theta) + b_1 $$</p>

<p>где $(x,y)$ и ($x&rsquo;,y&rsquo;)$ &ndash; координаты исходной и результирующей точек изображения,
$\theta$ &ndash; угол поворота, $s$ &ndash; коэффициент масштабирования.</p>

<p>Чтобы найти коэффициенты для матрицы перехода, используются опорные точки.
Есть координаты пяти опорных точек  $(\mathbf{s}_1 \dots \mathbf{s}_5),\: \mathbf{s}_i = [x_i, y_i, 1]^\top$ &ldquo;эталонного лица&rdquo;,
 имеющего правильный масштаб,
вертикальную ориентацию и расположенного в центре кадра. И есть координаты
опорных точек на фото $(\mathbf{a_1} \dots \mathbf{a_5})$, которые нашёл MTCNN. Задача &ndash; найти такие коэффициенты
 для $\mathbf{A}$, чтобы после преобразования точки c фото
по возможности совпали с эталонными точками, т.е. минимизировать
расстояние между эталоном и преобразованными точками с фото:
  $$\operatorname*{arg\,min}_\mathbf{A} \frac{1}{n}\sum_{i=1}^n \|\mathbf{s}_i - \mathbf{Aa}_i\|$$
Эта задача решается с помощью метода Umeyama<sup><a href="#umeyama1991" id="umeyama1991_t">[4]</a></sup></p>




<figure>

<img src="normalized.jpg" alt="Примеры нормализации лиц, слева исходное фото, справа результат." width="250" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Примеры нормализации лиц, слева исходное фото, справа результат.
    
    
    
  </p> 
</figcaption>

</figure>

<h2 id="определение-пола-и-возраста">Определение пола и возраста</h2>

<h3 id="выбор-модели">Выбор модели</h3>

<p>Модель для определения пола и возраста подбиралась из примерно тех же критериев,
что и модель для обнаружения лиц:</p>

<ol>
<li>Приемлемая скорость работы (и для обучения, и для предсказаний).</li>
<li>Наличие pretrained модели, чтобы не обучать всё с нуля.</li>
</ol>

<p>Рассматривались три архитектуры: ResNet<sup><a href="#he2016" id="he2016_t">[5]</a></sup>, NASNet<sup><a href="#zoph2017learning" id="zoph2017learning_t">[6]</a></sup>, MobileNet<sup><a href="#howard2017mobilenets" id="howard2017mobilenets_t">[7]</a></sup>.
За baseline был взят хорошо известный ResNet-50.</p>

<p>NASNet-A-Mobile-224, сконструированный с помощью &ldquo;искусственного интеллекта&rdquo;, содержит примерно в 5 раз параметров,
чем ResNet-50. Но на практике он обучался не в 5 раз быстрее, а даже медленнее, чем ResNet-50.</p>

<p>Аналогично mobilenet-1.0-224, несмотря на то, что содержит в разы меньше параметров,
чем ResNet-50, на практике обучался со скоростью, сопоставимой с ResNet-50,
показывая при этом худшие результаты предсказаний. Видимо, эта архитектура имеет смысл
именно для мобильных устройств, а не для стационарных GPU.</p>

<p>В итоге победил ResNet-50, как архитектура, оптимальным образом использующая
стационарный GPU (обучение шло на видеокартах GTX 1080TI).</p>

<h3 id="архитектура">Архитектура</h3>

<p>На выходе надо получить два предсказания, пол и возраст.
Можно обучить две отдельных модели, но эффективнее использовать <a href="https://en.wikipedia.org/wiki/Multi-task_learning" target="_blank">multitask
learning</a> и получить одну модель, выдающую одновременно два прогноза.
Тем более пол и возраст с точки зрения здравого смысла не являются независимыми переменными, женщины и мужчины
взрослеют и стареют по разному:
$$P_{gender}(D) \neq P_{gender}(D|age)$$
$$P_{age}(D) \neq P_{age}(D|gender)$$
таким образом знание моделью пола будет помогать предсказывать возраст и наоборот.</p>

<p>Предсказание возраста это регрессия, для регрессионных задач в качестве целевой функции
обычно используют MSE (среднеквадратичную ошибку от предсказываемой переменной). Но в данном случае прямое применение
MSE не оправдано. Во-первых возраст не может быть отрицательным. Во-вторых, MSE предполагает линейную шкалу.
В реальности разница в 5 лет между
2-x и 7-летними детьми это гораздо больше, чем разница в 5 лет между 60-летним и 65-летним человеком,
т.е. человеческий возраст имеет скорее логарифмическую шкалу. Поэтому на входе
в модель возраст трансформировался из исходного возраста $age$:
$$a=\log(age + \gamma)$$
где $\gamma$ это эмпирическая сглаживающая константа, чтобы разница между
новорожденным и взрослым не устремилась в бесконечность. Модель внутри себя
везде использует логарифмический возраст $a$, при выдаче результатов пользователю
он переводится обратно в линейную шкалу:
$$\widehat{age}=\exp(\hat{a}) - \gamma$$</p>

<p>Так как определение возраста по фото это сложная задача, с которой даже люди
далеко не всегда справляются, хотелось получить на выходе также оценку неопределённости
(uncertainty estimation). Для этого можно предсказывать не точечную
 оценку возраста, а распределение вероятностей, каким он мог бы быть. Для простоты было принято, что ошибка определения
возраста имеет Гауссовское распределение, т.е.
$$\hat{a} \sim \mathcal{N}(a, \sigma^2)$$
где $\hat{a}$ это предсказанный возраст, $a$ - истинный возраст,
$\sigma^2$ &ndash; дисперсия, отражающая степень неуверенности в прогнозе.</p>

<p>Тогда за целевую функцию можно принять то, насколько хорошо предсказанное распределение
соответствует истинному возрасту, т.е. функцию правдоподобия (likelihood):
$$\mathcal{L}(\hat{a}, \hat{\sigma} | a) = \prod_{i=1}^{n}\frac{1}{\hat{\sigma}_i\sqrt{2\pi}}\exp\left(-\frac{(a_i-\hat{a}_i)^2}{2\hat{\sigma}_i^2}\right)$$
Использование произведения по всем примерам из батча может привести к проблемам с точностью вычислений с плавающей точкой,
поэтому на практике используют негативную логарифмическую функцию правдоподобия (negative log-likelihood):
$$\ell=\log(\mathcal{L})=-\frac{n}{2}\log(2\pi)-\sum_{i=1}^{n}\log\hat{\sigma}_i-\sum_{i=1}^{n}\frac{(a_i-\hat{a}_i)^2}{2\hat{\sigma}_i^2}$$
$$L_{age} = -\ell$$</p>

<p>Определение пола является бинарной классификацией, в качестве целевой функции в модели используется
стандартная для таких задач перекрестная энтропия:
$$L_{gender} = -{(y\log(p) + (1 - y)\log(1 - p))}$$
где $y$ это бинарная метка класса, например 1 соответствует мужчине, 0 женщине,
а $p$ это предсказанная вероятность принадлежать классу с меткой 1, в данном случае
вероятность быть мужчиной.</p>

<p>Результирующая целевая функция является просто суммой функций по полу и по возрасту:
$$L=L_{age} + kL_{gender}$$
Абсолютные значения целевых функций находятся в разных шкалах, например
значение $L_{age}$ зависит от того, в каких единицах измеряется возраст.
Чтобы привести их к более-менее одному масштабу, нужен выравнивающий коэффициент
$k$, значение которого подбирается эмпирически.</p>

<h2 id="обучающая-выборка">Обучающая выборка</h2>

<p>В идеале обучающая выборка должна быть из того же распределения,
что и данные, на которых потом модель будет делать предсказания.
Это означает ручную разметку фотографий из Instagram, т.к. достоверную информацию
о возрасте владельцев аккаунтов взять негде.</p>

<p>Ручная разметка пола это в принципе посильная задача, а вот с возрастом
всё не так просто. Люди определяют возраст на глаз крайне субъективно,
это означает большую дисперсию и затруднительность контроля результатов.
Для того, чтобы получить от работников надёжный результат, обычно одна и та же
задача даётся трём-пяти людям, и за верный результат принимается большинство голосов.
Работник, часто дающий ошибочные результаты, заменяется. Для возраста
такая схема работать не будет, т.к. чтобы получить надёжную картину
максимума распределения возрастов для каждого фото и отсеять выбросы,
пришлось бы давать оценить каждое фото 10-20 людям, что было бы слишком затратно.</p>




<figure>

<img src="age_guess.png" alt="Распределение разницы между средним оценочным возрастом (по &gt; 10 оценкам от разных людей) и реальным возрастом. Источник: AgeGuess database, J. A. Barthold Jones et al, 2018, arXiv:1803.10063" width="400" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Распределение разницы между средним оценочным возрастом (по &gt; 10 оценкам от разных людей) и реальным возрастом. Источник: AgeGuess database, J. A. Barthold Jones et al, 2018, arXiv:1803.10063
    
    
    
  </p> 
</figcaption>

</figure>

<p>Оценка возраста людьми может сильно расходиться с реальным возрастом,
см. приведённый рисунок. Кроме того, оценка возраста зависит еще и от национальности и культурного контекста
оценщика, т.е. пришлось бы набирать распределённую по разным точкам мира команду.</p>

<p>Поэтому были рассмотрены другие источники. Стандартный dataset,
 используемый в академических кругах для подобных задач, это <a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/" target="_blank">IMDB-WIKI</a><sup><a href="#rothe2016dex" id="rothe2016dex_t">[8]</a></sup></p>

<p>Однако, качество разметки этого dataset-а, особенно в данных из IMDB, крайне низкое,
и неприемлемо для проекта, который будет использоваться в production.</p>

<p>Остальные доступные datasets: (<a href="https://talhassner.github.io/home/projects/Adience/Adience-data.html" target="_blank">Adience</a><sup><a href="#eidinger2014age" id="eidinger2014age_t">[9]</a></sup>,
 <a href="https://susanqq.github.io/UTKFace/" target="_blank">UTKFace</a><sup><a href="#zhifei2017cvpr" id="zhifei2017cvpr_t">[10]</a></sup>) слишком малы для полноценного обучения.</p>

<p>В результате самым продуктивным оказался самостоятельный автоматизированный сбор данных
из социальных сетей и Интернет-сайтов, с последующей модерацией.
Но найти хорошие источники размеченных фотографий для возрастов &lt;17 так и не удалось,
поэтому для этой возрастной категории были выкачаны фото из Инстаграм,
содержащие тэги, указывающие на возраст (обычно такие тэги бывают в фото с дней рождения).
Пол в этих фото размечался вручную, релевантность содержимого фото (что на ней изображены именно дети)
и разметки возраста (возраст с тэга совпадает с визуальным возрастом) контролировались командой модераторов.</p>

<p>Работа по получению и разметке обучающих данных была самым долгим этапом проекта
и заняла около 4-х месяцев.</p>

<h2 id="подготовка-данных">Подготовка данных</h2>

<p>В обучающей выборке выравнивалось количество мужчин и женщин
из каждой страны (например в арабских странах и в Индии женщины представлены в онлайне слабо,
и без выравнивания модель могла просто не научиться с ними работать).</p>

<p>Распределение количества
фото по странам по возможности приводилось в соответствие с распределением
кол-ва аккаунтов по странам в Instagram. При этом странам с преобладающим азиатским или негроидным населением
давался больший вес, чтобы в обучающей выборке не доминировала европеоидная раса
и у модели было достаточно данных, чтобы научиться работать с азиатскими
и негроидными типами лиц.</p>

<p>Также убирались перекосы по возрастам, чтобы распределение возрастов
было похожим на распределение, заявленное для Instagram в открытых источниках.</p>

<p>В итоговую выборку попало около 3 млн. фото взрослых и около 300 тыс.
фото детей и подростков.</p>

<h3 id="аугментация-данных">Аугментация данных</h3>

<p><a href="https://blog.algorithmia.com/introduction-to-dataset-augmentation-and-expansion/" target="_blank">Аугментация</a>
проводилась по тому же принципу, что и нормализация лиц:
на выходе должны получаться только такие фото, которые могут встретиться
в естественных условиях. Зашумленных, нерезких, тонированных и частично обрезанных
фото было и так достаточно в обучающих данных, при этом качество фото
на аватарах в Instagram в среднем достаточно высокое. Поэтому разновидности
аугментации, &ldquo;ухудшающие&rdquo; фото, не применялись.</p>

<p>Аугментация свелась к вертикальному зеркалированию (flip), случайному повороту на небольшой угол
и случайному кропу. MTCNN не абсолютно точно и одинаково определяет опорные
 точки на всех фото, от лица к лицу возможны вариации,
два последних вида аугментации как раз учат нейросеть справляться с такими отклонениями.</p>

<h2 id="обучение">Обучение</h2>

<p>Использовалась модель ResNet-50, предварительно обученная на данных Imagenet.
У неё убирался последний слой, отвечающий за классификацию ImageNet,
и вместо него добавлялся полносвязный (fully connected, FC) слой, генерирующий предсказания для модели
(3 выходных значения: пол, матожидание и дисперсия возраста).</p>

<p>Обучение проводилось в два этапа, сначала обучался только добавленный
FC слой, затем, когда ошибка переставала уменьшаться, в обучение включалась
вся модель. Transfer learning, т.е. обучение только последнего слоя,
сам по себе давал посредственные результаты: точность определения пола
была не выше 85%. Это объяснимо, т.к. человеческие лица представляют собой
довольно узкий и специфический домен, сильно отличающийся от данных ImageNet, к тому же в ImageNet
не существует классов &ldquo;человек&rdquo; или &ldquo;лицо&rdquo;.</p>

<p>Использовался оптимизатор Nesterov Momentum и cosine learning rate decay с рестартами<sup><a href="#loshchilov2016sgdr" id="loshchilov2016sgdr_t">[11]</a></sup>
Максимальный и минимальный learning rate подбирался с помощью техники <em>LR range test</em> <sup><a href="#smith2017cyclical" id="smith2017cyclical_t">[12]</a></sup></p>




<figure>

<img src="lr_test.png" alt="Один из результатов LR range test, ось Y - loss, ось X - learning rate, логарифмическая шкала" width="400" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Один из результатов LR range test, ось Y - loss, ось X - learning rate, логарифмическая шкала
    
    
    
  </p> 
</figcaption>

</figure>

<p>Один полный прогон обучения занимал около пяти дней.</p>

<h2 id="результаты">Результаты</h2>

<p>Результаты работы обученной модели сравнивались с результатами 4-х крупных коммерческих
систем Computer Vision: <a href="https://aws.amazon.com/rekognition/" target="_blank">AWS Rekognition</a>,
 <a href="https://azure.microsoft.com/services/cognitive-services/face/" target="_blank">Microsoft Azure</a>, <a href="https://www.faceplusplus.com/attributes/" target="_blank">Face++</a>,
  <a href="https://clarifai.com/models/demographics-image-recognition-model-c0c0ac362b03416da06ab3fa36fb58e3" target="_blank">Clarifai</a>.
Чтобы сравнение было объективным, замеры проводились не только на собственной
тестовой выборке, но и на дополнительных datasets: <a href="https://talhassner.github.io/home/projects/Adience/Adience-data.html" target="_blank">Adience</a><sup><a href="#eidinger2014age" id="eidinger2014age_t">[9]</a></sup>
 и <a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/" target="_blank">IMDB-WIKI</a><sup><a href="#rothe2016dex" id="rothe2016dex_t">[8]</a></sup>.</p>

<p>Для возраста точность оценивалась по ошибке MAPE (Mean absolute percentage error),
показывающей отклонение в процентах определенного моделью возраста от реального.</p>

<p>$$MAPE=\frac{100\%}{n}\sum_{i=1}^{n}\left|\frac{age_i - \widehat{age}_i}{age_i}\right|$$</p>

<p>где $age$ &ndash; истинный возраст, $\widehat{age}$ - предсказанный возраст</p>

<h3 id="собственный-dataset">Собственный dataset</h3>

<p>Распределение возрастов соответствует естественному распределению в социальных сетях,
откуда были собраны данные.</p>

<table>
<thead>
<tr>
<th>Service</th>
<th align="center">Gender accuracy, %</th>
<th align="center">Age MAPE, %</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Ours</strong></td>
<td align="center"><strong>99.3</strong></td>
<td align="center"><strong>14.4</strong></td>
</tr>

<tr>
<td>Face++</td>
<td align="center">92.2</td>
<td align="center">59.3</td>
</tr>

<tr>
<td>Clarifai</td>
<td align="center">84.8</td>
<td align="center">47.7</td>
</tr>

<tr>
<td>Azure</td>
<td align="center">96.9</td>
<td align="center">34.2</td>
</tr>

<tr>
<td>AWS Rekognition</td>
<td align="center">91.1</td>
<td align="center">38.1</td>
</tr>
</tbody>
</table>

<h3 id="imdb-wiki">IMDB-Wiki</h3>

<p>Из IMDB-WIKI были отброшены данные IMDB, как содержащие огромное количество
неточностей, в данных WIKI был дополнительно вручную исправлен пол там, где
были явные ошибки.
Для тестов были взяты фото людей в диапазоне возрастов 13-44 года (актуальный диапазон для Instagram).
Также были отброшены фотографии, сделанные до 2005 года, т.к. стилистика этих фото (косметика, причёски) отличается от современной,
и фото такой давности редко встречаются в соцсетях.</p>

<table>
<thead>
<tr>
<th>Service</th>
<th align="center">Gender accuracy, %</th>
<th align="center">Age MAPE, %</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Ours</strong></td>
<td align="center"><strong>98.7</strong></td>
<td align="center"><strong>14.9</strong></td>
</tr>

<tr>
<td>Face++</td>
<td align="center">92.6</td>
<td align="center">36.6</td>
</tr>

<tr>
<td>Clarifai</td>
<td align="center">91.9</td>
<td align="center">35.9</td>
</tr>

<tr>
<td>Azure</td>
<td align="center">89.4</td>
<td align="center">24.6</td>
</tr>

<tr>
<td>AWS Rekognition</td>
<td align="center">94.0</td>
<td align="center">43.8</td>
</tr>
</tbody>
</table>

<h3 id="adience">Adience</h3>

<p>Возраст в Adience указан в виде диапазона а не точного значения, поэтому для него использовались
другие метрики точности:</p>

<ul>
<li><em>Age accuracy</em> &ndash; процент попаданий предсказанного возраста в правильный диапазон.</li>
<li><em>Age accuracy one-off</em> &ndash; процент попаданий или в правильный диапазон или в два соседних с ним.</li>
</ul>

<table>
<thead>
<tr>
<th>Service</th>
<th align="center">Gender accuracy, %</th>
<th align="center">Age acc., %</th>
<th align="center">Age acc. one-off, %</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Ours</strong></td>
<td align="center"><strong>97.7</strong></td>
<td align="center">19.8</td>
<td align="center">83.8</td>
</tr>

<tr>
<td>Face++</td>
<td align="center">86.3</td>
<td align="center">14.8</td>
<td align="center">65.1</td>
</tr>

<tr>
<td>Clarifai</td>
<td align="center">84.6</td>
<td align="center">25.7</td>
<td align="center">78.4</td>
</tr>

<tr>
<td>Azure</td>
<td align="center">94.8</td>
<td align="center"><strong>36.9</strong></td>
<td align="center"><strong>89.8</strong></td>
</tr>

<tr>
<td>AWS Rekognition</td>
<td align="center">88.9</td>
<td align="center">21.3</td>
<td align="center">82.9</td>
</tr>
</tbody>
</table>

<p>В этом тесте точность определения возраста у модели уступает некоторым коммерческим сервисам (на первом месте - Azure).
Объясняется это тем, что Adience это академический dataset, в котором все возраста от 0 до 80 лет присутствуют
в примерно равной пропорции. Модель же обучалась под распределение возрастов,
наблюдающееся в реальной жизни в Instagram и социальных сетях, которое весьма далеко от равномерного (доминирует возраст 18-30).
Соответственно, на равномерном распределении точность модели хуже, т.к. при прочих равных предпочтение отдаётся возрастам в диапазоне 18-30.</p>

<p>Если бы целью было показать хороший результат именно на Adience,
надо было бы обучить модель на выборке с равномерным сэмплированием по всем возрастам.</p>

<h2 id="на-что-смотрит-модель">На что смотрит модель?</h2>

<p>Было бы интересно понять, какие области лица играют главную роль при
определении пола/возраста. Большинство традиционных методы выявления областей
внимания для этой модели, к сожалению, не выдают наглядных результатов, т.к.
в результате нормализации лицо занимает практически весь кадр, и вся его
площадь является активной областью. Самые наглядная визуализация получилась
 при использовании библиотеки SHAP<sup><a href="#lundberg2017shap" id="lundberg2017shap_t">[13]</a></sup> (метод DeepExplainer)
 


<figure>

<img src="shap.jpg" alt="Активные зоны, влияющие на определение пола на мужском и женском лицах" width="400" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Активные зоны, влияющие на определение пола на мужском и женском лицах
    
    
    
  </p> 
</figcaption>

</figure>
 Видно, что модель прежде всего обращает внимание на зоны, где возможна растительность
 на лице: область над губой, щёки. Для щек также вероятно важна структура кожи,
 более грубая у мужчин. Для женщин важны разрез глаз (и по видимому наличие там косметики)
 и форма подбородка. Для мужчин - форма и &ldquo;кустистость&rdquo; бровей.</p>

<p>Из собственного практического опыта работы с этой моделью &ndash; она смотрит примерно на те же
 признаки, что и человек, никакого сверхзнания у неё нет. Если показать
 модели фото мальчика, хорошо загримированного под девочку, модель выдаст
 ответ &ldquo;девочка&rdquo;, и наоборот. Трансгендеры и лица, не до конца определившиеся
с выбором визуального пола, вызовут у модели затруднения при определении биологическго пола,
такие же, как и у людей.</p>

<h2 id="эволюция-в-результате-обучения">Эволюция в результате обучения</h2>

<p>Ещё один вопрос, на который было интересно ответить &ndash; насколько
модель далеко ушла в своей эволюции от изначальной модели, обученной
на изображениях ImageNet? Поскольку выразить &ldquo;далеко&rdquo; или &ldquo;не очень&rdquo;
в виде числовой оценки затруднительно, лучше получить ответ в виде
визуализации. Я использовал визуализацию каналов ResNet с помощью
библиотеки <a href="https://github.com/tensorflow/lucid" target="_blank">Lucid</a>.
 Суть этой визуализации в том, что с помощью оптимизации подбирается такое входное изображение,
которое максимизирует ответ от канала. Содержимое этого изображения
будет указывать, на какие паттерны во входном изображении реагирует данный канал.</p>

<p>Если сравнить визуализации одних и тех же каналов в исходной сети
и в сети после обучения на лицах, будет видно, как поменялось &ldquo;восприятие&rdquo; сетью изображений.
Будем визуализировать избранные каналы в блоках ResNet от первых блоков,
самых примитивных, обрабатывающих контуры и границы
до последних, обрабатывающих целые визуальные объекты.
Верхний ряд каждой визуализции это каналы модели, обученной распознаванию пола и возраста,
нижний ряд - те же самые каналы исходной модели, обученной на ImageNet.</p>

<p>


<figure>

<img src="net_block1_6.jpg" alt="Визуализация паттернов в первом блоке ResNet, 6-й канал" width="654" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов в первом блоке ResNet, 6-й канал
    
    
    
  </p> 
</figcaption>

</figure>



<figure>

<img src="net_block1_7.jpg" alt="Визуализация паттернов в первом блоке ResNet, 7-й канал" width="654" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов в первом блоке ResNet, 7-й канал
    
    
    
  </p> 
</figcaption>

</figure>
В первом блоке обрабатываются самые простые паттерны. Видно что паттерны
для исходной и нашей модели не сильно отличаются. Тем не менее уже заметно,
что паттерны для ImageNet (нижние ряды) имеют более сложную структуру.</p>

<p>


<figure>

<img src="net_block2_5.jpg" alt="Визуализация паттернов во втором блоке ResNet, 5-й канал" width="654" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов во втором блоке ResNet, 5-й канал
    
    
    
  </p> 
</figcaption>

</figure>
Во втором блоке паттерны усложнились, но всё еще похожи друг на друга. Заметно,
что в паттернах ImageNet больше цветовое разнообразие, а паттерны нашей
модели окрашены в цвета, близкие к телесным.</p>

<p>


<figure>

<img src="net_block3_4.jpg" alt="Визуализация паттернов во третьем блоке ResNet, 4-й канал" width="850" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов во третьем блоке ResNet, 4-й канал
    
    
    
  </p> 
</figcaption>

</figure>
 


<figure>

<img src="net_block3_8.jpg" alt="Визуализация паттернов во третьем блоке ResNet, 8-й канал" width="850" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов во третьем блоке ResNet, 8-й канал
    
    
    
  </p> 
</figcaption>

</figure>
В третьем блоке паттерны продолжают усложняться, и схожесть между ними остаётся только на самом общем уровне.
Паттерны ImageNet имеют гораздо более проработанную структуру, которая
начинает соответствовать объектам из реального мира.</p>

<p>


<figure>

<img src="net_block4_4.jpg" alt="Визуализация паттернов в четвёртом блоке ResNet, 4-й канал" width="394" />



<figcaption data-pre="Рис. " data-post=":" >
  
  <p>
    Визуализация паттернов в четвёртом блоке ResNet, 4-й канал
    
    
    
  </p> 
</figcaption>

</figure>
 В последнем блоке паттерны окончательно перестали был похожими друг на друга.
 В паттернах нашей модели видна структура, соответствующая человеческим губам.
 в паттерах Imagenet &ndash; что то растительное.</p>

<p>Видно, что эволюция в верхних слоях зашла довольно далеко,
при этом паттерны нашей модели в целом проще исходных, т.е. произошла некоторая
деградация. Возможно, что ResNet-50 избыточен для данной задачи,
и можно было использовать более простую сеть.</p>

<h1 id="интерактивное-демо">Интерактивное демо</h1>

<p>В статье не публикуются образцы предсказаний модели, т.к. любые предсказания
можно сгенерировать самостоятельно, с помощью интерактивного
демо, находящегося по адресу <a href="https://ag-demo.suilin.ru/" target="_blank">https://ag-demo.suilin.ru/</a>.</p>

<p>В демо можно загружать любые фото, где есть лицо одного человека. Поддерживается
 работа как с компьютеров, так и со смартфонов (можно определять пол и возраст для селфи).</p>

<h1 id="резюме">Резюме</h1>

<p>Задача распознавания пола и возраста в промышленных масштабах оказалась
вполне решаемой. При этом модель угадывает возраст примерно на уровне
человека, часто даже точнее.</p>

<h2 id="возможные-улучшения">Возможные улучшения</h2>

<ol>
<li>Сделать более выравненную по возрастам обучающую выборку, чтобы не страдало качество вне основного диапазона возрастов</li>

<li><p>Попробовать ускорить обучение с помощью <em>Super-Convergence</em> <sup><a href="#smith2017superconvergence" id="smith2017superconvergence_t">[14]</a></sup>.
Если ускорить обучение (рекорд скорости обучения ResNet-50 &ndash; <a href="https://www.fast.ai/2018/08/10/fastai-diu-imagenet" target="_blank">18 минут</a>), появится
возможность обучаться на большем количестве данных. В результате можно увеличить размер обучающей выборки и активнее использовать аугментацию,
например применить алгоритмы AutoAugment<sup><a href="#cubuk2018autoaugment" id="cubuk2018autoaugment_t">[15]</a></sup>, Mixup<sup><a href="#zhang2017mixup" id="zhang2017mixup_t">[16]</a></sup>.</p></li>

<li><p>Попробовать более современные, чем ResNet, архитектуры: AmoebaNet<sup><a href="#real2018regularized" id="real2018regularized_t">[17]</a></sup>,
DenseNet<sup><a href="#huang2017densely" id="huang2017densely_t">[18]</a></sup>, WideResNet<sup><a href="#zagoruyko2016wide" id="zagoruyko2016wide_t">[19]</a></sup>.</p></li>

<li><p>Работа с лицами это достаточно узкая и специфичная задача, возможно оптимальнее
будет не использовать архитектуру общего назначения, а создать custom архитектуру,
нацеленную именно на обработку лиц. Можно использовать методики
автоматического создания и оптимизации архитектур:
ENAS<sup><a href="#pham2018efficient" id="pham2018efficient_t">[20]</a></sup>, DARTS<sup><a href="#liu2018darts" id="liu2018darts_t">[21]</a></sup>,
Auto-Keras<sup><a href="#jin2018efficient" id="jin2018efficient_t">[22]</a></sup>,  AdaNet<sup><a href="#cortes2017adanet" id="cortes2017adanet_t">[23]</a></sup>.</p></li>
</ol>

<hr style="margin-top: 3em;">
<h2>Источники</h2>
<ol>
  

  
  <li id="zhang2016">

    <span class="bib-title">Joint face detection and alignment using multitask cascaded convolutional networks</span>

    [<a href="http://dx.doi.org/10.1109/LSP.2016.2603342">link</a>] <a href="#zhang2016_t">^</a>
    <div class="bib-authors">
     
       K. Zhang,
     
       Z. Zhang,
     
       Z. Li,
     
       Y. Qiao,
     
    2016.
    IEEE Signal Processing Letters.
    23.10 (pp. 1499-1503) DOI:<a href="http://doi.org/10.1109/lsp.2016.2603342">10.1109/lsp.2016.2603342</a>
    </div>

  </li>
  
  

  
  <li id="bulat2017far">

    <span class="bib-title">How far are we from solving the 2D &amp; 3D face alignment problem? (And a dataset of 230,000 3D facial landmarks)</span>

     <a href="#bulat2017far_t">^</a>
    <div class="bib-authors">
     
       A. Bulat,
     
       G. Tzimiropoulos,
     
    2017.
    International conference on computer vision.
    
    </div>

  </li>
  
  

  
  <li id="ludwiczuk2017">

    <span class="bib-title">Demystifying face recognition</span>

    [<a href="http://blcv.pl/static/tag/face-recogition">link</a>] <a href="#ludwiczuk2017_t">^</a>
    <div class="bib-authors">
     
       B. Ludwiczuk,
     
    2017.
    
    
    </div>

  </li>
  
  

  
  <li id="umeyama1991">

    <span class="bib-title">Least-squares estimation of transformation parameters between two point patterns</span>

    [<a href="http://edge.cs.drexel.edu/Dmitriy/Matching_and_Metrics/Umeyama/um.pdf">PDF</a>] <a href="#umeyama1991_t">^</a>
    <div class="bib-authors">
     
       S. Umeyama,
     
    1991.
    IEEE Transactions on Pattern Analysis and Machine Intelligence.
    13.4 (pp. 376-380) DOI:<a href="http://doi.org/10.1109/34.88573">10.1109/34.88573</a>
    </div>

  </li>
  
  

  
  <li id="he2016">

    <span class="bib-title">Deep residual learning for image recognition</span>

    [<a href="http://dx.doi.org/10.1109/CVPR.2016.90">link</a>] <a href="#he2016_t">^</a>
    <div class="bib-authors">
     
       K. He,
     
       X. Zhang,
     
       S. Ren,
     
       J. Sun,
     
    2016.
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
     DOI:<a href="http://doi.org/10.1109/cvpr.2016.90">10.1109/cvpr.2016.90</a>
    </div>

  </li>
  
  

  
  <li id="zoph2017learning">

    <span class="bib-title">Learning transferable architectures for scalable image recognition</span>

    [<a href="http://arxiv.org/abs/1707.07012">arXiv</a>] <a href="#zoph2017learning_t">^</a>
    <div class="bib-authors">
     
       B. Zoph,
     
       V. Vasudevan,
     
       J. Shlens,
     
       Q. Le,
     
    2017.
    
    
    </div>

  </li>
  
  

  
  <li id="howard2017mobilenets">

    <span class="bib-title">MobileNets: Efficient convolutional neural networks for mobile vision applications</span>

    [<a href="http://arxiv.org/abs/1704.04861">arXiv</a>] <a href="#howard2017mobilenets_t">^</a>
    <div class="bib-authors">
     
       A. Howard,
     
       M. Zhu,
     
       B. Chen,
     
       D. Kalenichenko,
     
       W. Wang,
     
       T. Weyand,
     
       M. Andreetto,
     
       H. Adam,
     
    2017.
    
    
    </div>

  </li>
  
  

  
  <li id="rothe2016dex">

    <span class="bib-title">Deep expectation of real and apparent age from a single image without facial landmarks</span>

    [<a href="https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_01229.pdf">PDF</a>] <a href="#rothe2016dex_t">^</a>
    <div class="bib-authors">
     
       R. Rothe,
     
       R. Timofte,
     
       L. Gool,
     
    2016.
    International Journal of Computer Vision (IJCV).
    
    </div>

  </li>
  
  

  
  <li id="eidinger2014age">

    <span class="bib-title">Age and gender estimation of unfiltered faces</span>

    [<a href="https://www.openu.ac.il/home/hassner/Adience/EidingerEnbarHassner_tifs.pdf">PDF</a>] <a href="#eidinger2014age_t">^</a>
    <div class="bib-authors">
     
       E. Eidinger,
     
       R. Enbar,
     
       T. Hassner,
     
    2014.
    IEEE Transactions on Information Forensics and Security.
    9. (pp. 2170-2179) DOI:<a href="http://doi.org/10.1109/TIFS.2014.2359646">10.1109/TIFS.2014.2359646</a>
    </div>

  </li>
  
  

  
  <li id="zhifei2017cvpr">

    <span class="bib-title">Age progression/regression by conditional adversarial autoencoder</span>

    [<a href="https://arxiv.org/abs/1702.08423">arXiv</a>] <a href="#zhifei2017cvpr_t">^</a>
    <div class="bib-authors">
     
       S. Zhang,
     
       H. Qi,
     
    2017.
    IEEE conference on computer vision and pattern recognition (cvpr).
    
    </div>

  </li>
  
  

  
  <li id="loshchilov2016sgdr">

    <span class="bib-title">SGDR: Stochastic gradient descent with warm restarts</span>

    [<a href="http://arxiv.org/abs/1608.03983">arXiv</a>] <a href="#loshchilov2016sgdr_t">^</a>
    <div class="bib-authors">
     
       I. Loshchilov,
     
       F. Hutter,
     
    2016.
    
    
    </div>

  </li>
  
  

  
  <li id="smith2017cyclical">

    <span class="bib-title">Cyclical learning rates for training neural networks</span>

    [<a href="http://dx.doi.org/10.1109/WACV.2017.58">link</a>] <a href="#smith2017cyclical_t">^</a>
    <div class="bib-authors">
     
       L. Smith,
     
    2017.
    IEEE Winter Conference on Applications of Computer Vision (WACV).
     DOI:<a href="http://doi.org/10.1109/wacv.2017.58">10.1109/wacv.2017.58</a>
    </div>

  </li>
  
  

  
  <li id="lundberg2017shap">

    <span class="bib-title">A unified approach to interpreting model predictions</span>

    [<a href="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf">PDF</a>] <a href="#lundberg2017shap_t">^</a>
    <div class="bib-authors">
     
       S. Lundberg,
     
       S. Lee,
     
    2017.
    Advances in neural information processing systems 30.
     (pp. 4765-4774)
    </div>

  </li>
  
  

  
  <li id="smith2017superconvergence">

    <span class="bib-title">Super-convergence: Very fast training of neural networks using large learning rates</span>

    [<a href="http://arxiv.org/abs/1708.07120">arXiv</a>] <a href="#smith2017superconvergence_t">^</a>
    <div class="bib-authors">
     
       L. Smith,
     
       N. Topin,
     
    2017.
    
    
    </div>

  </li>
  
  

  
  <li id="cubuk2018autoaugment">

    <span class="bib-title">AutoAugment: Learning augmentation policies from data</span>

    [<a href="http://arxiv.org/abs/1805.09501">arXiv</a>] <a href="#cubuk2018autoaugment_t">^</a>
    <div class="bib-authors">
     
       E. Cubuk,
     
       B. Zoph,
     
       D. Mane,
     
       V. Vasudevan,
     
       Q. Le,
     
    2018.
    
    
    </div>

  </li>
  
  

  
  <li id="zhang2017mixup">

    <span class="bib-title">Mixup: Beyond empirical risk minimization</span>

    [<a href="http://arxiv.org/abs/1710.09412">arXiv</a>] <a href="#zhang2017mixup_t">^</a>
    <div class="bib-authors">
     
       H. Zhang,
     
       M. Cisse,
     
       Y. Dauphin,
     
       D. Lopez-Paz,
     
    2017.
    
    
    </div>

  </li>
  
  

  
  <li id="real2018regularized">

    <span class="bib-title">Regularized evolution for image classifier architecture search</span>

    [<a href="http://arxiv.org/abs/1802.01548">arXiv</a>] <a href="#real2018regularized_t">^</a>
    <div class="bib-authors">
     
       E. Real,
     
       A. Aggarwal,
     
       Y. Huang,
     
       Q. Le,
     
    2018.
    
    
    </div>

  </li>
  
  

  
  <li id="huang2017densely">

    <span class="bib-title">Densely connected convolutional networks</span>

    [<a href="http://arxiv.org/abs/1608.06993">arXiv</a>] <a href="#huang2017densely_t">^</a>
    <div class="bib-authors">
     
       G. Huang,
     
       Z. Liu,
     
       L. Maaten,
     
       K. Weinberger,
     
    2017.
    CVPR.
     (pp. 2261-2269)
    </div>

  </li>
  
  

  
  <li id="zagoruyko2016wide">

    <span class="bib-title">Wide residual networks</span>

    [<a href="http://arxiv.org/abs/1605.07146">arXiv</a>] <a href="#zagoruyko2016wide_t">^</a>
    <div class="bib-authors">
     
       S. Zagoruyko,
     
       N. Komodakis,
     
    2016.
    
    
    </div>

  </li>
  
  

  
  <li id="pham2018efficient">

    <span class="bib-title">Efficient neural architecture search via parameter sharing</span>

    [<a href="http://arxiv.org/abs/1802.03268">arXiv</a>] <a href="#pham2018efficient_t">^</a>
    <div class="bib-authors">
     
       H. Pham,
     
       M. Guan,
     
       B. Zoph,
     
       Q. Le,
     
       J. Dean,
     
    2018.
    
    abs/1802.03268.
    </div>

  </li>
  
  

  
  <li id="liu2018darts">

    <span class="bib-title">Darts: Differentiable architecture search</span>

    [<a href="http://arxiv.org/abs/1806.09055">arXiv</a>] <a href="#liu2018darts_t">^</a>
    <div class="bib-authors">
     
       H. Liu,
     
       K. Simonyan,
     
       Y. Yang,
     
    2018.
    
    
    </div>

  </li>
  
  

  
  <li id="jin2018efficient">

    <span class="bib-title">Auto-keras: Efficient neural architecture search with network morphism</span>

    [<a href="http://arxiv.org/abs/1806.10282">arXiv</a>] <a href="#jin2018efficient_t">^</a>
    <div class="bib-authors">
     
       H. Jin,
     
       Q. Song,
     
       X. Hu,
     
    2018.
    
    
    </div>

  </li>
  
  

  
  <li id="cortes2017adanet">

    <span class="bib-title">AdaNet: Adaptive structural learning of artificial neural networks</span>

    [<a href="http://arxiv.org/abs/1607.01097">arXiv</a>] <a href="#cortes2017adanet_t">^</a>
    <div class="bib-authors">
     
       C. Cortes,
     
       X. Gonzalvo,
     
       V. Kuznetsov,
     
       M. Mohri,
     
       S. Yang,
     
    2017.
    
    
    </div>

  </li>
  
  
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/instagram/">Instagram</a>
  
  <a class="badge badge-light" href="/tags/deep.social/">Deep.Social</a>
  
</div>



    






<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  <img class="portrait mr-3" src="/img/me_sq.jpg" itemprop="image" alt="Avatar">
  
  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/">Артур Суилин</a></h5>
    <h6 class="card-subtitle">CEO</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="//www.linkedin.com/in/suilin-arthur-304b8219/" target="_blank" rel="noopener">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
      
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="//github.com/Arturus" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="//www.kaggle.com/asuilin" target="_blank" rel="noopener">
          <i class="fab fa-kaggle"></i>
        </a>
      </li>
      
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="//instagram.com/asuilin" target="_blank" rel="noopener">
          <i class="fab fa-instagram"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>




    
    
    
    
    
    
    
    
    

    

  </div>
</article>


<div class="article-container article-widget">
  <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="https://suilin.ru/project/post_time/" rel="next">Предсказание времени поста в Instagram</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="https://suilin.ru/project/topic_tensor/" rel="prev">TopicTensor: тематики в Instagram</a>
  </div>
  
</div>

</div>


<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; Артур Суилин &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    
    <script src="/js/mathjax-config.500a6cbb2c0f345fcecc21b3116d6637aa78f1f11db8081ea581abe05390c2e8f3bbffe61be3cf0217baf785c40efceabe51050a4f007e69af94efd3643260e8.js" integrity="sha512-UApsuywPNF/OzCGzEW1mN6p48fEduAgepYGr4FOQwujzu//mG&#43;PPAhe694XEDvzqvlEFCk8AfmmvlO/TZDJg6A=="></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    
    <script>
      
      window.lazyLoadOptions = {elements_selector: ".lazy"};
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/intersection-observer@0.5.1/intersection-observer.js"></script>
    <script async src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@11.0.5/dist/lazyload.min.js"></script>

  </body>
</html>

