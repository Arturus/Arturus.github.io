[{"authors":null,"categories":["Programming"],"content":" ClickHouse \u0026ndash; самая быстрая в мире аналитическая СУБД. Для тех, кто с ним ещё не знаком, очень рекомендую попробовать, пересаживаться обратно на MySQL или Postgress потом не захочется.\nОбычно данные хранятся в ClickHouse в сыром, неагрегированном виде, и агрегируются на лету при выполнении SQL запросов. Но при решении data science задач часто возникает необходимость выгрузки именно сырых данных, для дальнейшей их обработки в памяти (например, для обучения модели по этим данным). Если выгружать данные в текстовый файл с помощью родного клиента ClickHouse, всё происходит достаточно шустро \u0026ndash; \u0026ldquo;ClickHouse не тормозит\u0026rdquo;™. Но если пользоваться драйвером для Python, то процесс выгрузки затягивается надолго. Почему?\nДело не в том, что драйвер плохо написан \u0026ndash; драйвер как раз отличный. Проблема лежит глубже. ClickHouse отдаёт данные в виде бинарного потока, каждый элемент которого соответствует машинному представлению числа на x86 процессоре. Если работать с этими данными на низкоуровневом языке, таком как С++ (как в родном клиенте), проблем с быстродействием не будет. Если колонка, например, имеет тип Int32, то на клиента приедет фактически готовый к использованию массив чисел с типом int32_t.\nНо Python представляет все числа, как объекты. Это означает, что драйвер проходит по загруженным данным, преобразует каждое число в объект, и потом уже из этих объектов собирает питоновский массив (состоящий из указателей). Такая операция называется boxing, и при больших объемах данных она отнимает значительное время. Собственно, в ходе загрузки данных через Python-драйвер основное занятие CPU это переупаковка чисел из машинного представления в объекты.\nВ то же время в data science принято работать c numpy массивами (pandas тоже работает через numpy), которые содержат числа в машинном представлении, как в С. То есть, сначала мы долго упаковывали числа в объекты, а потом, при конвертировании из Python массива в numpy массив будем распаковывать объекты обратно в числа (unboxing). Очевидно, что промежуточное объектное представление здесь только мешает, и если бы драйвер умел выгружать данные сразу в numpy массивы, процесс пошёл бы намного бодрее. Но драйвер этого не умеет, поэтому я его немного доработал, чтобы такая возможность появилась.\nИнсталляция  Если уже установлен пакет clickhouse-driver, удалить его: pip uninstall clickhouse-driver Инсталлировать из github версию с ускоренным чтением: pip install git+https://github.com/Arturus/clickhouse-driver.git  Использование При создании объекта Client надо включить новую опцию numpy_columns=True, а при выполнении запросов включать опцию columnar=True:\nclient = Client('localhost', database='db', settings=dict(numpy_columns=True)) data = client.execute(query, columnar=True)  В data будет содержаться набор колонок. Колонки, представляющие собой числа или timestamp, будут numpy-массивами, остальные колонки (например, строки) будут стандартными Python массивами. В numpy формат конвертируются следующие типы Clickhouse: Int8/16/32/64, UInt8/16/32/64, DateTime.\nПолученные данные часто преобразуются в pandas DataFrame с именами колонок, соответствующими именам колонок в БД. Чтобы не делать это каждый раз вручную, в класс Client добавлен метод query_dataframe():\ndf = client.query_dataframe('SELECT a,b FROM table')  Результатом будет DataFrame с двумя колонками, a и b. th {text-align: center;}\nBenchmarks Замерялась скорость выполнения запроса SELECT x1,x2,...,xn FROM table на таблице со 100 млн. записей (реальные данные из Logs API Яндекс.Метрики), engine=MergeTree. Запросы выполнялись на локальном ClickHouse c дефолтными настройками драйвера.\n   Запрос Время, numpy Время, standard Ускорение Memory, numpy Memory, standard     4 колонки Int8 0.34 s 5.8 s ×17 0.82 Gb 3.3 Gb   2 колонки Int64 1.38 s 12 s ×8.7 2.61 Gb 9.7 Gb   1 колонка DateTime 12.1 s 7.1 m ×35 1.16 Gb 4.8 Gb    Использование numpy ускоряет чтение на порядок. Особенно заметно ускорение на типе DateTime, потому что работа c временем на уровне Питоновских datetime-объектов происходит очень медленно. Фактически, без использования numpy время выполнения запроса, включающего колонку со временем, выходит за рамки разумного.\nВ последних двух колонках \u0026ndash; объём памяти, занимаемый процессом после выполнения запроса. Видно, что использование numpy не только ускоряет загрузку данных, но и уменьшает объём требуемой памяти примерно в 4 раза.\nОграничения  Поддерживается только чтение в numpy массивы. Запись возможна только в режиме numpy_columns=False. numpy массивы не используются при чтении nullable колонок и колонок-массивов. Впрочем, код чтения массивов тоже немного оптимизирован и теперь работает быстрее, чем в обычном драйвере. Также numpy не используется при чтении enums, decimal и прочих продвинутых типов (поддержка может быть добавлена в будущем).  Ограничения на чтение никак не мешают функционированию драйвера, просто для некоторых типов данных чтение ускоряется, а для некоторых \u0026ndash; работает, как обычно.\n","date":1556568000,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1556568000,"objectID":"cf0cdc5c60f7af80adc6dc28d6ffcb8a","permalink":"https://suilin.ru/post/clickhouse_driver/","publishdate":"2019-04-30T00:00:00+04:00","relpermalink":"/post/clickhouse_driver/","section":"post","summary":"ClickHouse \u0026ndash; самая быстрая в мире аналитическая СУБД. Для тех, кто с ним ещё не знаком, очень рекомендую попробовать, пересаживаться обратно на MySQL или Postgress потом не захочется.\nОбычно данные хранятся в ClickHouse в сыром, неагрегированном виде, и агрегируются на лету при выполнении SQL запросов. Но при решении data science задач часто возникает необходимость выгрузки именно сырых данных, для дальнейшей их обработки в памяти (например, для обучения модели по этим данным).","tags":["ClickHouse","Python","Performance"],"title":"Ускоряем драйвер ClickHouse","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" Некоторые сайты, например новостные, не имеют явных целевых действий, таких как заказ или подписка. Для них важно не выполнение посетителем каких либо действий, а само присутствие посетителя на сайте, желательно регулярное. Но и для e-commerce сайтов тоже очень важны возвраты посетителей, так как вернувшийся посетитель обходится намного дешевле, чем новый. Каждый возврат это шанс что нибудь продать, при этом чем меньше интервал между возвратами, тем больше шансов на осуществление продажи.\nЗадача определения того, продолжит ли посетитель/клиент пользоваться сервисом, или ушёл и никогда больше не вернётся, называется churn prediction (прогнозирование оттока). Лобовой подход к её решению \u0026ndash; взять определенный промежуток времени, например месяц или полгода, и спрогнозировать вероятность возврата клиента в этом промежутке. Обычно data scientist-ы так и поступают. Но такой подход не очень хорош с точки зрения качества прогноза.\nВо-первых, хорошо бы использовать прогнозное значение, как метрику вовлечённости клиента (т.е. насколько он \u0026ldquo;churned\u0026rdquo; или \u0026ldquo;not churned\u0026rdquo;). Клиент, который вернётся на сайт завтра, очевидно, более вовлечён, чем клиент, который вернётся через полгода. Но для модели, определяющей просто вероятность возврата в течение полугода, оба клиента будут совершенно равнозначны.\nВо-вторых, возникает вопрос выбора размера промежутка для прогноза. Каким он должен быть? Месяц? Квартал? Год? Интуитивно кажется, что чем больший период жизни клиента мы охватываем, тем лучше. Но если взять промежуток в год, то обучаться можно будет только на устаревших данных (старше года), так как данные за последний год будут использоваться только для формирования целевых значений. За год данные могут и \u0026ldquo;протухнуть\u0026rdquo;: изменится сам сайт, изменятся источники трафика и свойства аудитории. Чем свежее обучающие данные, тем качественнее будет прогноз.\n  Более правильным кажется предсказывать не вероятность возврата внутри фиксированного промежутка, а интервал времени до следующего посещения сайта, лежащий в диапазоне $[t_s, \\infty]$, где $t_s$ это таймаут, которым разделяются сессии (8 часов). Чем меньше этот интервал, тем более вовлечён клиент. Если предсказанный интервал близок к бесконечности, то клиент возможно не вернётся на сайт никогда, и можно считать, что он churned. Порог, после которого клиент становится churned, можно выбирать и изменять уже после обучения модели, это ещё один плюс.\nНо тогда возникает другая проблема: чему равен интервал для клиентов, которые ушли с сайта и пока не вернулись? Ведь верхняя его граница находится где то в будущем (возможно, бесконечно далёком). Точно известно только то, что этот интервал больше, чем now_time - last_event_time. Как обучать модель, если целевая переменная определена только нижней границей?\nТакое обучение возможно при использовании регрессионной модели, которая называется Cox Proportional Hazards. Чтобы понять, про какие \u0026ldquo;пропорциональные опасности\u0026rdquo; идет речь, надо немного познакомиться с анализом выживаемости (Survival analysis).\nSurvival analysis Задача прогноза времени до наступления события (в нашем случае - возврата на сайт) встречается в жизни довольно часто, но теория анализа выживаемости и соответствующие методики прогнозирования сначала были разработаны для медицинских исследований. Событием, наступление которого анализировалось, являлась смерть пациента, отсюда и название \u0026ldquo;анализ выживаемости\u0026rdquo;. Такой анализ часто применяется при сравнении групп пациентов, получивших разные способы лечения: более эффективен тот способ, где пациенты прожили дольше. В нашем случае логика инвертируется: хороший клиент это тот, кто вернулся на сайт (\u0026ldquo;умер\u0026rdquo; с точки зрения survival analysis) как можно раньше.\nЕстественно, врачи не хотели дожидаться естественной смерти всех пациентов, чтобы сделать выводы об эффективности лечения. Поэтому survival analysis умеет работать с цензурированными (censored) данными. Цензурирование возникает, когда точное время наступления события неизвестно (находится где то в будущем, пациент все еще жив), известно только, что событие не наступило до определенного момента времени (обычно до текущего момента).\n  Времена до возврата на сайт. Бордовым показаны посетители, у которых время возврата уже известно, голубым - которые еще не вернулись (вернутся в будущем), у них цензурированное время наступления события.   Базовое понятие анализа выживаемости это Survival function (функция выживания): $$ S(t) = \\Pr(T\u0026gt;t) $$ где $T$ \u0026ndash; время жизни члена популяции, $t$ \u0026ndash; произвольный интервал времени. Смысл функции \u0026ndash; вероятность того, что член популяции проживёт дольше, чем $t$. Функция является строго убывающей, в момент времени $t=0$ вероятность равна 100% (все только что родились), по мере возрастания $t$ вероятность убывает (постепенно умирают). Для пациентов вероятность станет практически нулевой после 100 лет, но в общем случае возможна \u0026ldquo;вечная жизнь\u0026rdquo;, когда часть популяции выживает в течение всего обозримого времени (в нашем случае \u0026ndash; клиент никогда не возвращается на сайт).\nДавайте визуализируем функцию выживания разных когорт посетителей на сайте shop.ml. Когорты сформируем из тех, у кого была только одна (первая) сессия, две и три сессии, и посмотрим на динамику возвратов:\n График функции выживаемости это по сути перевёрнутый график user retention. На старте есть 100% юзеров, постепенно часть из них возвращается, а часть теряется навсегда (графики заканчиваются выше нулевой точки). Посетители с единственной сессией возвращаются на сайт очень неохотно: за две недели возвращается только 10%, а в течение года количество вернувшихся не доходит даже до 20%. Посетители, у которых было уже две или три сессии намного более вовлечены: в течение недели возвращается 20% посетителей с двумя сессиями, а в течение двух месяцев \u0026ndash; половина посетителей, у которых было три сессии.\nHazard function Survival function показывает, какая доля популяции выживает (какая доля посетителей не возвращается) к моменту времени $t$. Комплементарная ей функция, lifetime distribution function, показывает, какая доля \u0026ldquo;умирает\u0026rdquo; к моменту $t$: $$F(t) = 1 - S(t)$$ Если взять производную от lifetime distribution function, получим плотность событий (количество смертей пациентов или возвратов посетителей) в единицу времени: $$f(t) = F\u0026rsquo;(t) = \\frac{d}{dt}F(t)$$ Плотность событий не очень удобна для использования, т.к. по мере вымирания популяции общее количество событий быстро уменьшается. Поэтому обычно используют другую функцию, hazard function (функция риска), в которой количество смертей в единицу времени $f(t)$ нормируется на долю популяции, выжившей к этому моменту времени: $$\\lambda(t) = \\frac{f(t)}{S(t)} = -\\frac{S\u0026rsquo;(t)}{S(t)}$$ Эта функция фактически показывает риск умереть в момент времени $t$ для тех, кто дожил до этого момента, отсюда название hazard function. Визуализируем функцию риска для наших когорт посетителей:\n Лёгкая волнистость, которая наблюдалась на графике функции выживания, превратилась в острые пики. Причина этих пиков \u0026ndash; суточная сезонность. Посетители чаще всего возвращаются на сайт в то же время дня, в которое они посещали его в предыдущий раз. Заметно, что интенсивность событий остается примерно постоянной до 30 дней, а затем идёт на спад, то есть через месяц посетители начинают забывать о сайте. Также обратим внимание: форма всех трёх графиков очень похожа, они отличаются в основном масштабом по оси $y$.\nИспользуется также cumulative hazard function (кумулятивная функция риска), она выражает суммарный риск, накопившийся к моменту времени $t$: $$\\Lambda(t)=\\int_0^t\\lambda(u) du$$ Cumulative hazard function и survival function связаны следующим выражением: $$S(t)=\\exp(-\\Lambda(t))$$\nCox proportional hazards Одна из часто используемых в survival analysis моделей это Proportional hazards. В этой модели делается допущение, что функции риска для каждого индивидуального члена популяции имеют примерно одинаковую форму, задаваемую через общую для всех baseline hazard функцию $\\lambda_0(t)$, и отличаются только на положительный коэффициент пропорциональности $k_i$, индивидуальный для каждого члена популяции: $$\\lambda_i(t) = \\lambda_0(t) k_i,\\;i \\in 1 \\mathrel{{.}\\,{.}} N $$ В классической модели, предложенной статистиком Дэвидом Коксом, коэффициент вычисляется с помощью линейной регрессии: $$k_i = \\exp(logits_i)$$ $$logits_i = \\mathbf{x_i}^\\top\\boldsymbol{\\beta}$$ где $\\mathbf{x_i}$ \u0026ndash; вектор признаков i-го члена популяции, $\\boldsymbol{\\beta}$ \u0026ndash; вектор коэффициентов линейной модели. Но ничего не мешает использовать для вычисления $logits$ любую другую модель машинного обучения, например нейросеть или деревья решений.\nИзящество этой модели в том, её можно обучать через максимизацию частичного правдоподобия, не задавая в явном виде baseline функцию $\\lambda_0(t)$, форма которой, вообще говоря, неизвестна до окончания обучения.\nДопущение, что функции риска всех членов популяции отличаются только на пропорциональный коэффициент, конечно, не всегда соответствует реальности. В порядке эксперимента, попробуем привести функции риска для наших когорт посетителей к одному масштабу, умножая каждый график на эмпирически подобранный коэффициент. В идеале графики должны совпасть:\nГрафики в значительной степени совпадают, но не на 100%. Сильнее всего различается поведение после 30 дней.\n  В более продвинутых моделях каждый член популяции может иметь индивидуальную функцию риска произвольной формы. Но с другой стороны, чтобы смоделировать индивидуальные функции, требуется очень много обучающих данных, а proportional hazards модели хорошо работают даже на небольших обучающих выборках.\nМоделирование Пациент может умереть только единожды, поэтому survival analysis работает исключительно с с терминальными событиями, т.е. событие должно быть последней точкой в жизни члена популяции. В отличие от пациента, посетитель сайта может возвращаться неограниченное количество раз, поэтому мы немного схитрим, и представим, что каждая сессия происходит от лица виртуального нового посетителя. В этом случае модель может переобучиться на посетителях, которые возвращались много раз, т.к. их признаки будут учтены многократно. Чтобы этого не случилось, дадим каждому виртуальному посетителю вес $w_i = s_i^{-1}$, обратно пропорциональный количеству его сессий $s_i$.\nВ качестве метрики качества будем использовать уже знакомый по предыдущим статьям Concordance Index \u0026ndash; обобщение метрики AUC для регрессионных задач. Важно, что Concordance Index корректно работает с цензурированными данными, ведь основной процент посетителей никогда не возвращается после первой сессии.\nМодель Proportional Hazards выдаёт значение коэффициента $k_i$. С помощью Breslow estimator можно найти базовую кумулятивную функцию $\\Lambda_0(t)$, и перейти от неё к функции выживания для i-го посетителя: $$S_i(t) = \\exp\\left(-\\int_0^t k_i d\\Lambda_0(u)\\right)$$ Таким образом, результатом является не точечная оценка вероятного интервала между сессиями (как было бы в традиционных моделях), а целая вероятностная функция, параметризованная коэффициентом $k_i$.\nДля скоринга посетителей по степени того, насколько они churned, абсолютные значения интервалов, и тем более вероятностные функции сложной формы не нужны. Достаточно значения коэффициента $k_i$: чем оно больше, тем более вовлечён (менее churned) посетитель. На практике удобнее использовать даже не сам коэффициент, а его логарифм (соответствующий $logits$ из модели), т.к. у логарифмического значения близкое к нормальному распределение, такие значения проще воспринимать.\nСлева распределение прогнозных значений коэффициента $k_i$, справа \u0026ndash; распределение значений его логарифма. Правое распределение ближе к нормальному, левое \u0026ndash; к экспоненциальному. Обратите внимание на логарифмическую шкалу частот (ось Y).\n  Shapely values рассчитываются тоже в этой логарифмической шкале. Одной единице шкалы соответствует двукратная разница в интенсивности событий \u0026ldquo;возврат на сайт\u0026rdquo;, т.е. за одинаковый промежуток времени на сайт вернётся в два больше раза посетителей из когорты, где $score=1$ по сравнению с когортой, где $score=0$.\nТакже обратим внимание, что распределение скоринговых значений зависит от того, считается оно по уникальным посетителям или по уникальным сессиями (после каждой сессии скоринговое значение обновляется): \nЕсли смотреть распределение по посетителям, кажется, что скоринговое значение $\u0026gt;2$ практически не встречается. Но в распределении по сессиям таких значений много, потому что активные посетители, у которых маленький промежуток между сессиями, генерируют в целом намного больше сессий, чем неактивные.\nРезультаты, shop.ml Для начала, визуализируем получившуюся baseline hazard функцию, относительно которой рассчитываются скоринговые коэффициенты: \nСюрпризов здесь нет, baseline повторяет форму функции риска, рассчитанную по исходным данным. Посмотрим, как будут отличаться функции выживания для посетителей с разными скоринговыми коэффициентами:\n Половина посетителей с коэффициентом +3 вернётся на сайт уже на следующий день, а в течение года вернётся почти 100%. В то же время среди посетителей с коэффициентом -2.4 (самое распространённое значение в нашей выборке) даже через год на сайт возвращается только ~13%.\nConcordance Index этой модели на тестовой выборке ~77%. С точки зрения классического machine learning это кажется довольно скромным результатом, но для survival analysis это хорошая точность. Ведь мы предсказываем по большому счёту случайную величину, зависящую от множества внешних факторов, которые невозможно учесть в модели.\nПосмотрим, какие признаки играют определяющую роль в том, насколько быстро посетитель вернётся на сайт: \nНа первом месте \u0026ndash; \u0026ldquo;возраст\u0026rdquo; user_age, т.е. насколько давно посетитель впервые появился на сайте.  Если присмотреться, становится понятным, что на самом деле роль здесь играет не столько возраст посетителя, сколько длительность его первой сессии (если была только одна сессия, то возраст совпадает с её длительностью). Точка \u0026ldquo;перелома\u0026rdquo; находится в районе 10 минут: если сессия продлилась дольше, вероятность возврата на сайт быстро возрастает.\nНа втором месте \u0026ndash; session_count, количество сессий:  \u0026ldquo;Возвращаемость\u0026rdquo; посетителя начинает расти с третьей сессии. Разница между теми, у которых было 1-2 сессии и теми у кого было много сессий, достигает 3 единицы, это 8-кратное увеличение \u0026ldquo;возвращаемости\u0026rdquo;.\nsession_interval \u0026ndash; средний промежуток времени между сессиями. Это примерно то, что мы пытаемся прогнозировать, поэтому неудивительно, что этот признак оказывает значительное влияние на результат:  Нулевое значение соответствует единственной сессии; минимальный интервал между сессиями 8 часов, поэтому на диаграмме такой разрыв между 0 и 8h. C увеличением исторического среднего интервала между сессиями растёт и прогнозируемый интервал (уменьшается score), что вполне логично.\npageview_count \u0026ndash; количество просмотренных страниц, также один из главных показателей вовлечённости посетителя.  Здесь любопытно взаимодействие с признаком session_count: если было много сессий, то положительная роль большого количества просмотренных страниц ослабляется. Если же просмотрено в менее 10 страниц, то большое количество сессий становится негативным фактором.\nРезультаты, другие сайты Результаты работы модели на других сайтах весьма похожи на результаты shop.ml, и отличаются только нюансами, связанными с разной структурой сайтов и разной структурой входящего трафика. Поэтому не буду приводить подробные результаты для каждого сайта, в качестве примера \u0026ndash; summary для courses.ml: \nТочность по Concordance Index для всех сайтов находится в районе 75%. Форма baseline hazard function тоже довольно схожа у всех сайтов.\nИндивидуальные прогнозы Основное применение этой модели \u0026ndash; прогнозирование вовлеченности отдельных посетителей, чтобы работать с ними на индивидуальном уровне. Поэтому здесь особенно важны scorecards, позволяющие \u0026ldquo;заглянуть в душу\u0026rdquo; любого посетителя и получить представление о его планах на будущее.  На диаграмме представлены образцы scorecards трёх посетителей:\n A \u0026ndash; посетитель, который по мнению модели точно вернётся на сайт в ближайшем будущем. Характерные особенности: много предыдущих сессий и просмотров страниц; относительно короткий (полтора дня) интервал между сессиями; появился на сайте два месяца назад С \u0026ndash; посетитель, который вряд ли вернётся на сайт. Особенности: единственная короткая сессия и единственный просмотр страницы, длившийся секунды. B \u0026ndash; посетитель, вероятность возврата которого близка к средней по сайту (а в среднем посетители возвращаются на сайт не очень охотно). Тоже единственная сессия, с единственным просмотром страницы, длившаяся 18 секунд. Но есть и положительные факторы: использование планшета, переход на сайт из поисковика (по видимому Google + AdWords), переход на \u0026ldquo;мейнстримовую\u0026rdquo; страницу ($\\sum$path2=other: 0).\n  Заключение Proportional hazards это не единственный возможный способ моделирования потенциального возврата посетителя на сайт. У этой модели есть свои недостатки, как математические (не всегда выполняющееся предположение о пропорциональности функций риска у разных членов популяции), так и технические: при обучении необходимо, чтобы весь dataset был в памяти, так как loss вычисляется по всему набору данных. По последней причине с этой моделью невозможно использовать минибатчи, и не получится обучиться на данных крупных сайтов, которые просто не влезают в память.\nНо для сайтов небольшого и среднего размера модель вполне хороша, её главный плюс это очень экономичное использование данных: во-первых, для обучения используются все данные, включая сессии, закончившиеся недавно. Традиционные модели игнорируют свежие (самые ценные) данные, используя их только для формирования целевой переменной. Во-вторых, с помощью baseline hazard автоматически моделируется динамика выживаемости любой произвольной сложности. Например, чтобы обучиться затухающей суточной сезонности, которую мы видели на графике функции риска, традиционным методикам машинного обучения потребовалось бы очень немаленькое количество данных.\n","date":1555272000,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1555272000,"objectID":"2d50cc892c77abbbc2b2ead63e87869b","permalink":"https://suilin.ru/post/user_churn/","publishdate":"2019-04-15T00:00:00+04:00","relpermalink":"/post/user_churn/","section":"post","summary":"Некоторые сайты, например новостные, не имеют явных целевых действий, таких как заказ или подписка. Для них важно не выполнение посетителем каких либо действий, а само присутствие посетителя на сайте, желательно регулярное. Но и для e-commerce сайтов тоже очень важны возвраты посетителей, так как вернувшийся посетитель обходится намного дешевле, чем новый. Каждый возврат это шанс что нибудь продать, при этом чем меньше интервал между возвратами, тем больше шансов на осуществление продажи.","tags":["Churn prediction","Survival analysis"],"title":"Конверсия и data science VI. Клиент скорее жив, чем мёртв?","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" В идеальном мире маркетолог или владелец бизнеса не только работает с \u0026ldquo;конверсиями\u0026rdquo;, но использует для оценки посетителей сайта Customer Lifetime Value (сокращённо CLV) \u0026ndash; грубо говоря, это сумма денег, которую принесёт посетитель за всё время, пока будет пользоваться сервисом. Оценка клиентов по CLV является предпочтительной и для мобильных приложений, и для игр, и для любых онлайн сервисов.\nНо в реальной жизни всё, конечно, не так, как в идеальном мире. Чтобы рассчитать CLV клиента, нужны исходные данные: средний чек, частота покупок, и время \u0026ldquo;удержания\u0026rdquo;, т.е. как долго клиент будет пользоваться сервисом.\n   Все эти параметры, во-первых, вообще неизвестны для нового посетителя, который только что пришёл на сайт. Во-вторых, параметры \u0026ldquo;средний чек\u0026rdquo; и \u0026ldquo;частота покупок\u0026rdquo; подразумевают достаточный объем накопленной статистики по каждому клиенту, как минимум 3-5 покупок. В реальной жизни такая статистика накапливается у очень небольшого процента посетителей. В третьих, время удержания конкретного клиента заранее неизвестно, а среднее время удержания, рассчитанное по сегменту посетителей, имеет огромную дисперсию: один посетитель перестал заходить на сайт сразу после первого визита (время удержания ноль), второй пользуется в течение двух лет. И каким, например, надо считать время удержания посетителя, который появился на сайте только вчера? (кстати, это интересная тема для следующей статьи).  Всё это делает расчёт CLV для индивидуального посетителя вообще невозможным, а расчёт для сегмента посетителей \u0026ndash; крайне ненадёжным, особенно для небольших сегментов, и новых посетителей.\nВ то же время было бы очень заманчивым использовать CLV, например, для оценки трафика, генерируемого рекламными кампаниями и даже отдельными рекламными объявлениями. Как это сделать?\n  Как заглянуть в будущее В предыдущих статьях мы прогнозировали конверсию, которая произойдёт или не произойдет после перехода на сайт. Но ничего не мешает расширить горизонт прогноза, и в буквальном смысле заглядывать в будущее \u0026ndash; предсказывать, сколько конверсий будет у посетителя в течение определённого интервала времени, например следующих двух месяцев. Это будет не совсем классический CLV, поскольку прогноз делается для конечного интервала, а не \u0026ldquo;на всю жизнь\u0026rdquo;, тем не менее такой прогноз вполне достаточен для решения многих маркетинговых задач. Более того, вместо конверсий можно предсказывать сумму заказов \u0026ndash; это уже почти CLV.\nВ отличие от классического расчёта CLV через предрассчитанные параметры клиентов (удержание, средний чек, частота), прогноз CLV моделью машинного обучения не требует никаких предварительных расчётов и работает на индивидуальном для каждого клиента уровне, что позволяет легко сравнивать клиентов друг с другом и группировать в сегменты по ожидаемой выручке.\nЧто прогнозируем Будем считать, что количество конверсий $k$ берётся из распределения Пуассона: $$\\Pr(k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}$$ тогда для каждого посетителя надо вычислить его индивидуальный коэффициент $\\lambda$, обозначающий ожидаемое количество конверсий за единицу времени (в нашем случае 60 дней). Эта задача известна, как пуассоновская регрессия. В классическом варианте она решается через линейную модель, но можно использовать любые продвинутые алгоритмы: деревья решений, нейросети и т.п.\nПостроение моделей и оценку качества будем делать на тех же сайтах, что и в предыдущих статьях.\nОценка качества Качество регрессии традиционно оценивается или через абсолютную ошибку MSE (среднеквадратичная ошибка) или MAE (средняя абсолютная ошибка), или через через относительную ошибку, например MAPE (Mean Absolute Percentage Error). К сожалению, все эти способы подразумевают, что целевая переменная имеет нормальное или близкое к нему распределение, и для нашей задачи не подходят. Поясню на примерах:\nЕсли использовать абсолютную ошибку, то разница между 10 конверсиями и 11 конверсиями будет такой же, как разница между нулём конверсий и одной конверсией. Но с точки зрения бизнеса, ноль конверсий и одна конверсия это большая разница, а 10 и 11 \u0026ndash; незначительная.\nЕсли использовать относительную ошибку, то в случае, когда предсказана одна конверсия, а в реальности было ноль конверсий, ошибка станет бесконечной \u0026ndash; так тоже не годится. Можно добавлять к истинному значению сглаживающую константу $\\epsilon$, чтобы избежать деления на 0, но такая ошибка станет просто абстрактным числом, не имеющим внятной интерпретации.\nБолее приемлемым вариантом оказалось использование метрики AUC, только для регрессионных задач она называется Concordance index (c-index, c-statistic), и интерпретируется не как площадь под кривой, а как вероятность того, что в парах \u0026ldquo;прогноз \u0026ndash; истинное значение\u0026rdquo;, относящимся к двум случайно выбранным примерам, прогнозы будут правильно отранжированы. Т.е. если истинное значение в первой паре больше значения во второй паре, то и прогноз из первой пары будет больше прогноза из второй пары.\nConcordance index не чувствителен к калибровке прогноза, это тоже важно (вопроса калибровки коснёмся немного позже).\nВходные данные Будем использовать такой же набор признаков, как и в предыдущей модели, за исключением признаков, относящихся к текущему переходу на сайт, т.к. в новой модели нет \u0026ldquo;текущего\u0026rdquo; перехода. Т.е. оставим только признаки, относящиеся к истории переходов, к истории сессий, и к посетителю в целом (браузер, гео, устройство и т.п.).\nКроме этого, нужен ещё один признак, играющий в модели важную роль: время с момента последней активности посетителя.\nВ предыдущих моделях за текущий момент времени принимался момент перехода на сайт. Это было удобным для изучения факторов, влияющих на вероятность конверсии, но прогноз, который выдавала модель, сам по себе не имел практической ценности. В самом деле, какой смысл прогнозировать результат сессии, если можно просто немного подождать, сессия закончится сама собой, и результат будет виден и так?\nВ новой модели мы прогнозируем количество конверсий для произвольной выборки посетителей, которые были на сайте когда то раньше. Очевидно, что ожидаемое количество конверсий для посетителя, который последний раз зашёл на сайт год назад, и c тех пор не появлялся, будет близко к нулю. В то же время посетитель, который заходил на сайт вчера, имеет гораздо больше шансов на появление новых конверсий, чем предыдущий. Поэтому время с момента последней активности time_since_last обязательно должно быть включено в модель.\nНо в истории действий посетителей, из которой формируются данные для обучения, нет понятия \u0026ldquo;текущее время\u0026rdquo;. В принципе любой момент после первого появления посетителя на сайте может быть выбран, как текущий. Тогда то, что было то этого, принимается за историю, а на основе того, что было после, рассчитывается целевая переменная, т.е. количество конверсий или сумма заказов. $t_0$ \u0026ndash; время первого появления посетителя на сайте; $t_{end}$ \u0026ndash; максимальное время в данных; $t_1,\\dots,t_9$ \u0026ndash; случайные моменты времени, используемые для обучения.\n \nИтак, для каждого посетителя мы будем случайным образом выбирать \u0026ldquo;текущий\u0026rdquo; момент времени, причём для одного и того же посетителя может быть выбрано несколько разных моментов, относящихся к разным периодам его жизни. Заодно мы таким образом проводим data augmentation: из одного физического посетителя генерируем несколько \u0026ldquo;виртуальных\u0026rdquo;. От каждого текущего момента отсчитывается 60 дней в будущее и берётся суммарное количество конверсий за этот период в качестве целевой переменной.\nУ посетителей, которые очень давно не были на сайте, количество ожидаемых будущих конверсий стремится к нулю (они скорее всего уже никогда не вернутся). Поэтому имеет смысл ограничить максимальный период \u0026ldquo;бездействия\u0026rdquo; сроком, например, в 180 дней. Т.е. будем включать в обучающую и тестовую выборки только таких посетителей, у которых последнее событие было не позже 180 дней от текущего момента времени. У остальных посетителей ожидаемое количество конверсий можно считать нулевым.\nРезультаты, shop.ml На тестовой выборке получен Concordance Index ~94%, это очень хороший показатель. В качестве тестовой выборки использовались данные за последние два месяца, которые модель не видела при обучении и настройке (из двенадцати имеющихся месяцев) \u0026ndash; всё, как в реальной жизни.\nЧтобы эксперимент был честным, из данных были удалены сессии посетителей, которые посещали внутренние домены, недоступные из внешнего мира (т.е. сессии персонала магазина и разработчиков). Если не удалять эти данные, то модель легко обучается распознавать такие нетипичные сессии, и Concordance Index вырастает до 95-96%.\nПосмотрим на влияние признаков (методика интерпретации диаграмм влияния описана в предыдущей статье):\nПризнаки с наибольшим абсолютным влиянием, сайт shop.ml\n  Решающую роль играет признак time_since_last, не зря мы включили его! Также выраженное положительное влияние у pageview_count и $\\sum$order_total (суммарная стоимость всех заказов посетителя). Физический смысл SHAP values: изменение SHAP value на одну единицу изменяет ожидаемое количество конверсий в два раза. Т.е. например value=2 даёт 4x прирост количества конверсий относительно среднего по сайту (в среднем на одного посетителя shop.ml приходится 0.00086 будущих конверсий)\nПосмотрим на детальные диаграммы: С течением времени SHAP value изменяется от +2.5 до -2.5, т.е. ожидаемое количество конверсий падает в ~32 раза за 180 дней.\n \nКоличество будущих конверсий начинает расти после просмотра 4-x страниц. Просмотр более 30 страниц вызывает рост кол-ва конверсий примерно в 4 раза.\n  Клиенты, которые заказывали много, продолжат делать заказы \u0026ndash; видна выраженная зависимость ожидаемого кол-ва конверсий от суммы заказов.\n  В этой модели нам интересны не столько зависимости от признаков, сколько результаты прогноза. Чтобы лучше понять, что предсказывает модель, разобьём тестовую выборку на несколько групп, в соответствии со значением целевой переменной:\n 0 конверсий 1 конверсия 2 конверсии 3 конверсии от 4 до 5 конверсий более 5 конверсий  и будем смотреть, насколько точным получается прогноз для каждой группы.\nКоличество примеров в каждой из групп. Группа с нулевой конверсией содержит более чем в 1000 раз больше примеров, чем остальные группы (обратите внимание на логарифмическую шкалу).\n  Посчитаем среднее прогнозное значение в каждой группе. В идеале средний прогноз должен совпасть со средним истинным значением, например для группы \u0026ldquo;1 конверсия\u0026rdquo; средний прогноз должен быть равен единице.\nСравнение прогнозных и истинных средних значений в каждой группе.\n  По графику видно, что чуда не произошло: прогноз и истина не совпадают. Форма кривой прогнозных значений в принципе близка к форме кривой истинных значений, и также монотонно растёт. Т.е. такие прогнозные значения вполне можно использовать, как абстрактную оценку потенциала посетителя, и для сравнения посетителей друг с другом. Но для каких либо абсолютных оценок (например, сколько конверсий ожидается в конкретном сегменте посетителей?) такой прогноз не годится. Впрочем, ситуация легко поправима.\nКалибровка прогноза Наша модель предполагает, что значения целевой переменной распределены более-менее в соответствии с распределением Пуассона. Посмотрим, сколько значений в каждой группе было бы, если бы значения были действительно распределены таким образом, т.е. посчитаем вероятное кол-во примеров в каждой группе для распределения Пуассона с $\\lambda=0.00086$ (среднее кол-во конверсий на посетителя):\n Видно, что наличие 3-х и более конверсий \u0026ndash; крайне маловероятно, ожидаемое количество примеров в этих группах приближается к нулю. То есть распределение Пуассона не очень хорошо описывает то, что происходит в реальной жизни. Количество примеров с нулевой конверсией в наших данных настолько велико, что они \u0026ldquo;перетягивают одеяло на себя\u0026rdquo;, модель считает отличное от нуля значение маловероятным, и занижает прогноз для примеров, в которых ожидается ненулевое количество конверсий. Это видно и на распределении прогнозных значений: Сравнение распределений истинных и прогнозных значений. Видно, что прогнозные значения \u0026ldquo;прижаты\u0026rdquo; влево, в область меньших величин.\n \nВ принципе можно было бы подобрать вместо распределения Пуассона другое, с более тяжелым \u0026ldquo;хвостом\u0026rdquo; справа (fat-tailed), например что нибудь из семейства распределений Парето. Мы пойдем более простым путём, и просто применим к прогнозам преобразование, которое приведёт их к правильному масштабу. Поиск и использование такого преобразования называется калибровкой модели.\nСуществуют разнообразные методики калибровки различной степени сложности, включая даже обучение отдельной модели, которая будет получать на вход прогнозы исходной модели и выдавать откалиброванный прогноз. Но мы не будем мудрить, у нас уже почти правильный прогноз, достаточно просто домножить его на константу и немного скорректировать форму \u0026ldquo;загиба\u0026rdquo; в конце кривой: $$y^* = \\hat{y}^{\\beta}k $$ где $\\hat{y}$ \u0026ndash; исходный прогноз; $y^*$ \u0026ndash; калиброванный прогноз; $k, \\beta$ \u0026ndash; коэффициенты, минимизирующие ошибку прогноза, и подбираемые с помощью кросс-валидации.\nПосле калибровки получается такой график:\n Стало намного лучше! Такой прогноз уже вполне можно использовать для абсолютной оценки потенциала сегмента аудитории.\nИндивидуальные прогнозы Мы рассматривали средний прогноз для аудиторного сегмента, а что с прогнозами на уровне отдельных посетителей, насколько они хороши? Чтобы получить представление об этом, построим распределения прогнозных значений отдельно для каждой группы:\n В идеале все распределения должны быть узкими, вытянутыми по вертикали вдоль истинного значения. В реальности распределения достаточно размыты: для истинного значения 1 конверсия 90% прогнозов находится в интервале от 0.02 до 3.9; для значения 3 конверсии \u0026ndash; в интервале от 0.4 до 8.6.\nТаким образом, совсем точного персонального прогноза не получается: для этого потребовалась бы настоящая машина времени, которая заглянет в будущее, и скажет, какое решение, покупать или не покупать, примет посетитель. Но если делать групповой прогноз, то ошибки \u0026ldquo;в плюс\u0026rdquo; и \u0026ldquo;в минус\u0026rdquo; нейтрализуют друг друга, и точность значительно вырастает.\nПрогноз суммы заказов, shop.ml Используется точно такая же модель, как и для прогноза конверсий, заменяется только целевая переменная. По своим характеристикам модель получается очень похожей на предыдущую, остановимся только на отличиях: Сравнение распределений истинных и прогнозных значений. Распределение истинных значений принципиально отличается от прогнозных.\n  В магазине не бывает заказов стоимостью рубль, и не бывает заказов стоимостью десять рублей, но ноль \u0026ndash; это допустимое значение, соответствующее отсутствию заказов. В результате получается бимодальное распределение, с одиночным пиком в районе нуля, и \u0026ldquo;куполом\u0026rdquo; в районе 100-100000 руб. Естественно, распределение Пуассона не может принять такую сложную форму, поэтому распределение прогнозов сильно отличается от распределения истинных значений, и говорить о точном прогнозе на индивидуальном уровне здесь уже нет смысла.\nТем не менее, на групповом уровне после калибровки получается очень приличный прогноз:\nПрогноз с разбивкой на группы: 0 (нет заказов), 0-2000 руб., 2000-5000 руб., 5000-10000 руб., \u0026gt; 10000 руб.\n  Использование распределения Твиди, которое теоретически может быть мультимодальным, дало результат, практически не отличающийся от распределения Пуассона. Видимо, чтобы получить более правильные прогнозные значения, надо честно смоделировать процесс генерации данных, и делать два прогноза, первый \u0026ndash; будут ли вообще заказы, и второй \u0026ndash; сумма ожидаемых заказов.\nРезультаты для luxshop.ml Тоже получилась очень похожая на предыдущую модель, с близким Concordance Index и с аналогичным влиянием признаков на прогноз: Признаки с наибольшим абсолютным влиянием, сайт luxshop.ml\n \n Ошибка предсказания после калибровки получилась немного больше, чем у shop.ml, возможно потому, что у luxshop.ml в целом меньше данных и меньше размеры групп, например в группе \u0026ldquo;2 конверсии\u0026rdquo; всего 116 посетителей, в отличие от 483 у shop.ml.\nРезультаты для courses.ml На этом сайте нет целей типа \u0026ldquo;заказ\u0026rdquo;, поэтому будем предсказывать не количество конверсий, а количество сессий за следующие 60 дней. Concordance Index этой модели 90%, несколько хуже, чем у магазинов.\nПризнаки с наибольшим абсолютным влиянием, сайт courses.ml\n  Как и в других моделях, наибольшее влияние оказывает признак time_since_last. Но набор остальных признаков, оказывающих заметное влияние, несколько иной.\n Эта модель чуть хуже поддаётся калибровке, чем модели магазинов, но результат всё равно приемлемый.\n Дисперсия прогнозов внутри каждой группы выше, чем при прогнозе конверсий. Например, для группы \u0026ldquo;1 сессия\u0026rdquo; 90% прогнозов лежит в диапазоне [0.0015, 5.7], в то время как для shop.ml аналогичный диапазон \u0026ldquo;1 конверсия\u0026rdquo; более узкий: [0.02, 3.9]. То есть предсказывать количество будущих сессий сложнее, чем количество конверсий.\nScorecards Образцы scorecards для прогноза количества конверсий на сайте shop.ml. Одному делению шкалы соответствует изменение количества конверсий в 2 раза.\n  Одно из преимуществ применяемого нами подхода это то, что прогноз не является чёрным ящиком, или предсказанием, волшебным образом появившимся из недр модели. Ведь довольно трудно принимать бизнес-решения на основе непонятной AI магии. А вдруг модель ошибается? Вдруг на вход пришли такие данные, с которыми модель не обучена работать?\nНо благодаря наличию SHAP values, работа модели становится абсолютно прозрачной. Достаточно просто посмотреть на scorecard, чтобы понять, почему для данного посетителя выдан такой прогноз, и соотнести решение, принятое моделью, с собственным опытом и здравым смыслом. Это не оставляет места никакой магии: есть обоснование решений, можно или согласиться с ним, или не согласиться и перестать использовать модель (или отправить модель на переобучение).\nКстати, не до конца решенная проблема в машинном обучении \u0026ndash; это проверка и тестирование всего конвейера подготовки данных для модели. В коде легко допустить ошибку, которая будет совершенно невидима извне, и единственное её проявление будет в ухудшении качества работы модели. Но заметить это ухудшение очень сложно, т.к. нет эталона качества. SHAP values помогают и здесь: за время подготовки моделей для этих статей я обнаружил с помощью SHAP-диаграмм несколько серьезных ошибок, которые в противном случае надолго (возможно навсегда) остались бы незамеченными.\n Заключение Мы убедились в том, что прогноз будущих конверсий и CLV для посетителей сайта вполне возможен и даёт пригодные для практического использования результаты даже на несложных моделях.\nКонечно, это модели еще не пригодны для применения в production, есть ещё много вещей, которые можно сделать лучше. В частности, мы использовали весьма простое конструирование признаков: всего два уровня адресов страниц; игнорировали содержащиеся в URL параметры, которые могут нести ценную информацию о действиях посетителя; никак не разделяли действия, которые были совершены недавно и действия, которые были в далёком прошлом; использовали из данных о заказах только сумму заказа; не обращали внимания на последовательность действий (брали только суммарное их количество).\nТакже использовался ограниченный набор входных данных: у нас не было ни соцдема, ни поисковых фраз, ни внутренней информации о посетителях, которая содержится в CRM магазина, ни информации о взаимодействии с сайтом на интерфейсном уровне, подобной той, которую собирает WebVisor. Если задействовать больше данных, то прогнозы, естественно, станут более точными.\nСтруктура данных о действиях посетителей в принципе очень близка к структуре данных о действиях пользователей мобильных приложений, т.е. аналогичные модели можно использовать и для работы с CLV в монетизируемых приложениях и играх.\n","date":1553025600,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1553025600,"objectID":"19dcadc0e7a6b1546e71bd0049bf8303","permalink":"https://suilin.ru/post/clv/","publishdate":"2019-03-20T00:00:00+04:00","relpermalink":"/post/clv/","section":"post","summary":"В идеальном мире маркетолог или владелец бизнеса не только работает с \u0026ldquo;конверсиями\u0026rdquo;, но использует для оценки посетителей сайта Customer Lifetime Value (сокращённо CLV) \u0026ndash; грубо говоря, это сумма денег, которую принесёт посетитель за всё время, пока будет пользоваться сервисом. Оценка клиентов по CLV является предпочтительной и для мобильных приложений, и для игр, и для любых онлайн сервисов.\nНо в реальной жизни всё, конечно, не так, как в идеальном мире. Чтобы рассчитать CLV клиента, нужны исходные данные: средний чек, частота покупок, и время \u0026ldquo;удержания\u0026rdquo;, т.","tags":["Conversion","CLV","Lifetime Value"],"title":"Конверсия и data science V. А сколько корова даёт молока?","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" В предыдущей статье мы рассмотрели самую простую модель оценки конверсионности посетителя по параметрам первого посещения. Благодаря использованию SHAP values можно интерпретировать модели любой сложности, поэтому усложним задачу, и оценим конверсионность посетителя, у которого есть история предыдущего взаимодействия с сайтом.\nНаличие такой истории даст нам много новой и полезной информации о намерениях посетителя. Но для начала немного разберёмся с микроструктурой данных, в частности с понятиями \u0026ldquo;визит/сессия\u0026rdquo; и \u0026ldquo;переход\u0026rdquo;, которые в современной интернет-аналитике перемешаны друг с другом. Работа полученных моделей будет оцениваться на тех же сайтах , что и в предыдущей статье.\nЧто такое сессия? Базовое понятие интернет-аналитики, вокруг которого выстраивается большинство показателей, которые мы видим в отчётах, это сессия (она же визит в русскоязычной интерпретации). Под сессией обычно подразумевается интервал времени, в котором не было более чем получасовых пауз в активности посетителя. То есть посетитель пришёл на сайт (сессия началась), что-то посмотрел, ушёл обедать, через час вернулся, и его послеобеденная активность на сайте засчитается уже в новую сессию. Хорошо ли это?\nНа самом деле этот получасовой тайм-аут имеет происхождение ещё из тех доисторических времён, когда пользователь натурально выходил в интернет с помощью модема, и по завершению сеанса работы (с поминутной оплатой) отключался. Естественно, никто в здравом уме не ушёл бы на обед с подключенным модемом и продолжающейся сессией. Получасовой тайм-аут вполне соотносился с реальностью и соответствовал общепринятой практике работы c интернетом.\nНо в современном мире человек находится в онлайне всегда, и сайт может быть открыт во вкладке браузера днями, неделями и даже месяцами: пользователь просто будет переключаться на вкладку с сайтом в моменты, когда в этом возникает необходимость. Эти переключения никак не связаны с \u0026ldquo;сеансом работы в интернет\u0026rdquo;, и тайм-аут 30 минут в современных реалиях оказывается просто взятым с потолка. Поэтому лучше определить тайм-аут, исходя из реальной статистики поведения людей на сайтах.\nРаспределение интервалов между просмотрами страниц каждым посетителем сайта shop.ml. Слева отображены интервалы от 5 минут до 2 часов, справа от 5 минут до 24 часов. Такое же распределение, с незначительными отличиями, наблюдается и на других сайтах.\n  На левой диаграмме видно, что точка \u0026ldquo;30 минут\u0026rdquo; не обладает никакими особенными свойствами: количество интервалов плавно падает с возрастанием размера интервала, и до, и после этой точки. На правой диаграмме более интересная картина: начиная примерно с 8 часов интервалы между просмотрами выходят на \u0026ldquo;равнину\u0026rdquo;, а ближе к 24 часам количество интервалов даже растёт. Рост объясняется суточной цикличностью, например человек посмотрел сайт перед уходом на работу, количество времени было ограничено, поэтому \u0026ldquo;досматривает\u0026rdquo; его на следующий день примерно в то же время. Но нас больше интересует момент выхода на минимальную вероятность интервала: разумно предположить, что 8 часов это и есть наиболее естественный тайм-аут между сессиями. Этот тайм-аут мы и будем использовать в дальнейшем во всех моделях.\nСессии или переходы? \u0026ldquo;Истории жизни\u0026rdquo; нескольких посетителей сайта. Цветными кружками обозначены переходы на сайт, цветными треугольниками \u0026ndash; достижения целей, тонкими вертикальными штрихами \u0026ndash; просмотры страниц, черными точками \u0026ndash; моменты начала сессий.\n  На диаграмме видно, что обычно переход на сайт совпадает с началом сессии, но не всегда. Возможны сессии без переходов, например у пользователей #11 и #17 (вероятно, сайт висит во вкладке браузера и пользователь время от времени переключается на него), и наоборот, возможны переходы внутри сессий. На диаграмме переходы внутри сессий не очень хорошо различимы (несколько полупрозрачных кружков сливаются в один более насыщенный), поэтому рассмотрим подробную историю переходов одного из посетителей, например #15: th {text-align: center;}\n   Дата и время Тип перехода Номер сессии     2016-12-12 00:03:59 organic 1   2016-12-17 15:18:20 ad 2   2016-12-17 15:19:46 direct 2   2016-12-18 00:25:05 ad 3   2016-12-22 00:19:23 direct 4   2016-12-22 00:20:25 ad 4   2016-12-22 00:24:11 ad 4   2017-01-06 00:20:07 ad 5   2017-01-06 00:21:51 direct 5   2017-02-13 19:13:00 ad 6   2017-02-13 19:13:27 ad 6    Видно, что в течение многих сессий совершается два или три перехода. В среднем по всем посетителям количество переходов на 15-30% превышает количество визитов.\nКак с этим поступают современные системы интернет аналитики? Очень просто: каждый новый переход на сайт считается началом нового визита. Фактически переходы приравниваются к визитам, что приводит к тому, что у посетителей, совершающих много переходов, визиты нарезаются в мелкую лапшу. Для систем интернет-аналитики это нормальный компромиссный подход, в противном случае пришлось бы ввести дополнительную сущность \u0026ldquo;переход\u0026rdquo; и все отчёты сильно бы усложнились.\nНо мы при построении моделей не станем упрощать действительность и честно учтём визиты и переходы, как отдельные сущности. Границы визитов будут определяться только таймаутом 8 часов, и ничем более.\nКонструируем признаки Перейдем к собственно моделированию. Мы рассматриваем посетителей, которые уже сделали хотя бы один переход на сайт, то есть у них есть история. История содержит данные о сессиях, переходах, просмотрах страниц, кликах на ведущие вовне ссылки, загрузках файлов и т.п. Как и в прошлой модели, будем рассматривать посетителя в момент, когда он совершает новый переход на сайт (только это будет уже не первый переход), и предсказывать, произойдет ли конверсия до следующего перехода или до завершения сессии.\nК признакам, которые уже были в предыдущей модели, добавляются новые признаки, сконструированные по историческим данным. Пример нескольких таких признаков:\n time_since_prev_landing \u0026ndash; интервал времени с момента предыдущего перехода на сайт. time_since_sess_start \u0026ndash; интервал времени от начала сессии до текущего перехода. Для переходов, совпавших с началом сессии, равен нулю. time_since_prev_sess \u0026ndash; интервал времени от последнего события в предыдущей сессии до текущего перехода landing_num \u0026ndash; порядковый номер текущего перехода во всей истории переходов sess_interval \u0026ndash; средний интервал времени между сессиями посетителя, по данным предыдущих сессий. avg_sess_len \u0026ndash; средняя продолжительность сессии посетителя, по данным предыдущих сессий. $\\sum$order_stepn \u0026ndash; количество достижений цели, являющейся n-ным шагом в конверсионной воронке, т.е. одним из этапов совершения заказа, за всю историю до текущего момента. user_age \u0026ndash; \u0026ldquo;возраст\u0026rdquo; посетителя, т.е. промежуток времени от момента, когда посетитель впервые зашёл на сайт, до текущего перехода. нет смысла засчитывать это время, как активное). $\\sum$path1=X, $\\sum$path2=X \u0026ndash; суммарное количество просмотров страниц в истории, соответствующих пути X (берётся первый уровень пути для path1, первый и второй уровни для path2). Это запись в нотации Айверсона $\\sum[path_i=X]$, у которой опущены для краткости квадратные скобки. В обобщённом виде, без указания конкретного пути, эти признаки записываются как path1_history и path2_history.   Все ли признаки одинаково полезны? На основе истории посетителя можно сконструировать в общем то неограниченное количество признаков, лимитом здесь является только фантазия автора модели. Возникает логичный вопрос \u0026ndash; в какой момент надо остановиться? Проблема в том, что несмотря на объем данных (миллионы посетителей), в них не так много положительных примеров совершившихся конверсий (всего несколько тысяч). В то же время количество признаков в моделях, учитывая что у нас в основном категориальные признаки, которые переводятся в числовые через one-hot encoding, тоже легко переваливает за тысячу и приближается к количеству позитивных примеров. Это означает, что наши модели потенциально подвержены переобучению.\nЕсли подать на вход модели просто шум вместо реальных признаков, всё равно в нём найдутся случайные корреляции с целевой переменной (конверсией), модель сделает вид, что обучилась, и даже будет показывать степень влияния \u0026ldquo;признаков\u0026rdquo; на результат. Но это влияние будет миражом, фейковой новостью, и только введёт в заблуждение пользователя модели. Любой из сконструированных нами признаков в принципе может оказаться таким шумом, и ухудшить способности модели к предсказанию на новых данных. Как же определить, в каких признаках содержится полезный сигнал, а в каких только шум?\nНекоторые виды моделей, например деревья решений, способны выдавать показатели \u0026ldquo;важности\u0026rdquo; признаков (feature importance), и обычно полезные признаки отбирают именно по этим показателям. Но эта \u0026ldquo;важность\u0026rdquo; точно так же переобучается, как и сама модель: в конечном итоге важность это показатель того, сколько корреляций с целевой переменной обнаружила модель в признаке. Это корреляции могут оказаться просто случайными.\nМы пойдём более честным, хотя и вычислительно затратным путём, и будем использовать метод, называемый Sequential Feature Selection. Суть метода простая: в начале есть набор из всех имеющихся признаков. На каждом шаге мы убираем по очереди один из признаков, заново обучаем модель, и смотрим, как изменится качество её работы на тестовой выборке. Если признак был бесполезен и содержал шум, качество повысится. Если мы наоборот убрали полезный признак, качество упадёт. По итогам исключается признак, без которого модель работает лучше всего, и цикл начинается снова, уже с уменьшенным набором признаков. Так признаки исключаются до тех пор, пока не останется ни одного. Признаки, исключенные последними \u0026ndash; самые важные; признаки, исключенные первыми \u0026ndash; самые бесполезные, или даже вредные.\nВычислительная сложность такого метода $q \\times n^2/2$ циклов обучение-валидация, где $n$ это количество признаков, $q$ это количество разбиений в кросс-валидации. Это довольно много, поэтому получить разумное время вычислений удалось только при использовании нескольких GPU.\nНиже представлены результаты для одного сайта (часть признаков скрыта): Результат работы полного цикла Sequential feature selection. В основном доминируют признаки, относящиеся к адресам просмотренных страниц, на их фоне роль других признаков слабо различима.\n \nОтдельно \u0026ldquo;маловажные\u0026rdquo; признаки. Видно, как ошибка модели (logloss) сначала падает, потом начинает расти.\n  Признаки, удаление которых привело к улучшению работы модели (т.е. вредные), показаны красным; признаки, удаление которых привело к улучшению (полезные) показаны зелёным; нейтральные \u0026ndash; серым.\nПо результатам feature selection для каждого сайта были отброшены признаки из красной зоны и часть признаков из серой.\nМодель для luxshop.ml, результаты Проверка обученной модели на тестовой выборке дала AUC около 86%, что очень неплохо для предсказаний действий посетителей, ведущих себя в общем то случайным образом. AUC заметно вырос по сравнению с предыдущей моделью по первому переходу на сайт (было ~75%), потому что появилось много дополнительной информации из истории посетителя.\nВ предыдущих моделях все используемые признаки были категорийными, и мы измеряли только влияние наличия признака, т.е. да/нет. В новых моделях многие признаки это числовые значения, например время с момента последнего перехода может быть любым числом от нуля до $\\approx3.2 \\times 10^7$ (количество секунд в году), поэтому поход к интерпретации влияния признаков будет другим.\nДля начала визуализируем признаки, обладающие наибольшим абсолютным влиянием на модель:\n Это довольно сложная визуализация, требующая пояснений. Цвет объекта на диаграмме соответствует числовому значению признака. Минимальным значениям (в нашем случае это обычно ноль) соответствуют голубые цвета, максимальным значениям \u0026ndash; красные, промежуточным значениям \u0026ndash; оттенки синего, фиолетового и пурпурного. Диапазон от минимальных до максимальных значений индивидуален для каждого признака, например у бинарных признаков (таких как regionCity=RussiaMoscow) всего два значения, 0 и 1, и, соответственно, всего два цвета, красный и голубой.\nЗначение некоторых признаков равно ∅. Этим символом обозначается отсутствие логического значения, например lastSearchEngine=∅, если переход совершался не из поисковой машины, или path2=∅, если в URL перехода был только первый уровень, например / или /catalog. Можно интерпретировать символ ∅, как \u0026ldquo;дырка от бублика\u0026rdquo;.\nКаждый признак представлен набором точек: одна точка это один пример из тестовой выборки. Точек много, поэтому они сливаются в вытянутые по горизонтали облака. Координата точки по оси X соответствует степени влияния признака на результат. Вдоль диаграммы проходит вертикальная нулевая ось (серая линия), чем точка правее от этой оси, тем больше признак действует \u0026ldquo;в плюс\u0026rdquo;, чем левее, тем больше \u0026ldquo;в минус\u0026rdquo;. Размер этого влияния есть на шкале внизу диаграммы.\nЧтобы облако точек для каждого признака не превратилось в горизонтальную линию, точки случайным образом разбросаны немного вверх и вниз от горизонтальной оси, соответствующей признаку. Чем больше точек уложено на отрезке оси, тем толще получается в этом месте облако после \u0026ldquo;разброса\u0026rdquo;. Например, если облако толще всего в околонулевых координатах по оси X, это означает, что для большинства примеров этот признак оказывает околонулевое воздействие на результат.\nЧтобы стало понятнее, рассмотрим несколько топовых признаков по отдельности.\n$\\sum$path1=/basket/  Количество просмотров страниц в истории, URL которых начинался с /basket/. Нулевые значения (голубое облако слева) оказывают отрицательное воздействие, положительные (вытянутая красно-фиолетовая линия справа) \u0026ndash; сильно положительное. Степень положительного воздействия значительно отличается от примера к примеру (большая дисперсия, вытянутая линия вместо облака). Эти же значения можно отобразить на развёрнутой диаграмме: \nЭта диаграмма тоже требует пояснений. По оси X \u0026ndash; значения нашего признака, в логарифмической шкале (от 0 до примерно 300). По оси Y \u0026ndash; соответствующее воздействие на результат модели. Каждая точка это один пример из тестовой выборки. Значения немного \u0026ldquo;разбросаны\u0026rdquo; по оси X для лучшей визуализации, в противном случае например все значения, где X=0, выстроились бы в узкую вертикальную полоску, на которой сложно было бы разглядеть детали. Чем ближе значение к нулю, тем больше \u0026ldquo;разброс\u0026rdquo;.\nОкраска точек отличается от окраски на мини-графике. Цвет каждой точки соответствует значению второго признака, в данном случае второй признак это UTMMedium=∅. Символ ∅, напомню, обозначает \u0026ldquo;пустоту\u0026rdquo;, т.е. если в URL перехода на сайт не было метки UTMMedium, значение признака UTMMedium=∅ равно единице, а если метка была, значение равно нулю. Но какое отношение UTMMedium имеет к признаку $\\sum$path1=/basket/, зачем нам вообще второй признак?\nДело в том, что мы используем достаточно сложные модели, в которых большую роль играет взаимодействие признаков (feature interaction). Другими словами, влияние, которое оказывает на результат работы модели один признак, может зависеть не только от значения самого этого признака, но и от значений других признаков, взаимодействующих с первым. Визуализировать такое множественное взаимодействие непросто, поэтому применяется упрощённый подход: берётся единственный признак, который более всего взаимодействует с основным, и его значения используются, как палитра для раскраски точек. Соответствие цветов значениям вторичного признака отображается на вертикальной цветной полоске справа. В данном случае у нас бинарный признак, 0 или 1, поэтому на полоске всего два цвета, нулю соответствует голубой, а единице \u0026ndash; красный.\nИтак, что же мы видим на подробной диаграмме?\nЕсли в истории посетителя не было ни одного захода на страницы, связанные с корзиной, т.е. значение признака $\\sum$path1=/basket/ равно нулю, вероятность конверсии понижается примерно на 0.5 единиц. При этом, если в URL перехода на сайт нет метки UTMMedium, негативное влияние нулевого значения основного признака ослабляется: мы видим, что верх левого \u0026ldquo;столбика\u0026rdquo; на графике окрашен в красный цвет, а низ \u0026ndash; в синий. Красный соответствует единичному значению вторичного признака, т.е. \u0026ldquo;метки нет\u0026rdquo;, а синий - наоборот, \u0026ldquo;метка есть\u0026rdquo;.\nЕсли в истории был хотя бы один заход на страницу, относящуюся к корзине, вероятность конверсии сразу резко возрастает (для всех значений X \u0026gt; 0 вероятность повышается на 1-2 единицы). При этом значение вторичного признака уже не играет роли: красные и синие точки в \u0026ldquo;ненулевой\u0026rdquo; части графика перемешаны более-менее равномерно.\nПри повышении значения признака, положительное влияние растёт, т.е. чем чаще посетитель заходил в корзину, тем выше вероятность конверсии; при более чем 100 заходах положительное влияние может достигать трёх единиц.\nВот такое длинное описание потребовалось, чтобы расшифровать такую маленькую диаграмму 😌 Но мы теперь умеем интерпретировать такие диаграммы, и следующие описания будут короче.\nregionCity=RussiaMoscow  Это бинарный признак, интерпретация очень простая: положительные значения (посетитель из Москвы, красное облако справа) повышают конверсию, отрицательные (не из Москвы, голубое облако слева) \u0026ndash; понижают. \nЗаметно любопытное взаимодействие: если посетитель перешёл из поисковика (нулевое значение lastAdvEngine=∅, голубая окраска точек), влияние Москвы, как отрицательное, так и положительное, снижается.\ntime_since_prev_sess  По мини-графику сложно сказать что-то конкретное, кроме того, что в большинстве случаев воздействие на конверсию околонулевое или слабо отрицательное (по положению утолщений). \nВиден большой разрыв в значениях, вызванный 8-часовым тайм-аутом сессии. Если переход происходит в рамках первой сессии, время от предыдущей сессии принимается равным нулю, в противном случае время от конца предыдущей сессии будет как минимум 8 часов.\nПереход в первой сессии несколько снижает вероятность конверсии. Для переходов, совершённых в следующих сессиях, зависимость очень любопытна: есть \u0026ldquo;хорошее\u0026rdquo; время, примерно в течение недели после сессии, и \u0026ldquo;плохое\u0026rdquo;, от недели до двух месяцев. По видимому, в течение первой недели посетитель ещё \u0026ldquo;горячий\u0026rdquo;, с твёрдым намерением сделать покупку, а потом \u0026ldquo;остывает\u0026rdquo; и заходит на сайт просто посмотреть на товары, не собираясь их приобретать, или просто помимо своей воли перекидывается на сайт рекламными системами.\nПрослеживается также интересное взаимодействие со вторым признаком.\n$\\sum$path2=∅ Это значение признака интерпретируется, как количество раз, когда посетитель заходил на \u0026ldquo;топовые\u0026rdquo; страницы сайта, не опускаясь вглубь, на второй уровень. \nПо мини-графику можно предположить, что большие значения признака имеют в среднем нулевой эффект (красные точки сконцентрированы около нуля), а околонулевые и близкие к ним значения могут проявлять как положительное, так и отрицательное воздействие на конверсию. Также в большинстве случаев воздействие признака на конверсию равно нулю или чуть больше нуля (есть утолщение в районе нуля). \nРазвёрнутая диаграмма подтверждает наше предположение: Нулевое значение отрицательно действует на конверсию, единичное - максимально действует в \u0026ldquo;плюс\u0026rdquo;, и по мере увеличения значений признака воздействие уменьшается и становится снова отрицательным.\nТакже любопытно взаимодействие со вторым признаком: messenger, как UTM источник, сильно увеличивает негативное воздействие для нулевого значения основного признака.\nДальше будем рассматривать только развёрнутые диаграммы, т.к. дано уже достаточно примеров интерпретации мини-графиков.\n$\\sum$pageview_duration  Хороший посетитель просматривал сайт достаточное время, но не слишком долго, оптимальное время от 10 минут до нескольких часов. Посетители, которые разглядывали сайт магазина более 6 часов, видимо используют его, просто как источник для вдохновения.\n$\\sum$path1=/delivery/  Идеальный посетитель интересуется доставкой дважды. Лучше, если это происходит в первой или второй сессии. Для посетителей, интересовавшихся доставкой более 7 раз, есть подозрение на проблемы с памятью: возможно, делать покупки они тоже забывают.\nlanding_num  Принято считать, что чем чаще посетитель заходит в магазин, тем выше его лояльность, и тем лучший он покупатель. Но эта диаграмма показывает прямо обратное. Забегая вперёд, скажу, что похожая зависимость наблюдается и на других сайтах, т.е. это не случайное статистическое отклонение и не особенность этой модели. Причины такой зависимости будут более подробно разобраны позже, на примере сайта shop.ml.\ntime_since_sess_start  Модель показывает, что посетителю в идеале нужен примерно час от начала сессии, чтобы дозреть до решения о покупке.\nuser_age  Как и в случае с landing_num, здесь не наблюдается однозначного повышения \u0026ldquo;качества\u0026rdquo; посетителей с увеличением времени их знакомства с магазином. Наоборот вероятность конверсии с возрастом падает, и только у \u0026ldquo;старой гвардии\u0026rdquo; возрастом более 120 дней опять начинает повышается. $$\\DeclareMathOperator{\\E}{E}$$\nРанжирование по среднему воздействию признака До этого момента мы ранжировали признаки по силе их абсолютного воздействия, то есть по $\\sum_n|S_i|$, где $S_i$ это SHAP value i-го признака, $n$ \u0026ndash; количество примеров в тестовой выборке. Но можно отранжировать признаки и по матожиданию $\\E[S_i]$, как мы делали это в предыдущей статье, и построить соответствующую диаграмму. Тогда в топ выйдут признаки, которые в среднем влияют максимально положительно или максимально отрицательно: \nВсю верхушку положительного топа оккупировал признак $\\sum$path1=/basket/, это соответствует тому, что мы уже видели на диаграммах. Этот же признак, но с нулевым значением \u0026ndash; второй по негативному воздействию. Также в топе неожиданно засветился признак path1=/404, скорее всего соответствующий адресу страницы с ошибкой 404 Page not found. Можно предположить, что такая ошибка выдавалась посетителям, у которых было твёрдое намерение приобрести определённый товар, исчезнувший из ассортимента магазина, и они всё равно делали заказ, приобретая замену.\nUser scorecards Из рассмотренных примеров видно, что интерпретация воздействия признаков в продвинутой модели это не такая простая задача. Все диаграммы, которые мы построили, не охватывают думаю и нескольких процентов возможных кейсов по воздействию на конверсию и особенно по взаимодействию признаков друг с другом.\nПоэтому для продвинутых моделей особенно хороши user scorecards, которые позволяют без построения гипотез и абстрактных рассуждений просто взять и увидеть, как конкретные значения признаков влияют на вероятность конверсии конкретного посетителя сайта. \nНапомню, черная горизонтальная черта это то, насколько вероятность конверсии данного посетителя отличается от средней по сайту (средней конверсии соответствует горизонтальная пунктирная линия, идущая из нулевой точки); красные столбцы над этой чертой \u0026ndash; положительно влияющие признаки, размер каждого столбца равен силе влияния признака; синие столбцы под чертой \u0026ndash; признаки с отрицательным влиянием.\nВидно, что на конверсию каждого посетителя влияет большое количество признаков, их даже не получилось все детализировать на диаграмме (розовая и светло-голубая \u0026ldquo;шапки\u0026rdquo; сверху и снизу у каждого посетителя)\nРезультаты для shop.ml Пр проверке модели на тестовой выборке получен AUC 84%. Посмотрим, что есть интересного в признаках: \nПрежде всего, отметим сильное влияние признака $\\sum$path1=/cart/. \nУже единственный заход на страницу, связанную с корзиной, поднимает вероятность конверсии на полпункта, а три захода \u0026ndash; на целый пункт (напомню, один пункт это изменение вероятности конверсии примерно в два раза). Ни один другой признак не влияет так сильно.\nCильное влияние и у признака $\\sum$path2=/cart/done, также в топе есть $\\sum$path2=/cart/laststep и $\\sum$path1=/delivery/. Похоже, что работа с корзиной и доставкой является критически важным фактором в оценке посетителя.\nТеперь посмотрим на признаки с выраженным отрицательным влиянием. Здесь нас ждёт сюрприз. \nПризнак с самым явно выраженным негативным влиянием это landing_num, порядковый номер захода на сайт. Про росте значения признака конверсионность посетителя резко падает. Казалось бы, должно быть наоборот: чем чаще человек пользуется сайтом, тем более это лояльный покупатель? \nСхожим образом работает признак $\\sum$UTMCampaign=∅ и близкие к нему по смыслу признаки $\\sum$referer=∅ и $\\sum$lastSocialNetwork=∅. \nПри увеличении \u0026ldquo;возраста\u0026rdquo; посетителя его конверсионность тоже падает. Что же происходит с посетителями, у них постепенно вырабатывается отвращение к покупкам?\nНа самом деле нет. В данных сайта shop.ml заметно, что часть трафика приходит из довольно сомнительных источников. Представим источник, перекидывающий людей на сайт через popunder, clickunder или другим не слишком цивилизованным способом. У посетителей из таких источников будет много переходов, но так как они не собираются ничего покупать, они не будут работать с корзиной. Таких посетителей много: из всех посетителей, совершивших более 10 переходов на сайт shop.ml, меньше трети когда либо пользовались корзиной.\nНо если посетитель совершает переходы и не работает с корзиной, значит это мусорный трафик, и модели надо научиться его распознавать? Да, и именно поэтому у признаков landing_num, user_age и признаков, связанными с пустыми UTMCampaign / referer / socialNetwork, наблюдается негативное влияние. Как только посетитель воспользуется корзиной, весь негатив от этих признаков сразу будет скомпенсирован сильным положительным влиянием признака $\\sum$path1=/cart/. Ну а пока посетитель набирает переходы на сайт и не использует корзину, априори считаем его низкокачественным трафиком \u0026ndash; вполне разумно.\nВ подтверждение этой версии, на диаграммах заметно взаимодействие признака $\\sum$path1=/cart/ и генерирующих негатив признаков landing_num и $\\sum$UTMCampaign=∅: работа с корзиной снижает степень негатива.\nВзаимную компенсацию признаков видно и на scorecards: \nУ посетителя A присутствует весь набор негативных признаков, тем не менее у него повышенная более чем на 2.5 пункта вероятность конверсии, потому что есть работа с корзиной. У посетителя B негативные признаки выражены не так явно, но прогноз конверсии в целом негативен, т.к. заходов в корзину не было.\nИтак, фактически у нас получилась модель оценки качества трафика, использующая конверсионность, как метрику качества. Неплохо, не правда ли?\nРезультаты для courses.ml Для этого сайта не существует цели \u0026ldquo;заказ\u0026rdquo;, поэтому сделаем синтетическую цель, соответствующую бизнес-задачам сайта. Сайт существует, чтобы обучать, поэтому конверсию будем засчитывать, если посетитель продержался на сайте ещё как минимум полчаса после перехода. Будем считать, что в эти полчаса он обучался, а не просто рассматривал картинки.\nСуммарное влияние признаков: \n$\\sum$pageview_duration Более всего на результат влияет суммарное время просмотра страниц, т.е. модель предполагает, что если посетитель уже потратил заметное количество времени на обучение, он будет продолжать обучаться дальше. \nНа детальной диаграмме зависимость выражена очень явно.\ntime_since_prev_sess  Прослеживается интересная форма зависимости, похожая на синусоиду. Рассмотрим этот участок поближе, и в линейной шкале по оси X: \nДействительно синусоида! Наша модель самостоятельно вывела такую сложную зависимость из входных данных, которые в общем то не предполагали никакой периодичности. Но какова причина появления этой синусоиды?\nЛюди серьезно занимающиеся самообучением, обычно проходят курс по вечерам после работы или после домашних дел. Или по утрам: конкретное время неважно, главное, что обучение происходит примерно в одно и то же время каждый день. В течение для есть довольно узкий интервал времени, в течение которого человек может сесть за обучение и по настоящему в него погрузиться. Если начать слишком рано, скорее всего найдутся другие занятия: работа, дом, дети, и т.п. Если начать слишком поздно, то просто пора будет спать. То есть, если с момента завершения предыдущей сессии обучения прошло менее 12 часов или более 22-24 часов, то новая сессия скорее всего получится неудачной или вовсе не начнётся. Примерно это мы и видим на диаграмме.\nСинусоида с периодом в сутки образуется, потому что не принципиально, когда закончилась предыдущая сессия: вчера, позавчера, или три дня назад. Всё равно начало следующей сессии наиболее вероятно только в определённом интервале времени.\nПочему максимум вероятности приходится не ровно на 24 часа с момента предыдущей сессии? Потому что время отсчитывается от конца предыдущей сессии, а не от начала. Сессии имеют достаточно большую длительность, она отображена справа на шкале взаимодействующего признака avg_sess_len. Размерность шкалы \u0026ndash; секунды. Для понимания масштаба: 5 часов это 18000 секунд. Там же видно, что наибольший \u0026ldquo;размах\u0026rdquo; синусоиды наблюдается у посетителей с длинными сессиями. Если сеансы обучения короткие, то обучаться можно в принципе в любое время дня, и роль суточной периодичности снижается.\nsess_num  Онлайн обучение это занятие для достаточно терпеливых и упорных людей. Терпения и упорства не всем хватает надолго. После четвёртой сессии видно отрицательное влияние растущего номера сессии: известно, что онлайн курсы проходит до конца только небольшой процент начавших обучение. Кроме того, для усидчивых посетителей будет наблюдаться компенсационный эффект от растущего $\\sum$pageview_duration (он виден и по взаимодействию признаков), а большие значения sess_num могут использоваться моделью для определения случайных переходов на сайт \u0026ndash; аналогично механизму, описанному для сайта shop.ml.\ntime_since_sess_start  Для переходов, которые не совпали с началом сессии, модель вероятно рассуждает так: если посетитель просидел на сайте больше 10 минут, то скорее всего просидит и ещё полчаса. А вот если время с начала сессии приближается к 6 часам, то пора бы уже и закругляться!\ntime_since_prev_landing  Синусоида, аналогичная time_since_prev_sess, но максимумы приходятся уже ровно на 24 часа, т.к. нет поправки на продолжительность сессии.\nДополнительные визуализации  Эта визуализация представляет собой взгляд с альтернативной точки зрения, выявляющий несколько другие признаки. На что можно обратить внимание:\n path1=/unsubscribe/ резко понижает вероятность продолжения сессии. Неудивительно.\n $\\sum$path2=/lesson/J \u0026ndash; по видимому очень неудачный курс: единственного захода на эту страницу достаточно, чтобы посетитель перестал надолго задерживаться на сайте. $\\sum$linkUrl=https://vk.com/courses \u0026gt; 8 \u0026ndash; вероятно, на эту ссылку кликают посетители, которые не удовлетворены имеющимися на сайте курсами. $\\sum$lastTrafficSource=saved \u0026gt; 2 \u0026ndash; посетители, которые сохранили страничку сайта себе на компьютер, и переходят с неё на сайт, явно заинтересованы в продолжительном обучении.  И, в дополнение, образцы scorecards для пары \u0026ldquo;хороших\u0026rdquo; и пары \u0026ldquo;плохих\u0026rdquo; посетителей: \nЗаключение  \u0026hellip;We have so much data on the web, almost all of it available for free, that we dive into the the data ocean hoping that magically awesome things will follow. They never do.\nAvinash Kaushik.\n Основная проблема интернет-аналитики, на мой взгляд, в том, что в ней физически огромное количество данных, но эти данные приносят удивительно мало пользы своим обладателям.\nПочему? Потому что это не самые удобные для обработки данные, у них сложная структура, это sparse categorical multivariate time series \u0026ndash; подобные типы данных мейнстримовая Machine Learning (а сейчас и AI) индустрия традиционно обходит стороной. Ведь гораздо эффективнее впечатлять инвесторов распознаванием изображений, генерацией музыки или чат-ботами, чем копанием в каких то там логах сессий 🙊\nОтчасти поэтому традиционные инструменты интернет аналитики застряли на примитивном уровне и не способны извлечь из данных даже малую часть содержащейся в них полезной информации. За время работы с моделями, описанными в этой статье, я, честно говоря, узнал о поведении посетителей сайтов намного больше, чем за пару лет работы над WebVisor и несколько лет работы в Яндекс.Метрике.\nХочется верить, что разумное применение технологий машинного обучения и AI поможет наконец раскрыть потенциал океана данных интернет-аналитики, и что \u0026ldquo;awesome things\u0026rdquo; действительно \u0026ldquo;will follow\u0026rdquo;.\n","date":1552852800,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1552852800,"objectID":"be1b3c908e209404e2461d7943fb83ea","permalink":"https://suilin.ru/post/conversion_history/","publishdate":"2019-03-18T00:00:00+04:00","relpermalink":"/post/conversion_history/","section":"post","summary":"В предыдущей статье мы рассмотрели самую простую модель оценки конверсионности посетителя по параметрам первого посещения. Благодаря использованию SHAP values можно интерпретировать модели любой сложности, поэтому усложним задачу, и оценим конверсионность посетителя, у которого есть история предыдущего взаимодействия с сайтом.\nНаличие такой истории даст нам много новой и полезной информации о намерениях посетителя. Но для начала немного разберёмся с микроструктурой данных, в частности с понятиями \u0026ldquo;визит/сессия\u0026rdquo; и \u0026ldquo;переход\u0026rdquo;, которые в современной интернет-аналитике перемешаны друг с другом.","tags":["Conversion"],"title":"Конверсия и data science IV. Кто владеет прошлым, тот контролирует будущее","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" $$\\DeclareMathOperator{\\E}{E}$$ Как понять, что влияет на конверсию положительно, а что отрицательно? В любом отчёте Google Analytics или Я.Метрики есть столбец \u0026ldquo;конверсия\u0026rdquo;, и сравнив конверсию в разных строках отчёта, казалось бы, можно получить исчерпывающий ответ.\nНо не надо забывать, что любой отчёт показывает срез данных, сделанный по единственному измерению (источник трафика, география, и т.п.), что даёт неполную, а иногда даже неверную картину. Рассмотрим простой пример: есть два города, Москва и Екатеринбург и два вида трафика, органический и реферальный. Данные по кол-ву визитов и конверсий сведены в кросс-таблицу в виде конверсии/визиты = конверсионность:\nth {text-align: center;}     Referral Organic Всего (город)     Москва $1/100=1.00\\%$ $15/1000=1.50\\%$ $16/1100=\\mathbf{1.45}\\%$   Екатеринбург $12/1000=1.20\\%$ $2/100=2.00\\%$ $14/1100=\\mathbf{1.27}\\%$   Всего (источник) $13/1100=1.18\\%$ $17/1100=1.54\\%$     В отчёте по географии будет конверсионность 1.45% для Москвы и 1.27% для Екатеринбурга (последняя колонка), т.е. трафик из Москвы кажется более конверсионным. Но если присмотреться внимательнее, видно, что конверсионность трафика из Москвы на самом деле меньше для обоих видов трафика, 1.0% против 1.2% в реферальном трафике и 1.5% против 2.0% в органике!\nПричина кажущейся высокой конверсионности Москвы в том, что в ней больше больше доля органического трафика, который по своей природе более конверсионный. То есть, чтобы сделать полноценный вывод о том, что влияет на конверсионность, анализировать единственное измерение недостаточно.\nТеоретически можно построить отчёт одновременно по двум измерениям, но во-первых надо ещё догадаться, какие измерения пересекать и на какие цифры обращать внимание, во-вторых в реальной жизни измерений много, десятки, и все они одновременно взаимодействуют друг с другом. На конверсионность влияет одновременно и география, и источник трафика, и время суток, и то, с какого устройства заходят на сайт, и многое другое. Построить отчёт сразу по десятку измерений и понять, от чего на самом деле зависит конверсия, для обычного человека нереально: слишком много данных.\nНужны более совершенные автоматические способы оценки воздействия факторов, учитывающие совместный вклад всех измерений. Результатом может быть сводный отчёт/инфографика, присваивающие рейтинг каждому фактору, например Екатеринбург даёт +0.3 к конверсии, а реферальное происхождение трафика даёт -0.5.\nЛинейная модель Самый простой и часто используемый способ решения задач, похожих на нашу - использование линейной модели. Дадим каждому фактору вес $w_i$, являющийся оценкой влияния фактора. Если наличие соответствующего фактора увеличивает вероятность конверсии, то вес положителен: чем он больше, тем больше увеличивается вероятность. Отрицательные веса наоборот уменьшают вероятность конверсии.\nСумма всех факторов каждого $j$-го посетителя, умноженных на их веса, даст значение $z_j$, называемое также logits. Факторы в нашем случае задаются индикаторными переменными, принимающими значения 1 (фактор присутствует) и 0 (фактор отсутствует). Для приведённого выше примера было бы четыре фактора город=Москва, город=Екатеринбург, источник=Organic, источник=Referral и четыре соответствующих переменных $x_1,\\dots,x_4, \\; x \\in \\{0,1\\}$ , плюс одна свободная переменная $b$, или bias, которая будет примерно соответствовать средней конверсии сайта: $$z_j = w_1x_{j,1} + w_2x_{j,2} + w_3x_{j,3} + w_4x_{j,4} + b$$ или в обобщённой векторной записи: $$z_j=\\mathbf{w}^\\top\\mathbf{x_j} + b$$\nПеременные, формирующие вектор $\\mathbf{x_j}$, характеризующий каждого посетителя сайта, в машинном обучении называются признаками.\nВ зависимости от того, что мы хотим в итоге вычислить, выбирается link-функция, преобразующая logits в целевую переменную ($y$). В нашем случае примем за целевую переменную факт наличия конверсии, т.е. $y=1$, если конверсия случилась, и $y=0$, если не случилась. В этом случае link-функцией будет сигмоид: $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$ Модель будет выдавать вероятность конверсии для заданного набора факторов ($\\mathbf{x}$). Такой тип модели называется логистическая регрессия: $$\\Pr(y=1|\\mathbf{x})=\\sigma(\\mathbf{w}^{\\top}\\mathbf{x} + b)$$\nДалее используется стандартная процедура машинного обучения, подбирающая такие веса для факторов, которые максимизируют правдоподобие модели. На деталях обучения не будем останавливаться, интереснее другое \u0026ndash; что получится на реальных сайтах?\nСайты Эту и все последующие модели будем тестировать на данных реальных сайтов, полученных через Logs API Я.Метрики. Будем использовать три сайта, дадим им условные названия:\n shop.ml \u0026ndash; e-commerce сайт, продающий недорогие товары, в основном хозяйственно-бытового назначения luxshop.ml \u0026ndash; тоже e-commerce, с достаточно дорогими товарами, не являющимися предметами первой необходимости courses.ml \u0026ndash; сайт с онлайн курсами.  Настоящих адресов раскрывать не будем, т.к. в результатах работы моделей находятся достаточно чувствительные данные. Адреса страниц сайтов и имена рекламных кампаний тоже изменены, чтобы сайт было невозможно идентифицировать.\nДля каждого сайта используется выборка данных за один год, в которую входит от 1 до 10 млн. уникальных посетителей. Число реальных посетителей в 2-4 раза меньше, из за того, что посетители часто теряют cookies (чистят, переустанавливают windows, меняют смартфоны, и т.п.)\nОценка качества моделей Чтобы понять, насколько модель соотносится с реальной жизнью, будем оценивать её качество по результатам прогноза на тестовой выборке, т.е. на данных, которые модель не видела в процессе обучения и настройки параметров. Есть два варианта формирования тестовой выборки:\n Равномерный сэмпл, например случайным образом отобранные 10% от всех данных. Это традиционный способ, принятый в machine learning. Данные делятся на две части, \u0026ldquo;прошлое\u0026rdquo; и \u0026ldquo;будущее\u0026rdquo;, за будущее принимаются данные например за последний месяц, за прошлое - всё остальное. \u0026ldquo;Будущее\u0026rdquo; это тестовая выборка, \u0026ldquo;прошлое\u0026rdquo; - обучающая выборка.  Последний способ ближе соответствует тому, как модели применяются в реальной жизни, поэтому мы будем использовать в основном его. Прогноз из \u0026ldquo;прошлого\u0026rdquo; в \u0026ldquo;будущее\u0026rdquo; является более сложным, т.к. здесь проявляется нестационарность: например рекламные кампании, которые давали хорошую конверсионность в прошлом, совсем не обязательно будут так же хорошо работать в будущем.\nДля единообразия, и чтобы модели были сравнимы друг с другом, будем всегда использовать метрику качества AUC (Area Under Curve).\nСтруктура модели Вернёмся к нашей модели. Будем прогнозировать конверсии для посетителей, впервые оказавшихся на сайте, т.е. не имеющих никакой предыдущей истории. Конверсия должна случиться в рамках первой сессии, т.е. отложенные конверсии не учитываются. Модель работает со следующими признаками:\n browser \u0026ndash; браузер посетителя device \u0026ndash; тип устройства, desktop/smartphone/tablet/tv, advEngine \u0026ndash; рекламная система, из которой произошел переход searchEngine \u0026ndash; поисковик, из которого произошел переход socialNetwork \u0026ndash; соцсеть, из которой произошёл переход referer \u0026ndash; сайт, с которого произошел переход trafficSource \u0026ndash; тип перехода: прямой, organic, реферальный и т.п. mobilePhone \u0026ndash; бренд смартфона/планшета networkType \u0026ndash; тип сетевого подключения, если известен - ethernet/wifi/etc OS \u0026ndash; операционная система посетителя city \u0026ndash; город посетителя country \u0026ndash; страна посетителя urlDomain \u0026ndash; домен из адреса landing page. Смысл в том, что у сайта может быть несколько доменов, некоторые используются для внутренних нужд, например тестирования перед выкаткой релиза (test.shop.ml, dev.shop.ml), и модели надо уметь различать эти домены. landing_l1 \u0026ndash; первый компонент пути из landing page. Например, для адреса http://shop.ml/catalog/electronics/audio/1 это будет /catalog/ landing_l2 \u0026ndash; второй компонент пути из landing page. Для адреса http://shop.ml/catalog/electronics/audio/1 это будет /catalog/electronics/ UTMCampaign, UTMMedium, UTMSource, UTMTerm \u0026ndash; UTM тэги, которые присутствовали в landing page URL.\n openstatService, openstatSource - OpenStat разметка, автоматически проставляется для переходов из Я.Директа.  Все эти признаки являются категорийными, т.е. каждый из них для конкретного посетителя принимает одно значение из набора возможных, например city:Moscow, city:Krasnoyarsk, и т.п. Набор возможных значений ограничен частотой встречаемости: значения, встречающиеся реже определённого порога, помещены в отдельную категорию (other), например city:(other). У признака может не быть явного значения, например значение mobilePhone не определено, если посетитель зашёл на сайт c desktop устройства. В этом случае используется специальная категория \u0026ldquo;значение отсутствует\u0026rdquo;: mobilePhone:-.\nКатегорийные признаки переводятся в бинарные (т.е. принимающие значения только 0 или 1) с помощью one-hot кодирования.\nЛинейная модель, результаты Конверсионность на e-commerce сайтах невысокая, в районе 0.1-0.3%. Т.е. на 1000 посетителей приходится всего 1-3 конверсии, что делает обучение модели не совсем тривиальной задачей из за несбалансированности количества позитивных и негативных примеров. Так, для сайта luxshop.ru в обучающей выборке всего 2458 позитивных примеров (конверсий), несмотря на казалось бы большой объем выборки 2.5 млн посетителей.\nТем не менее, линейная модель вполне работоспособна на таких данных: AUC для сайтов shop.ml и luxshop.ml находится в районе 76%-78%. AUC, заметно отличающийся от 50%, говорит о том, что модель выявила статистические закономерности в данных, поэтому веса факторов, которые получились в результате обучения, не являются случайными.\nПосмотрим на результаты. Выведем факторы, которые больше всего влияют на конверсию, как в положительную так и в отрицательную стороны, для магазина shop.ml:\n  Положительные факторы сверху и окрашены красным, отрицательные - снизу и окрашены синим. Наблюдаемая картина в целом не противоречит здравому смыслу, например для отрицательных факторов:\n OS Ubuntu скорее всего используется ботами или для тестирования Посетители из Украины и Казахстана вряд ли будет делать заказы в магазине, работающем в другой стране. браузер Opera Mini уже весьма маргинален, и скорее всего говорит об использовании устаревшего смартфона и пониженной платежеспособности владельца.\n  Но это ещё не окончательная версия. Обратим внимание на фактор country:Russia. Он имеет солидный положительный вес ~1.4, при этом доля России в трафике сайта ~95%. C позиции здравого смысла страновые факторы для остальных 5% трафика должны иметь огромный отрицательный вес (в районе -27), чтобы уравновесить положительное влияние России. Однако этого не наблюдается, и если учесть влияние всех страновых факторов для каждого посетителя из обучающий выборки, то в среднем получается положительное влияние ~1.34. То есть веса, получаемые из линейной модели, не гарантируют что сумма факторов для каждой группы признаков (страна, город, и т.п.) будет в среднем нейтральной. Но это легко исправить, рассчитав для каждого признака поправку (насколько среднее по всем посетителям влияние факторов по этой переменной отклоняется от нуля), и скорректировав веса на эту поправку. $$\\phi_i=w_i - \\frac{1}{N}\\sum_{k=1}^N\\sum_{j\\in G_i} w_jx_{k, j}$$\n $\\phi_i$ \u0026ndash; фактор для $i$-го признака. $w_i$ \u0026ndash; вес из линейной модели для $i$-го признака. $N$ \u0026ndash; количество примеров в обучающей выборке. $G_i$ \u0026ndash; индексы всех признаков, относящихся к той же группе, что $i$-ый признак, например индексы всех страновых признаков. $x_{k, j}$ \u0026ndash; значение (0 или 1) $j$-го признака для $k$-го примера из обучающей выборки.  Посмотрим, что получилось:\n  Страновые факторы действительно стали другими, остальные тоже немного скорректировались. Как и ожидалось, у России теперь фактор, близкий к нулю, поэтому она не попала в топ, зато в топе много зарубежных стран с отрицательными факторами, что хорошо соответствует здравому смыслу.\nСамое большое положительное влияние, с большим отрывом \u0026ndash; у белорусского города Могилёв. Действительно, конверсия трафика из Могилёва целых 3.6%, при средней конверсии 0.38%, и средней конверсии трафика из Беларуси (без Могилёва) 0.31%. Возможно, это аномалия, вызванная неверным определением географического положения (география определяется по IP адресам).\nИтак, линейная модель вполне работоспособна. В то же время у неё есть очевидные недостатки, связанные с её простотой:\n Линейная модель хорошо работает только для самых простых прогнозов. Если есть взаимодействие факторов \u0026ndash; например, конверсионность выше у посетителей из Москвы, но только в том случае, если они заходят со смартфонов, а с десктопных устройств конверсионность наоборот ниже \u0026ndash; то линейная модель будет неспособна обнаружить такое взаимодействие. Теоретически можно добавить в модель синтетические факторы второго и более порядка, т.е. попарные сочетания всех факторов первого уровня, но тогда общее количество факторов станет огромным, что затруднит интерпретацию модели и усложнит обучение.\n Если имеются нелинейные зависимости между фактором и целевой переменной, или факторов друг c другом, линейная модель будет неспособна с этим работать (что очевидно даже из названия линейная). Сейчас это проблема не проявилась, потому что в модели участвуют только категорийные признаки, для работы с которыми достаточно линейной комбинации, но если добавить в модель, например, время от последней сессии или количество сессий, то нелинейность очень пригодится.\n  Учитывая недостатки линейной модели, мы сейчас не будем подробно рассматривать её работу на других сайтах, вместо этого поищем другое, более универсальное решение.\nВклады признаков в результат Долгое время существовал выбор из двух взаимоисключающих вариантов:\n Использовать простую, но при этом интерпретируемую модель (линейная модель и её разновидности, различные параметрические модели). В основном такие модели применяются в мире статистики, где традиционно требуется объяснение полученных результатов.\n Использовать сложную модель, которая может обучиться потенциально чему угодно (нейросеть, gradient boosting, SVM, etc), но при этом описать в доступных обычному человеку терминах, что происходит внутри модели, и как она принимает решения, будет нереально. Модель будет просто магическим чёрным ящиком, выдающим прогнозы. Такие модели применяются в мире машинного обучения, там где важен сам прогноз, а не объяснение, почему он получился именно таким.\n  Но существует альтернатива, позволяющая совместить и сложность модели и её интерпретируемость.\nЛюбые модели, используемые в машинном обучении, можно представить как функцию от признаков (features): $f(x_1, x_2, \u0026hellip; x_n)$. Каждому значению признака $x_i$ можно сопоставить его \u0026ldquo;вклад\u0026rdquo; $\\phi_i$. Вклад \u0026ndash; это то, насколько данное значение воздействует на результат работы модели, отклоняя его от среднестатистического значения. На примере нашей линейной модели: есть признак страна, его значение, отличное от Россия, отклоняет результат в отрицательную сторону (вероятность конверсии уменьшается). Для признака город значение Могилёв наоборот отклоняет результат в положительную сторону. Сумма вкладов всех признаков для конкретного примера (в нашем случае для посетителя) равна отклонению результата работы модели на этом конкретном примере от среднестатистического результата (в нашем случае от средней конверсионности по всему сайту):\n$$\\sum\\phi_i = f(x) - \\E[f(X)] $$ где $x = (x_1,\\dots, x_n)$ \u0026ndash; набор признаков одного посетителя, для которого рассчитываются $\\phi_i$, $X$ \u0026ndash; признаки всех посетителей, $\\E[\\cdot]$ \u0026ndash; матожидание.\nЕстественно, при сложении положительные и отрицательные вклады могут полностью или частично нейтрализовать друг друга. Например, вероятность конверсии посетителя из Литвы (country:Lithuania, вклад -1), который зашёл на сайт со страницы landing_l2:/proctuct/1H (вклад +1), будет примерно равна средней конверсионности по сайту, если не учитывать вклады остальных признаков.\nВ линейной модели вклады $\\phi_i$ получаются из весов $w_i$ и не зависят от конкретного посетителя, например вклад от признака city:Могилёв будет одинаковым для любого посетителя. Но в более сложных моделях вклад $\\phi_i$ может зависеть от взаимодействий признаков друг с другом, и рассчитывается индивидуально для каждого примера. Другим словами, вклад признака city:Могилёв может быть разным для разных посетителей из Могилёва, в зависимости от значений других признаков.\nДля линейной модели расчёт вкладов $\\phi_i$ очень простой, и мы его уже проделали выше. Но как рассчитываются вклады для сложных моделей?\nВектор Шепли Решение приходит с довольно неожиданной стороны, из теории игр, основные положения которой был разработаны в 50-х года прошлого века. Представим группу игроков, играющих в любую игру с денежным выигрышем, и кооперирующихся друг с другом (т.е. играющих не друг против друга, а в команде). Цель игроков - получение максимального выигрыша. Но вот выигрыш получен, как справедливо разделить его между игроками? У игроков может быть неодинаковый вклад в игру, один игрок опытный, и \u0026ldquo;тянул на себе\u0026rdquo; всю партию, другой \u0026ndash; новичок, который только учится играть, и польза от которого вообще сомнительна. Очевидно, нужен способ для справедливого расчёта персонального вклада каждого игрока.\n\u0026ldquo;Справедливость\u0026rdquo; можно выразить в более конкретных терминах:\n Сумма вознаграждений всех игроков равна общему выигрышу (Эффективность) Если два игрока принесли одинаковую пользу, они получают равное вознаграждение, независимо от того, в составе каких команд они играли. (Симметричность) Если игрок не принёс никакой пользы, его вознаграждение равно нулю. (Аксиома болвана) Если группа игроков играет несколько партий, сумма вознаграждения игрока равна сумме его вознаграждений в каждой партии. (Аддитивность)  Теория игр постулирует, что единственный способ расчёта, удовлетворяющий всем четырём условиям, это вектор Шепли.\n  Расчёт вектора Шепли можно представить через маржинальные вклады игроков, когда вклад игрока оценивается по росту выигрыша команды, к которой он присоединился:\n В начале вся команда состоит из игрока A. Выигрыш $7, значит и вклад игрока A равен 7. Когда к нему присоединяется игрок C, выигрыш команды растёт до $15. Значит вклад игрока C равен 8. Присоединяется игрок B, выигрыш растёт до $19. Вклад игрока B равен 4 единицам, и т.п.  Но это ещё не полноценный расчёт. Умения игроков могут пересекаться, например игрок B может обладать примерно теми же навыками, что и игрок А. В одиночку A выигрывает $7, а присоединение к нему игрока B не поднимает выигрыш, т.е. вклад игрока B получается нулевым, хотя B в одиночку выигрывает $4. Получается, что результат зависит от порядка добавления игроков. Чтобы убрать эту зависимость, надо учесть все возможные варианты, например для игрока С: C, A + С, AB + C, B + C и взять среднее значение вклада.\nНекоторые последовательности будут избыточными, например AB + C и BA + C эквивалентны с точки зрения оценки вклада С, поэтому вектор Шэпли рассчитывают не через последовательности, а на основе множеств, в которые может войти игрок, такие множества называются коалиции.\nРезюмируя всё сказанное выше, получаем определение: Вектор Шепли (Shapely values) - это распределение выигрышей, соответствующее среднему маржинальному вкладу каждого игрока во всех возможных коалициях с его участием.\nПолную формулу расчёта вектора Шэпли не привожу, она довольно громоздкая. Формула есть в Википедии, или в книге Interpretable Machine Learning есть подробная глава о применении shapely values для интерпретации моделей.\nSHAP Вернёмся к машинному обучению. Если рассматривать игру, как получение моделью результата на основе заданного примера, а выигрыш, как разницу между матожиданием результата на всех имеющихся примерах и результатом, полученном на заданном примере: $f(x) - \\E[f(X)]$ (выигрыш может быть и отрицательным), то вклады игроков в игру это не что иное, как вклад $\\phi_i$ каждого значения признака в \u0026ldquo;выигрыш\u0026rdquo;, т.е. насколько сильно это значение признака повлияло на результат. Концепция из теории игр оказывается вполне применимой в машинном обучении.\nПри расчёте вектора Шэпли необходимо формировать коалиции из ограниченного набора признаков, но далеко не каждая модель позволяет просто взять и убрать признак без необходимости заново обучаться с нуля. Потому на практике для формирования коалиций обычно не убирают \u0026ldquo;лишние\u0026rdquo; признаки, а заменяют их на случайные значения из \u0026ldquo;фонового\u0026rdquo; набора данных. Усреднённый результат модели со случайными значениями признака эквивалентен результату модели, в которой этот признак вообще отсутствует.\nПрямой расчёт вектора Шэпли возможен только для совсем маленьких моделей, т.к. число возможных сочетаний (коалиций) $n$ признаков это $2^n$. С увеличением количества признаков время расчёта растёт экспоненциально и быстро выходит за пределы разумного. В нашей простой модели всего 21 признак, $2^{21}$ это уже 2097152 возможных коалиций. А если учесть, что эти признаки категорийные, и для работы их надо перевести в бинарное кодирование, то получится около 1000 признаков. $2^{1000}$ коалиций это число, в котором 302 цифры (для сравнения, во всей Вселенной содержится всего лишь около $2^{265}$ атомов). По этой причине вектора Шепли долгое время не представляли практического интереса для машинного обучения.\nНо недавно были разработаны способы эффективного расчёта векторов Шэпли (библиотека SHAP - SHapely Additive Explanations), которые используют дополнительную информацию о структуре модели и в результате проводят вычисления за приемлемое время. Результаты, выдаваемые библиотекой, могут быть не совсем \u0026ldquo;честными\u0026rdquo; векторами Шэпли, а их аппроксимацией. Будем называть их SHAP values. Такие расчёты сейчас лучше всего проработаны для методов обучения, использующих tree ensembles, например Gradient Tree Boosting и нейросети. Попробуем рассчитать SHAP values для моделей, обученных с использованием библиотеки XGBoost.\nXGBoost + SHAP Новые модели обучены с помощью XGBoost на том же наборе признаков, что и линейные модели. В результате получился AUC несколько выше (на 1-2%), чем у логистической регрессии: XGBoost использует взаимодействие между признаками, которое было недоступно для линейных моделей. Возможно, результат может быть ещё лучше при более тщательной настройке гиперпараметров XGBoost.\nЧтобы увидеть факторы, воздействующие на конверсию, рассчитаем SHAP values для каждого примера из тестовой выборки. В отличие от линейной модели, где величина каждого фактора была константой, SHAP values индивидуальны для каждого примера из выборки, поэтому на выходе получается не единственное значение для каждого фактора, а распределение. Таким образом мы получаем ещё и приблизительную оценку уверенности в степени воздействия фактора.\nshop.ml Для визуализации распределений будем использовать box plot. Концы \u0026ldquo;усов\u0026rdquo; соответствуют минимальному и масксимальному значениям SHAP, закрашенный прямоугольник \u0026ndash; диапазону, в котором сконцентрирована основная масса значений. Оранжевая вертикальная линия внутри прямоугольника соответствует медианному значению.\n  Видно, что в целом результаты похожи на линейную регрессию: большое положительное влияние оказывают некоторые landing pages и рефереры, отрицательное влияние \u0026ndash; в основном страны, отличные от России. Город Могилёв, который был рекордсменом положительного влияния в линейной модели, тоже имеет положительное влияние, но не такое экстремальное: среднее всего +0.5, что даже не позволило ему войти в топ 20, поэтому его нет на диаграмме.\nФакторы, рассчитанные на основе SHAP values, более консервативны: большое положительное или отрицательное влияние приписывается фактору, только если модель видит достаточно большое количество подтверждающих примеров в выборке. Это хорошо для анализа воздействия на конверсию, т.к. значения конверсионности, рассчитанные на маленькой выборке, ненадёжны (см. Каким цифррам можно верить?).\nМожно проанализировать и факторы, относящиеся к одному и тому же признаку, например по метке UTMSource:  \nВ этому случае получается некоторый аналог отчёта Я.Метрики или Google Analytics, но дающий более внятную картину \u0026ldquo;что хорошо, а что плохо\u0026rdquo;. Для большинства меток нет выявленной закономерности по воздействию на конверсионность, поэтому их медиана находится в околонулевой зоне. В то же время метки, явно действующие \u0026ldquo;в плюс\u0026rdquo; или \u0026ldquo;в минус\u0026rdquo; чётко отделены от основной массы, это вместо каши из цифр, которая была бы в стандартном отчёте.\nluxshop.ml    В этом магазине очень сильное влияние у landing страниц, ассоциированных с корзиной. Похоже, что использовался retargeting: если посетитель оставлял товары в корзине и уходил без покупки, его \u0026ldquo;возвращали\u0026rdquo; в магазин. Теоретически, когда такой посетитель вернётся в магазин, это будет уже повторный визит, и он не должен попасть в выборку. Но отслеживание посетителей через cookies не на 100% точное, видимо посетитель успевал потерять или очистить cookies.\nЧтобы остальные факторы были более заметны, отобразим ту же диаграмму без топовых посадочных страниц:  \nМожно сделать выводы:\n Очень хорошо по сравнению со всем остальным выглядят прямые заходы (trafficSource:direct). Видимо у магазина работает оффлайн реклама, и/или имеется сильный бренд. Трафик из Москвы конвертируется намного лучше, чем из остальных городов. Что не удивительно, учитывая стоимость товаров. Заходы с лицевой страницы сайта landing_l1:/ повышают конверсию. То есть людям важнее бренд в целом, чем конкретный товар. Конверсия ощутимо зависит от типа устройства: desktop устройства device:desktop конвертируются лучше мобильных device:mobile. Возможно это связано с тем, что покупка достаточно дорогих товаров не происходит спонтанно (достал в метро смартфон и купил), нужно время на рассматривание и размышления.  Для наглядности, дополнительно выведем факторы только по типу устройства и только по городу:\n    \nСкоринг посетителей До этого момента мы рассматривали суммарную статистику по факторам для всей выборки посетителей. Но не менее интересно увидеть индивидуальные факторы для отдельных посетителей. В линейной модели это не имело смысла, так как факторы для всех одни и те же, но SHAP рассчитывает индивидуальные факторы на каждый пример (в нашем случае на каждого посетителя). Но сначала надо выяснить, в какой шкале вычисляются факторы. Например, фактор, дающий +1, это сколько к конверсии?\nИ линейная модель и большинство других моделей в машинном обучении не оперируют напрямую вероятностями, выраженными в процентах. Более удобная для вычислений шкала это log-odds. Если вероятность события это $p$, то odds, или шансы события, это $p/(1-p)$. В быту шансы записывают как $x:y$, где $x$ и $y$ соответствуют числителю и знаменателю в выражении, конвертирующем вероятность в шансы. Например, вероятность 50% это шансы 1:1 $$\\frac{0.5}{1-0.5} = \\frac{0.5}{0.5} = 1:1$$ вероятность 1% это шансы 1:99 $$\\frac{0.01}{1-0.01} = \\frac{1}{100-1} = 1:99 $$\nlog-odds это логарифм шансов, или логистическая функция, давшая название логистической регрессии, которую мы применяли, работая с линейной моделью:\n$$\\textit{log-odds}(p) \\equiv logit(p)=\\log\\left(\\frac{p}{1-p}\\right)$$\nВнутри моделей все вычисления происходят в logit шкале (она же log-odds). То есть вероятности 50% соответствует значение $\\log(0.5/(1-0.5)) = \\log(1) = 0$. Обычно используется натуральный логарифм, но мы будем использовать логарифм по основанию 2, с которым удобнее работать вручную. Тогда вероятности 1% соответствует значение $$\\log_2\\left(\\frac{1}{100-1}\\right) = \\log_2(1/99) = -6.63$$ а вероятности 99% значение $$\\log_2\\left(\\frac{99}{100-99}\\right) = \\log_2(99/1) = -6.63$$\nВекторы Шэпли рассчитываются в этой же шкале, фактор +1 сдвигает log-odds на одну единицу. Это означает, что шансы увеличиваются в два раза (т.к. используется основание 2). Например, если средняя конверсионность 1%, или 1:99, то фактор +1 даст конверсионность 2:99 или 1.98%. Для типичных значений конверсионности можно приблизительно считать изменение шансов в $n$ раз эквивалентным изменению конверсионности в $n$ раз ($1.98\\% \\approx 2 \\times 1\\%$), тогда:\n$$p_f = 2^{f}\\cdot\\E[p] $$ где $f$ \u0026ndash; значение фактора, $p_f$ \u0026ndash; конверсионность с учётом влияния фактора $f$, $\\E[p]$ \u0026ndash; средняя конверсионность. Фактор +1 увеличивает конверсионность в два раза, фактор -1 соответственно уменьшает в два раза, и т.п.\nТеперь собственно скоринг. Будем отображать scorecard каждого посетителя в виде столбца, у которого есть:\n Базовое значение (жирная черная горизонтальная линия). Позиция базового значения по шкале Y соответствует тому, насколько сдвинута по мнению модели вероятность конверсии посетителя относительно матожидания конверсии по всем посетителям, т.е \u0026ldquo;средней\u0026rdquo; конверсионности. Положительные факторы (сегменты красного цвета над базовым значением). Высота сегмента соответствует вкладу фактора. Чтобы не загромождать диаграмму, отображаются только факторы, имеющие вклад больше 0.02. Отрицательные факторы (сегменты синего цвета под базовым значением).    Сумма положительных и отрицательных факторов равна базовому значению. Шкала по оси Y \u0026ndash; log-odds. Нулевой уровень шкалы (отмечен пунктиром) соответствует средней конверсии.\n Посетитель A имеет самые высокие шансы сконвертироваться, его базовый уровень $+3.3$, что соответствует увеличению конверсии в $2^{3.3} \\approx 9.85$ раз, т.е. почти десятикратное увеличение. Это результат удачного сочетания факторов: прямой заход, Москва, стационарный компьютер. Посетитель С наоборот практически лишён положительных факторов, всё работает против него. Его базовый уровень $-1.3$, это уменьшение конверсионности в $2^{1.3} \\approx 2.46$ раза.\n Посетители B и D имеют промежуточную конверсионность, близкую к средней по сайту.  Scorecards позволяют наглядно увидеть, что модель \u0026ldquo;думает\u0026rdquo; о любом посетителе, а также сравнивать посетителей друг с другом. Они могут быть быть например хорошим дополнением к информации о посетителе в Я.Метрике. В простой модели, которой мы сейчас пользуемся, scorecard носит скорее разъяснительный характер, т.к. после завершения первого визита всё равно известно, сконвертировался посетитель или нет. Но потом мы рассмотрим и более сложные модели, которые предсказывают поведение посетителя в будущем, и там scorecard станет важным рабочим инструментом.\ncourses.ml Этот сайт не имеет \u0026ldquo;конверсии\u0026rdquo; в традиционном понимании, его цель \u0026ndash; сделать так, чтобы посетитель заинтересовался, остался и начал проходить курсы. Поэтому вместо конверсии будем использовать вероятность того, что после первого посещения посетитель вернётся на сайт в течение месяца.\nСайт привлекает посетителей в основном через размещение ссылок на сайтах подходящей тематики, а не через традиционную онлайн рекламу, поэтому расклад факторов, влияющих на конверсию, здесь сильно отличается от магазинов:\n  Видно сильное отрицательное влияние служебных доменов: модель поняла, что посетители, заходящие с этих доменов, ведут себя совсем не так, как остальные. Чтобы они не мешали анализу, исключим их и построим диаграмму ещё раз:\n  В топе положительных факторов переходы с coursera.org, т.к. там есть аудитория, уже заинтересованная в обучении, а также домен партнёра partner.courses.ml, это white label сервис на отдельном домене. Также в топе много переходов с почтовых сервисов, т.е. хорошо работает привлечение (или возврат) посетителей через рассылки.\nВ топе отрицательных факторов:\n landing_l1:/unsubscribe/ \u0026ndash; отписка от рассылки (неудивительно) browser:— \u0026ndash; посетители, у которых не определился браузер, вероятно боты. urlDomain:(other) \u0026ndash; низкочастотные доменные имена, попавшие в группу \u0026ldquo;остальное\u0026rdquo;, видимо это тоже служебные домены. referer:lady.mail.ru \u0026ndash; переходы с http://lady.mail.ru. На сайте courses.ml курсы в основном IT тематики, и к сожалению нет курсов \u0026ldquo;как найти богатого мужа\u0026rdquo; или \u0026ldquo;101 кулинарный шедевр с майонезом\u0026rdquo;. Поэтому интерес со стороны аудитории lady.mail.ru весьма низок.  Scorecards:  \nПосетитель A демонстрирует повышение вероятности повторного визита в $2^{3.5} \\approx 11.3$ раза по отношению к среднему уровню, посетитель B \u0026ndash; понижение в $2^{4.2} \\approx 18.4$ раз, у посетителей D и C положительные и отрицательные факторы уравновешивают друг друга и получаются вероятности, близкие к средним.\nРезюме Мы научились применять модели машинного обучения для анализа факторов, влияющих на конверсию. Благодаря применению векторов Шэпли интерпретация модели любой сложности сводится к линейной комбинации факторов, которая легко воспринимаются человеком и легко отображается графически (scorecards).\nПока рассматривались только простые модели, использующие исключительно параметры первого перехода на сайт. Более интересны модели, работающие с информацией о поведении посетителя на сайте, они позволяют точнее предсказывать дальнейшие действия посетителя. Такие продвинутые модели мы рассмотрим в следующих статьях (1, 2, 3).\nНо даже такие довольно простые выводы, которые можно получить на основе рассмотренных моделей, уже облегчают работу с Интернет-аналитикой. Вместо копания в десятках отчётов можно сразу получить готовые ответы \u0026ndash; это и есть результат, к которому надо стремиться.\n","date":1550174400,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1550174400,"objectID":"e251e0bbd9dbd79481b69ed09371b3bf","permalink":"https://suilin.ru/post/conversion_factors/","publishdate":"2019-02-15T00:00:00+04:00","relpermalink":"/post/conversion_factors/","section":"post","summary":"$$\\DeclareMathOperator{\\E}{E}$$ Как понять, что влияет на конверсию положительно, а что отрицательно? В любом отчёте Google Analytics или Я.Метрики есть столбец \u0026ldquo;конверсия\u0026rdquo;, и сравнив конверсию в разных строках отчёта, казалось бы, можно получить исчерпывающий ответ.\nНо не надо забывать, что любой отчёт показывает срез данных, сделанный по единственному измерению (источник трафика, география, и т.п.), что даёт неполную, а иногда даже неверную картину. Рассмотрим простой пример: есть два города, Москва и Екатеринбург и два вида трафика, органический и реферальный.","tags":["Conversion"],"title":"Конверсия и data science III. Как отличить хорошее от плохого?","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" При управлении онлайновыми рекламными кампаниями при подключении новых источников (объявлений, баннеров, SMM и т.п.) часто приходится решать проблему:\n С одной стороны, разумно подождать, пока источник не выдаст побольше переходов, посмотреть на конверсии, и тогда принимать решение, оставить его в рекламной кампании или отключить. Но если долго ждать, тогда рекламный бюджет будет зря расходоваться на неэффективные источники. С другой стороны, если сократить ожидание, можно случайно отключить источник, который на самом деле конверсионный, и наоборот, оставить неэффективный источник, случайно показавший высокую конверсию.  Очевидно, нужно найти некий баланс: с одной стороны, дать источникам достаточно времени, чтобы проявить себя, с другой стороны вовремя отключать неэффективных.\nПоиском такого баланса мы и займемся. Для этого будем тестировать эффективность стратегий управления источниками на модели, имитирующей поступление трафика и конверсии на реальном сайте.\nМодель Мы будем моделировать рекламную кампанию, которая длится $T$ дней. Средний объем трафика, приходящего на сайт \u0026ndash; $V$ визитов в день, этот трафик распределяется между $N$ источниками. У каждого источника есть вес, $w_{i,t}$ обозначающий долю всего трафика, которая приходится на источник в день $t$:\n$$\\sum_{i=1}^N w_{i,t} = 1, \\; t \\in 1 \\dots T$$\nНа старте у источников одинаковые равные веса $w_{i, 1}=\\frac{1}{N}$. Затем каждый день стратегия меняет вес источников на своё усмотрение.\nВес $w_{i,t}$ может быть нулевым, это означает, что источник отключен.\nКаждый источник в соответствии со своим весом генерирует количество визитов с матожиданием $\\lambda_{i,t}$ (интенсивность визитов): $$\\lambda_{i,t} = w_{i,t} V$$\nНо это среднее количество визитов, а реальное их количество моделируется как процесс Пуассона с интенсивностью $\\lambda_{i,t}$. Тогда количество визитов $v_{i,t}$, которое выдаст источник в каждый конкретный день $t$, сэмплируется из распределения Пуассона: $$v_{i,t} \\sim \\mathrm{Pois}(\\lambda_{i,t})$$ Таким образом моделируется ситуация из реальной жизни: далеко не всегда можно управлять точным количеством переходов на сайт из источника.\nКаждый источник имеет латентную конверсионность $\\theta_i$ (т.е. это истинная конверсионность источника, которая проявится на большом количестве трафика). Распределение конверсионностей источников смоделируем логнормальным распределением: $$\\theta_i \\sim \\exp\\left(\\mathcal{N}(\\mu, \\sigma^2)\\right)$$ Для наших экспериментов примем, что медианная конверсионность источников 1%: $\\mu = \\ln(0.01)$ и $\\sigma = 0.5$. Получится распределение, похожее на то, что встречается на реальных сайтах:  \nЛогнормальное распределение несимметрично (длинный хвост справа), поэтому для него матожидание (т.е. средняя конверсионность) и медиана будут немного отличаться: $$\\mathrm{E}=\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\\\\n\\mathrm{Median}=\\exp(\\mu)$$ В используемом нами распределении средняя конверсионность будет $\\approx 1.13\\%$\n Латентная конверсионность источника $\\theta_i$ недоступна стратегии, доступна только реализация конверсионности $r_{i,t}$: $$c_{i,t} \\sim \\mathrm{Bin}(v_{i,t}, \\theta_i) \\\\\nr_{i,t} = \\frac{\\sum_{j=1}^t c_{i,j}}{\\sum_{j=1}^t v_{i,j}}$$ т.е. количество конверсий у источника в конкретный день $c_{i,t}$ сэмплируется из биномиального распределения, а параметры биномиального распределения, в свою очередь, тоже сэмплируются из своих распределений, описанных выше. Сэмплирование количества визитов $v_{i,t}$ происходит каждый день, сэмплирование конверсионностей $\\theta_i$ один раз перед стартом каждого эксперимента.\n  Принципиальная схема модели   Параметры по умолчанию Сведём вместе все значения параметров по умолчанию:\n $N=100$. В каждом эксперименте участвует 100 источников, из которых будем отбирать лучших. $T=90$. Рекламная кампания (т.е. один эксперимент) длится 90 дней. $V=100$. Каждый день между источниками распределяется 100 визитов. При таком объеме трафика на каждый источник приходится в среднем всего один визит в день, и за всё время эксперимента произойдет в среднем одна конверсия на источник! Очевидно, что выявление лучших источников при столь малом объеме трафика это нетривиальная задача. Для некоторых экспериментов будем использовать менее экстремальное значение $V=1000$ Конверсионность источников берётся из логнормального распределения с параметрами $\\mu=\\ln(0.01), \\sigma=0.5$, что соответствует средней конверсионности $\\approx 1.13\\%$. Результаты каждого эксперимента будут отличаться друг от друга из за использования стохастических переменных, поэтому итоговый результат будем рассчитывать, как среднее по 2000 повторений эксперимента.  Цель стратегий Стратегия в конце каждого дня (момент времени $t$) смотрит на накопленные результаты работы источников (кол-во визитов, кол-во конверсий и вычисленная на их основе конверсионность) и принимает решение, какие веса дать источникам на следующий день, т.е. на момент времени $t+1$.\nЦель стратегии \u0026ndash; получить максимальное кол-во конверсий за время рекламной кампании, т.е. дать максимальный вес источникам с высокой конверсионностью и минимальный вес всем остальным. В реальной жизни ёмкость источников ограничена, поэтому существует дополнительное условие: $w_{i} \\leq w_i^{max}$\nКаким должен быть максимальный вес $w_i^{max}$? Обычно источники с высокой конверсионностью имеют меньшую ёмкость, так как по сути являются узкими (и часто дорогими) сегментами аудитории. Чтобы отразить это в модели, сделаем максимальный вес источника обратно пропорциональным его конверсионности: $$w_{i}^{max} = \\frac{k}{\\theta_i} $$ где $\\theta_i$ это латентная конверсионность, $k$ \u0026ndash; коэффициент, регулирующий среднюю ёмкость источников. Примем $k$ равным матожиданию конверсионности: $$k=\\mathrm{E}(\\theta)$$ Тогда источник со \u0026ldquo;средней\u0026rdquo; конверсионностью будет иметь максимальный вес, равный единице. Источник \u0026ldquo;хуже среднего\u0026rdquo; сможет быть единственным активным источником для сайта, а источник \u0026ldquo;лучше среднего\u0026rdquo; \u0026ndash; не сможет. Если конверсионность источника в 2 раза выше средней, он сможет иметь максимум 50% трафика, если в 4 раза выше средней \u0026ndash; 25% трафика, и т.п.\nВ идеале к концу эксперимента должен остаться активным только топ лучших источников. При используемом распределении конверсионностей в идеальный топ будет входить в 80% случаев 3 источника и в 20% случаев 4 источника.\nОценка стратегий Эффективность стратегий оценивается по улучшению конверсионности по сравнению с baseline (0%) и идеальным вариантом (100%). За baseline принимается средняя конверсионность, которая будет, если вообще ничего не делать и оставить все источники в равных долях, как они были на старте. За идеальный вариант принимается максимальная конверсионность, которая была бы, если с первого же дня оставить только лучшие источники и отключить все остальные.\nОбщее улучшение \u0026ndash; это то, как полученные результаты соотносятся с baseline и максимумом: $$improvement=\\frac{result-baseline}{maximum-baseline} \\times 100\\%$$ Улучшение рассчитывается для каждого дня отдельно, затем результат усредняется. Если не получилось ничего улучшить по сравнению с baseline, улучшение будет 0%, если наоборот с первого дня удалось достичь максимальной возможной конверсионности, улучшение будет 100%. Реальное улучшение обычно будет между 0% и 100% (но может быть и отрицательным, если результатом стратегии стало ухудшение конверсионности вместо улучшения).\nТакже интересно финальное улучшение, это то, где между baseline и максимумом был результат в последний день рекламной кампании.\nНаивная стратегия Начнём с самого простого, и протестируем наивную стратегию, имитирующую традиционный способ управления источниками:\n Ждём, пока все источники не увидят в среднем $M$ визитов Оставляем лучшие источники, отключаем все остальные.  Каким должно быть $M$, т.е. сколько надо ждать \u0026ndash; неизвестно, поэтому попробуем разные значения:\n  Видно, что при любых $M$ улучшение не поднимается выше 17%. Если ждать недолго, то не успеем накопить достоверную информацию о конверсионности, а если ждать дольше, то неэффективные источники отключатся только в конце рекламной кампании. Естественно, при длительном ожидании финальное улучшение будет высоким, но для всей рекламной кампании важно именно общее улучшение.\nПосмотрим на динамику конверсионности и распределение улучшений при оптимальном $M=38$:  \nНа левой диаграмме пунктирными линиями представлены baseline и максимальная возможная конверсионность. График реальной конверсионности, являющийся результатом работы стратегии \u0026ndash; голубая линия с верхней и нижней границами, соответствующими доверительному интервалу 95%. Общее улучшение соответствует площади под этим графиком по отношению ко всей площади между зеленой и оранжевой пунктирными линиями. Общее/финальное улучшения отображены в заголовке диаграммы.\nНа правой диаграмме видно, что в части экспериментов улучшение было отрицательным, т.е. ожидания в 38 дней явно недостаточно для получения достоверной информации о конверсионности.\nМногорукие бандиты Проблему, которую решают наши стратегии, можно сформулировать так:\n Найти баланс между а) исследованием конверсионности источников (exploration), и б) использованием найденной конверсионности для оптимизации (exploitation), таким образом, чтобы максимизировать выигрыш (в нашем случае количество конверсий).\n Эта проблема известна в более широком смысле, как проблема многорукого бандита ( multi-armed bandit, MAB). Представим набор игральных автоматов (слот-машин, также называемых однорукими бандитами), каждый из которых генерирует единичный выигрыш с вероятностью $\\theta_i$, неизвестной игроку. Задача игрока \u0026ndash; за конечное время получить максимальный выигрыш, т.е. поиграв на каждом бандите, приблизительно определить его $\\theta_i$ (exploration phase), и по результатам играть только на \u0026ldquo;прибыльных\u0026rdquo; бандитах (exploitation phase).\n  В классической постановке MAB-проблемы игрок за один шаг взаимодействует (arm pull) только с одним бандитом, и на основе полученного результата принимает решение о выборе бандита для следующего шага. Такая постановка хороша для проведения A/B тестов, но совершенно не подходит для управления источниками: представьте, если бы мы запускали трафик на сайт по одному посетителю, и на основе того, сконвертировался он или нет, решали, из какого источника должен придти следующий посетитель. Выглядит малореалистично, не так ли?\nВ нашем случае:\n Игрок не имеет прямого контроля над тем, какой конкретный бандит будет использоваться на каждом шаге, он может управлять только вероятностью игры на каждом бандите (вероятности это веса источников $w_i$). В каждом раунде происходит множество игр, в MAB терминологии это называется *batch arm pulls*[1]. Текущая \u0026ldquo;прибыльность\u0026rdquo; бандитов вычисляется после окончания всего раунда. В нашем случае один раунд это один день.  Вообще MAB-стратегии очень хорошо изучены, но именно в классическом варианте проблемы. Проблему в нашей постановке, как это ни удивительно, практически никто не исследовал. Но ничто не мешает восполнить этот пробел и провести исследования самостоятельно. Мы адаптируем несколько известных MAB-стратегий к нашей задаче и посмотрим на результаты.\nSuccessiveHalving Очень простая стратегия, не требующая сложных вычислений, и отлично подходящая для \u0026ldquo;ручного\u0026rdquo; применения. Впервые описана в [2] под названием SequentalHalving. Алгоритм:\n Так же как и в наивной стратегии, выбираем порог ожидания $M$ визитов Когда порог достигнут, отключаем половину источников, которые показали худшую конверсию, и удваиваем порог. Повторяем шаг 2 до тех пор, пока не останется минимально возможное количество источников (в нашем случае 3 или 4)  Смысл стратегии в том, что по мере отключения явно плохих источников, у нас остаётся больше ресурсов, чтобы исследовать потенциально хорошие источники.\n   Результаты SuccessiveHalving при оптимальном $M=11$.   SuccessiveHalving демонстрирует результат примерно в два раза лучше по сравнению с наивной стратегией (улучшение 33.7% против 16.4%). Ухудшение вместо улучшения теперь почти не происходит, а финальное улучшение в отдельных случаях достигает 100%, т.е. стратегии удаётся выйти на оптимальный набор источников. Ступеньки на графике конверсионности образуются после каждого \u0026ldquo;уполовинивания\u0026rdquo; источников.\n   Для этой и следующих стратегий будем визуализировать также динамику изменения весов источников во времени, для трёх случайно выбранных экспериментов. Видно, что сначала веса распределены поровну (exploration phase), затем плохие источники постепенно отключаются, и их вес переходит к перспективным источникам. В районе 40-го дня отключаются все плохие источники, и происходит переход к чистому exploitation.\n$\\varepsilon$-decreasing Основная идея $\\varepsilon$-стратегий это явное разделение ресурсов, отводимых на exploration и exploitation. Выбирается число $0\u0026lt;\\varepsilon\u0026lt;1$, на exploration отводится доля ресурсов, равная $\\varepsilon$, а на exploitation доля $1-\\varepsilon$. Наивная стратегия тоже является $\\varepsilon$-стратегией, в терминологии MAB она называется $\\varepsilon$-first, т.к. сначала происходит 100% exploration (до момента времени $t_M$, когда накопится $M$ визитов), а затем 100% exploitation (до последнего дня $T$): $$\\varepsilon=\\frac{t_M}{T}$$\nExploration и exploitation могут быть совмещены во времени, т.е. доля трафика $1-\\varepsilon$ отводится \u0026ldquo;хорошим\u0026rdquo; источникам, а остальной трафик распределяется между всеми другими \u0026ndash; такая стратегия будет называться $\\varepsilon$-greedy. Более оптимальны $\\varepsilon$-decreasing стратегии[3],[4], в которых $\\varepsilon$ изменяется во времени: сначала больше ресурсов отводится на exploration, но постепенно $\\varepsilon$ уменьшается почти до нуля: $$\\varepsilon= \\min\\left(1, \\frac{\\varepsilon_0}{t}\\right), \\; t \\in 1,\\dotsc,T$$ где $\\varepsilon_0$ это базовое значение $\\varepsilon$ (может быть больше единицы). Для нашей модели оптимум $\\varepsilon_0=3.5$:   Результаты заметно улучшились по сравнению с SuccessiveHalving (38.2% против 33.7%). Это связано с тем, что $\\varepsilon$-decreasing стратегия начинает exploitation уже на третий день и изменяет веса источников ежедневно, а не скачками.   На диаграммах с весами заметно характерное поведение: веса \u0026ldquo;фоновых\u0026rdquo; источников плавно уменьшаются (градиент от фиолетового к чёрному), в то же время стратегия пытается выявить лучшие источники, активно переключаясь между кандидатами в течение первого месяца.\nSoftmax В предыдущих стратегиях трафик распределялся поровну между лучшими источниками (а также между активными \u0026ldquo;худшими\u0026rdquo;). Но почему бы распределять трафик не поровну, а в соответствии с \u0026ldquo;качеством\u0026rdquo; источника, т.е. его конверсионностью? Эту идею воплощает softmax стратегия: $$w_i=\\frac{e^{r_i/\\tau}}{\\sum_i e^{r_i/\\tau}}$$ где $r_i$ это наблюдаемая конверсионность источника, $w_i$ \u0026ndash; вес источника. Правая часть формулы представляет собой softmax-функцию, аналогичную распределению Гиббса-Больцмана, поэтому компонент $\\tau$ называют температурой. Конечно, никакого отношения к статистической физике это стратегия не имеет, использование softmax функции это просто удобный эмпирический способ выразить концепцию \u0026ldquo;источник получает долю трафика, соответствующую его качеству\u0026rdquo;. Идея такого использования softmax предложена в [5], softmax-стратегия в применении к MAB проблемам проанализирована в [3].\n\u0026ldquo;Температура\u0026rdquo; обычно принимается обратно пропорциональной времени: $$\\tau=\\frac{\\tau_0}{t}$$   Результаты softmax стратегии лучше, чем $\\varepsilon$-decreasing (39.2% против 38.2%), т.е. неравномерное распределение весов это работоспособная идея. Видно, что стратегия пытается начинать активный exploration с первого же дня, но часто принимает ошибочные решения и в следующие дни конверсия немного падает.   Стратегия очень оптимистична по поводу новых кандидатов: при появлении нового \u0026ldquo;фаворита\u0026rdquo; почти весь вес сразу переносится на него, но в следующие дни оптимизм уменьшается.\nБайесовские стратегии Всем стратегиям, которые мы уже протестировали, присущ один недостаток: они сравнивают источники друг с другом только по их наблюдаемой конверсионности. Такое сравнение предполагает, что наблюдаемая конверсионность содержит одинаковое количество информации для всех источников, т.е. они стартуют в один и тот же момент времени, выдают сопоставимое количество трафика, и рекламная кампания завершается одномоментно, в заранее известный день.\nВ самом деле, нет смысла сравнивать конверсионность источника, который только что стартовал (она скорее всего будет нулевой) с конверсионностью источника, через который прошло уже несколько тысяч визитов \u0026ndash; такое сравнение сразу забракует \u0026ldquo;молодой\u0026rdquo; источник и стратегия будет работать неправильно.\nНо условие \u0026ldquo;одновременный старт, сопоставимый трафик, одновременное завершение\u0026rdquo; далеко не всегда выполнимо. Рекламные кампании могут включать в себя \u0026ldquo;старые\u0026rdquo; источники, к ним в любой момент могут быть добавлены новые. Также постоянно происходит ротация источников, неэффективные выводятся из кампании, вместо них добавляются свежие. Момент завершения рекламной кампании малопредсказуем и зависит от её успешности, финансовых возможностей рекламодателя, текущих цен на платный трафик, и т.д. Поэтому уже рассмотренные стратегии будут хорошо работать только в тщательно контролируемых условиях. Для промышленного применения нужны anytime-стратегии, способные адекватно учитывать уже накопленную в источниках информацию и стартовать в любой момент, не привязываясь ко времени.\nОчевидно, вместо наблюдаемой конверсионности такие стратегии должны использовать количество визитов и количество конверсий. И тут мы возвращаемся к байесовским методам работы с конверсией, описанным в предыдущей статье.\nCredible Bounds Racing Вернёмся немного назад и вспомним картинку, на которой у источников есть \u0026ldquo;усы\u0026rdquo;. Каждый ус обозначает верхнюю и нижнюю границы credible interval, например для интервала 90% нижняя граница интерпретируется как \u0026ldquo;вероятность 5%, что конверсионность будет меньше этой границы\u0026rdquo;, верхняя соответственно \u0026ldquo;вероятность 5%, что конверсионность окажется больше\u0026rdquo;.   Эти границы \u0026ndash; всё, что нужно для работы простейшей байесовской стратегии. Алгоритм:\n Смотрим, есть ли источники, у которых верхняя граница интервала меньше, чем нижняя граница любого другого источника (т.е. \u0026ldquo;усы\u0026rdquo; не перекрываются). На приведённой выше диаграмме это источники A и B. Если такие источники-аутсайдеры есть, значит они явно хуже одного из существующих источников, и их можно отключить. На следующий день повторяем всё с пункта 1. Если по новым данным видно, что источник был отключен зря (т.е. его верхняя граница опять перекрывается со всеми остальными), включаем его обратно.  При такой стратегии в конце концов останется один источник, у которого не хватит трафика на весь сайт. Поэтому вводится дополнительное условие на отключение: отключаемый источник не должен входить в топ \u0026ldquo;самых перспективных\u0026rdquo;. Топ рассчитывается так: сортируем источники по верхней границе credible interval (т.е. какую конверсионность они могут показать), и отбираем их в топ, начиная от самого перспективного, пока не наберется общий максимальный вес больше единицы, т.е. пока топ не будет способен сформировать весь трафик для сайта.\nТакой же алгоритм используется при ручном управлении источниками на основе диаграммы с \u0026ldquo;усами\u0026rdquo;. Собственно, приведённое описание алгоритма это всего лишь формализация методики ручного управления.\nОсталось только понять, какой должна быть оптимальная ширина credible interval, т.к. при традиционной ширине 90% все \u0026ldquo;усы\u0026rdquo; будут перекрываться до последнего дня рекламной кампании \u0026ndash; у нас слишком мало визитов. Выясним оптимум экспериментальным путём:   Оптимальный квантиль 0.34, что соответствует ширине интервала всего 32%.   Результат стратегии чуть хуже (на 0.2%), чем у предыдущего чемпиона (softmax). Но при этом, как уже говорилось, эта стратегия более применима для управления реальными источниками.   Видно, что стратегия пытается отобрать оптимальные источники уже на второй день, но потом понимает, что поторопилась, и продолжает exploration. Если сделать ширину credible interval немного больше, эти артефакты исчезнут.\nПосмотрим, каких результатов можно добиться (и при какой ширине правдоподобного интервала) в других ситуациях:\n Более продолжительная рекламная кампания. Больший объем трафика.    Получились похожие диаграммы. Если пересчитать продолжительность рекламной кампании на левой диаграмме в эффективное количество визитов на источник за всё время кампании, получим копию правой диаграммы. Т.е. результаты стратегии и оптимальный credible interval зависят только от среднего количества визитов на источник. Если это количество увеличить до 900 (90 дней x 1000 визитов в день / 100 источников), можно получить улучшение \u0026gt;70%, а при увеличении до 9000 улучшение превышает 90%, т.е. близко к максимально возможной эффективности.\nProbability matching Если у нас есть все апостериорные распределения, почему бы не ответить напрямую на вопрос, который нас на самом деле интересует? Нет, не о смысле жизни, а более простой:\n Какова вероятность того, что $i$-ый источник имеет латентную конверсионность выше, чем все остальные?\n Если мы знаем эту вероятность, то логично установить веса источников пропорциональными ей, и это будет оптимальным решением нашей задачи! В самом деле, если вероятность нулевая, то источнику надо дать нулевой вес, если вероятность 100%, то весь вес надо перенести на этот лучший источник, если вероятности для двух источников 50:50, надо дать им одинаковые веса по 50% трафика, и т.п. Сопоставление вероятности оптимальности каждой \u0026ldquo;руке бандита\u0026rdquo; или в нашем случае источнику, называется Probability Matching.[6] $$w_i=\\Pr(\\theta_i=\\max\\{\\theta_1,\\dotsc,\\theta_N\\})$$ где $\\theta_i$ это латентная конверсионность $i$-го источника. Это выражение можно представить, как матожидание индикаторной функции: $$\\mathbb{I}_i(\\theta)=\\begin{cases} 1 \u0026amp;\\text{if } \\theta_i=\\max\\{\\theta_1,\\dotsc,\\theta_N\\}, \\\\\n0 \u0026amp;\\text{otherwise } \\end{cases}$$ $$w_i=\\mathrm{E}\\left(\\mathbb{I}_i(\\theta)\\right)=\\int \\mathbb{I}_i(\\theta)p(\\theta)\\mathrm{d}\\theta$$ Апостериорное распределение переменной $\\theta$ мы вычисляем через сопряжённое бета распределение: $$p(\\theta_i)=\\mathrm{Be}(\\theta_i|\\alpha_i,\\beta_i) \\\\\n\\alpha_i=\\alpha_{prior} + S_i \\\\\n\\beta_i=\\beta_{prior} + N_i - S_i$$ где $S_i$ это количество наблюдаемых успехов (конверсий), $N_i$ - количество наблюдений, т.е. визитов у $i$-го источника.\nВероятность того, что значение $\\theta_j$ окажется меньше $\\theta_i$, задаётся через кумулятивную функцию бета распределения: $$\\Pr(\\theta_j \u0026lt; \\theta_i) = \\mathrm{Be_{CDF}}(\\theta_i|\\alpha_j,\\beta_j)$$ Скомбинировав всё вместе, получаем: $$w_i=\\int_0^1 \\mathrm{Be}(\\theta_i|\\alpha_i,\\beta_i)\\prod_{i \\ne j} \\mathrm{Be_{CDF}}(\\theta_i|\\alpha_j,\\beta_j) \\mathrm{d}\\theta_i$$ Интеграл легко рассчитывается числовыми методами:\nimport numpy as np from scipy.stats import beta from scipy.integrate import quad def opt_prob(α, β): def integrand(θ, i): logp = beta.logpdf(θ, α[i], β[i]) logcdf = beta.logcdf(θ, α, β) logcdf[i] = 0 return np.exp(logcdf.sum() + logp) return [quad(lambda θ: integrand(θ, i), 0, 1)[0] for i in range(len(α))]  Числовое интегрирование работает достаточно медленно, альтернативный и более универсальный способ расчёта это сэмплирование из апостериорного распределения. Согласно закону больших чисел, при большом количестве сэмплов среднее значение выборки сойдется к матожиданию: $$w_i=\\lim\\limits_{M \\to \\infty}\\frac{1}{M} \\sum_{m=1}^M \\mathbb{I}_i(\\theta_m) $$ Т.е. надо взять $M$ сэмплов, и для каждого источника вычислить, в каком проценте сэмплов его значение конверсионности оказалось максимальным. На практике достаточно 1K\u0026ndash;8K сэмплов, для ускорения можно проводить сэмплирование и часть расчёта на GPU.\n  Расчет через сэмплирование для классической MAB-задачи (когда не нужны веса, а надо просто выбрать следующего бандита) известен, как Thompson sampling.[7] В этом случае достаточно единственного сэмпла, и для следующего шага выбирается бандит, у которого в сэмпле оказалось максимальное значение вероятности. Thompson sampling был предложен ещё в 1933 году, но тогда вычисления были слишком дорогими, метод был забыт, и о нём вспомнили только в конце XX века. Доказательство его оптимальности было получено в 1997 году.\nProbability matching \u0026ndash; оптимальная стратегия, всегда сходящаяся к выбору лучших источников. Это схождение происходит достаточно быстро по меркам MAB-стратегий, но слишком медленно для нас: probability matching не сделает нулевым вес источника, пока не убедится в том, что он абсолютно безнадёжен. Но чтобы убедиться в этом, требуется гораздо больше визитов, чем имеющиеся у нас 90 на источник.\nВместо выбора гарантированно лучших источников в отдалённом будущем, нам нужен возможно не самый оптимальный выбор просто хороших источников в течение первых недель работы стратегии. Чтобы сдвинуть баланс в сторону exploitation, добавим новый параметр $\\rho$, который будем называть жадностью: $$w\u0026rsquo;_i = w_i^\\rho, \\; \\rho \\geq 1 $$ где $w\u0026rsquo;_i$ \u0026ndash; эффективный вес $i$-го источника, который будет использовать стратегия, $w_i$ \u0026ndash; теоретически оптимальный вес, рассчитанный через probability matching, $\\rho$ \u0026ndash; степень, в которую возводится теоретический вес. Чем больше $\\rho$, тем больше ресурсов переходит к уже известным \u0026ldquo;хорошим\u0026rdquo; источникам и меньше ресурсов остаётся на exploration. Оптимальная жадность для стандартных условий эксперимента: $\\rho\\approx7$. Посмотрим на результаты:\n   У нас новый чемпион! Улучшение 41.1% это почти на 2 единицы больше, чем предыдущий рекорд 39.2%. Probability matching действительно оптимальная стратегия.\n   Если стратегия не уверена в своём выборе, она не прекращает exploration до самого последнего момента, в надежде добраться до истины. Если выбор очевиден (попались явно хорошие источники), вес переходит к ним. Exploration выглядит несколько хаотичным из за шума, вносимого случайным сэмплированием, но на результаты этот шум не влияет.\nПосмотрим, какие результаты покажет probability matching на других объемах визитов:   Результаты превосходны. Улучшение \u0026gt;80% достигается сразу после планки в 1000 визитов (т.е. 10 визитов в день на источник), а после 5000 визитов доступно улучшение \u0026gt;90%!\nЕщё очень важная деталь: эффективность стратегии слабо зависит от выбора гиперпараметра $\\rho$. В принципе на всём диапазоне протестированных объемов одинаковое значение $\\rho=3$ показало бы неплохие результаты. Для сравнения можно посмотреть на аналогичные диаграммы для Credible Bounds Racing: там правильный выбор ширины интервала играет решающую роль.\nНеточное априорное распределение Кроме $\\rho$ есть еще один неявный гиперпараметр \u0026ndash; априорное распределение конверсионностей. Для тестирования стратегий я просто сгенерировал выборку 100 сэмплов из используемого моделью логнормального распределения и с помощью MLE вычислил параметры соответствующего бета распределения. Понятно, что в реальной жизни сэмплы из распределения латентных конверсионностей взять негде, разве что попросить их у Господа, поэтому придётся использовать наблюдаемую конверсионность, что негативно скажется на точности. Как неточное определение параметров априорного распределения повлияет на результат? Давайте проверим.  \nДля практических расчётов бета распределение удобно параметризовать не через $\\alpha$ и $\\beta$, а через конверсионность $r$ (играет роль матожидания) и эффективное количество визитов $v$ (играет роль дисперсии): $$\\alpha = rv \\\\\n\\beta = (1-r)v$$ \u0026ldquo;Правильные\u0026rdquo; значения $\\hat{r}$ и $\\hat{v}$, полученные через MLE, отображены на диаграмме зелёным пунктиром. Точка пересечения пунктирных линий \u0026ndash; параметры априорного распределения, которые использовались во всех предыдущих экспериментах. Чтобы определить устойчивость стратегии к ошибкам в априорном распределении, я провёл расчёты в диапазоне конверсионностей $[0.5\\hat{r}, 2\\hat{r}]$ и диапазоне количества визитов $[\\hat{v}/3,3\\hat{v}]$, при объеме 1000 визитов в день.\nНа диаграмме видно, что стратегия не чувствительна к ошибкам в эффективном количестве визитов, область оптимальных результатов простирается от $v=100$ до $v=1100$. Ошибки в конверсионности более критичны. На практике затруднения обычно вызывает как раз определение правильного $v$, а промахнуться в два раза мимо конверсионности довольно сложно. При определении $v$ лучше ошибиться в сторону меньшего количества визитов (т.е. большей дисперсии): видно, что область оптимальных результатов расширяется к низу.\nПрименимость для других распределений конверсионности Посмотрим, как ведёт себя probability matching при использовании распределений конверсионностей, отличных от распределения по умолчанию (матожидание конверсии $1.13\\%$, $\\sigma=0.5$). Возьмем диапазон средней конверсии от 0.2% до 5% и стандартного отклонения 0.25\u0026ndash;1.5. Примеры таких распределений приведены ниже:  \nСтратегия работоспособна во всём диапазоне, но результаты, естественно, различаются:   С увеличением средней конверсионности эффективность стратегии растёт, т.к. появляется больше информации о конверсиях. При увеличении дисперсии эффективность также растёт, так как увеличивается различие между \u0026ldquo;плохими\u0026rdquo; и \u0026ldquo;хорошими\u0026rdquo; источниками. Чем больше это различие, тем раньше можно его выявить, и тем лучше результат. При благоприятных условиях (высокая конверсионность + высокая дисперсия) стратегия достигает улучшения \u0026gt;80% (напомню, это результат для 100 визитов в день, т.е. в среднем один (!) визит на источник). Если стратегия работает с объемом 1000 визитов в день, результат ещё лучше:   Белая область на диаграмме \u0026ndash; это улучшение \u0026gt;90%.\nНестационарная конверсионность Во всех предыдущих экспериментах мы исходили из того, что латентная конверсионность источников, единожды заданная на старте, не меняется в ходе эксперимента. В реальной жизни этого никто не может обещать. Более того, стабильная конверсионность источников будет скорее исключением, чем правилом. Сможет ли probability matching справиться с этим?\nСмоделируем нестационарность как геометрическое случайное блуждание латентной конверсионности c Гауссовскими приращениями и возвратом к среднему: $$\\theta\u0026rsquo;_0 = \\ln(\\theta_0) \\\\\n\\mu_t = \\eta(\\theta\u0026rsquo;_0 - \\theta\u0026rsquo;_t) \\\\\n\\theta\u0026rsquo;_{t+1} = \\theta\u0026rsquo;_t + \\mathcal{N}(\\mu_t, \\sigma^2) \\\\\n\\theta_{t+1} = \\exp(\\theta\u0026rsquo;_{t+1}) $$ где $\\theta_0$ \u0026ndash; начальная конверсионность, $\\eta$ \u0026ndash; скорость возврата к среднему, $\\sigma$ \u0026ndash; волатильность. Изменение конверсионности в ходе эксперимента может выглядеть например так:  \nЧтобы стратегия могла адаптироваться к нестационарности, научим её \u0026ldquo;забывать\u0026rdquo; прошлое. Обычно суммарное количество конверсий к моменту времени $t$ получается сложением конверсий за все предыдущие дни: $$c_t = \\sum_{i=1}^t c\u0026rsquo;_i$$ где $c\u0026rsquo;_i$ это количество конверсий в день $i$. Теперь модифицируем суммирование, и будем домножать каждое предыдущее кол-во конверсий на коэффициент затухания, $\\gamma$: $$c_t = \\gamma c_{t-1} + c\u0026rsquo;_t, \\; \\gamma \\leq 1 $$ Таким образом на момент времени $t$ от конверсий первого дня останется часть, пропорциональная $\\gamma^t$. Стратегия забудет часть отдалённого прошлого. Такая же операция проделывается с визитами, в результате на выходе получим эффективное количество визитов и конверсий, которое будет меньше, чем просто сумма.\n  Результаты работы probability matching c $\\gamma=0.8$, $\\rho=2.1$, $N=1000$ (количество визитов в день увеличено, чтобы у стратегии оставались ресурсы на exploration).   Максимальная конверсионность (зелёный пунктир) постепенно увеличивается, потому что используются относительные приращения конверсионности, и для смещения вверх есть больше места, чем для смещения вниз (снизу конверсионность ограничена нулём). Видно, что график реальной конверсионности почти успевает за графиком максимальной, т.е. стратегия видит изменения и адаптируется к ним. Также помогает относительно небольшое $\\rho$, стимулирующее exploration.\nДля сравнения: получено улучшение 65.7%, улучшение при работе без затухания ($\\gamma=1$) составит 59%, улучшение при тех же условиях в стационарном режиме (т.е. с неизменной конверсией) \u0026ndash; около 75%.\n   На диаграмме весов видно, что стратегия регулярно переключается между режимами exploration и exploitation, т.к. конверсионность источников постепенно изменяется.\nИтоги Если пересчитать результаты работы стратегий в прирост количества конверсий на сайте, Probability Matching даcт прирост около 33% относительно оптимизированной наивной стратегии, CB Racing соответственно немного меньше. Учитывая, что в реальной жизни источниками часто управляют хаотично и качество такого управления не дотягивает даже до наивной стратегии, прирост может быть ещё выше.\nУ байесовских стратегий хорошие перспективы применения, учитывая то, что они не обязаны непосредственно управлять источниками. Вычисленное с помощью probability matching оптимальное количество трафика для источников можно отображать в отчётах просто в качестве подсказки \u0026ldquo;как лучше распределить трафик\u0026rdquo;. Ну а CB Racing это не только стратегия, но и инструкция по использованию \u0026ldquo;усов\u0026rdquo; \u0026ndash; правдоподобных интервалов конверсионности.\n  Probability matching устойчив к ошибкам в подборе гиперпараметров, это тоже важно для практического применения. Я протестировал эту стратегию во всех режимах, со всеми возможными ошибками, на которые хватило фантазии, и ни в одном их них не было серьезных сбоев.\nProbability matching также легко расширяется для использования дополнительной контекстной информации. В реальной жизни нам известно об источниках и о трафике, который из них приходит, гораздо больше, чем просто количество конверсий. Тип источника, рекламная система, поведение посетителей из источника на сайте, просмотренные страницы, продолжительность визита, и т.д. \u0026ndash; всю эту информацию можно и нужно использовать для повышения точности прогноза. Возможности здесь практически безграничны, т.к. можно сэмплировать конверсионность не только из аналитически заданных распределений, но и из моделей машинного обучения, включая многослойные нейросети[8].\nЕстественно, область применения не ограничивается сайтами: реклама мобильных приложений, игровая реклама, call tracking \u0026ndash; стратегии могут использоваться везде, где существует выбор между несколькими альтернативными источниками пользователей/клиентов.\nКод для работы с моделью и стратегиями, описанными в статье, выложен в GitHub: https://github.com/Arturus/conv_simulator\nТакже в Google Colab доступен готовый для работы Notebook со стратегиями: https://colab.research.google.com/github/Arturus/conv_simulator/blob/master/demo.ipynb\nДопущения Чтобы не переусложнять модель и не загромождать текст подробностями, были сделаны некоторые допущения:\n Не учитывается сезонность, т.е. колебания объема трафика и конверсионности между рабочими и выходными днями. Стоимость трафика у всех источников принимается одинаковой. В реальной жизни для коммерческих источников надо было бы учитывать не кол-во конверсий на единицу трафика (конверсионность), а кол-во конверсий на единицу потраченных средств (стоимость конверсии), или прибыль на единицу затрат (ROI). Не учитываются отложенные конверсии, т.е. предполагается, что конверсия совершается в течение суток, между обновлениями весов источников. Для сайтов с \u0026ldquo;медленной\u0026rdquo; конверсией надо вводить поправки к наблюдаемому числу конверсий. Поддерживаются только модели атрибуции, сопоставляющие конверсию с единственным источником (т.е. по первому или последнему переходу)  Источники  Top arm identification in multi-armed bandits with batch arm pulls. ^ K. Jun, K. Jamieson, R. Nowak, X. Zhu, 2016. AISTATS. (pp. 139-148)   Almost optimal exploration in multi-armed bandits ^ Z. Karnin, T. Koren, O. Somekh, 2013. ICML. (pp. 1238-1246)   Finite-time regret bounds for the multiarmed bandit problem. ^ N. Cesa-Bianchi, P. Fischer, 1998. ICML. (pp. 100-108)   Finite-time analysis of the multiarmed bandit problem ^ P. Auer, N. Cesa-Bianchi, P. Fischer, 2002. Machine learning. 47.2-3 (pp. 235-256)   Individual choice behavior: A theoretical analysis ^ R. Luce, 1959.   A modern bayesian look at the multi-armed bandit ^ S. Scott, 2010. Applied Stochastic Models in Business and Industry. 26.6 (pp. 639-658)   A tutorial on thompson sampling ^ D. Russo, B. Van Roy, A. Kazerouni, I. Osband, Z. Wen, 2018. Foundations and Trends in Machine Learning. 11.1 (pp. 1-96)   Deep contextual multi-armed bandits [arXiv] ^ M. Collier, H. Llorens, 2018.    ","date":1545336000,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1545336000,"objectID":"a098f765c19c9ed47c4fb3644b1839b0","permalink":"https://suilin.ru/post/conversion_opt/","publishdate":"2018-12-21T00:00:00+04:00","relpermalink":"/post/conversion_opt/","section":"post","summary":"При управлении онлайновыми рекламными кампаниями при подключении новых источников (объявлений, баннеров, SMM и т.п.) часто приходится решать проблему:\n С одной стороны, разумно подождать, пока источник не выдаст побольше переходов, посмотреть на конверсии, и тогда принимать решение, оставить его в рекламной кампании или отключить. Но если долго ждать, тогда рекламный бюджет будет зря расходоваться на неэффективные источники. С другой стороны, если сократить ожидание, можно случайно отключить источник, который на самом деле конверсионный, и наоборот, оставить неэффективный источник, случайно показавший высокую конверсию.","tags":["Conversion","MAB"],"title":"Конверсия и data science II. Оптимизируем неизвестность","type":"post"},{"authors":null,"categories":["Internet analytics"],"content":" Введение Этой статьей я начинаю цикл о проблемах Интернет-аналитики (аналитики в широком смысле: аналитика сайтов, аналитика мобильных приложений, игровая аналитика и т.п.) и возможных способах их решения с позиций современного data science. Над этими проблемами я начал размышлять давно, ещё когда делал свои первые проекты по интернет-аналитике, а потом когда создавал Вебвизор и работал в Яндекс.Метрике. Но до практического их решения я добрался только сейчас, когда появилось время на изучение нужной литературы и расширение своего кругозора. Можно сказать, что появилась возможность закрыть гештальт, и я рад ей воспользоваться.\nНесмотря на бурное развитие data science и машинного обучения, современная Интернет-аналитика по большей части осталась такой же, как и 10 лет назад. Те же отчеты, графики, сегментирование, и т.д. По большому счёту это просто удобный интерфейс к табличной базе данных с функциями агрегации и выборки нужных сегментов. Всё осмысление выдаваемых отчётами цифр ложится на плечи пользователя. Между тем, напрямую глядя на эти цифры, можно получить на так уж много полезной информации, и гораздо больше бесполезной. Я попробую описать методики работы с этими сырыми данными (и по возможности создать практическую реализацию этих методик), чтобы повысить value современной интернет-аналитики, сделать её более полезной и информативной.\nТерминология В русской терминологии существует некоторая путаница, словом \u0026ldquo;конверсия\u0026rdquo; может обозначаться и событие совершения посетителем целевого действия (достижение цели) и соотношение кол-ва визитов, где произошла конверсия, к общему количеству визитов (conversion rate, коэффициент конверсии, показатель конверсии). Чтобы избежать неоднозначности, я буду использовать для соотношения $\\frac{визиты\\,с\\,конверсией}{все\\,визиты}$ термин конверсионность, как меру способности входящего трафика генерировать конверсии, а для события достижения цели - слово конверсия (т.е. так же, как его использует Google Analytics).\nПроблема измерения конверсионности Итак, конверсионность, или коэффициент конверсии (в терминах Google Analytics), или просто конверсия (в терминах Яндекс.Метрики), это отношение количества \u0026ldquo;конверсионных\u0026rdquo;, достигших цели визитов к общему количеству визитов. Вместо визитов могут быть посетители, конверсией может быть покупка товара в магазине, регистрация, приобретение артефакта в игре, суть от этого не меняется. Конверсионность обычно рассматривают в разрезе источников трафика, собственно это основной показатель качества источника. Чем выше конверсионность, тем лучше. Если бы конверсионность измерялась на основе непрерывных физических величин, таких как градусы или километры, на этом можно было бы завершить рассказ и перейти к более интересным темам. Проблема в том, что и конверсии и визиты, на основе которых измеряют конверсионность, это дискретные события, а конверсии еще и относительно редкие.\nЕсли есть два визита и ноль достижений цели, это означает, что конверсионность источника равна нулю и его надо немедленно отключить? А если один визит и одно достижение цели, то конверсионность 100%? Мы нашли курицу, несущую золотые яйца, и надо отключить все другие источники, кроме этого? С точки зрения здравого смысла - нет, надо еще подождать, и тогда станет более ясно. Но сколько подождать? Если есть 10 визитов, уже можно верить конверсионности, или ещё нет? Или надо сто визитов? Тысячу визитов?\nУвы, современная интернет-аналитика не дает сколько нибудь внятного ответа на такой простой вопрос, возникающий каждый раз оценке конверсионности недавно появившихся или просто слабых источников трафика, а также при подключении новых рекламных объявлений. Будем искать ответ сами.\nКонверсионность как вероятность Посмотрим на конверсии, как на вероятностный процесс. Будем считать каждый визит на сайт попыткой конверсии. Т.е. посетитель, приходящий на сайт, может сконвертироваться, а может и не сконвертироваться. Вероятность конверсии это и есть конверсионность, принимающая значения от 0 до 1 (или от 0% до 100%). Вероятность успехов (конверсий) в ходе независимых попыток описывается простейшим распределением Бернулли: $$\\Pr(X=1)=p=1-\\Pr(X=0)$$ где $X$ это случайная величина, которая принимает одно из двух значений: $1$ (успех, произошла конверсия) или $0$ (неудача, конверсии не было), а $p$ это вероятность успеха, она же конверсионность в нашем случае. Выражение $\\Pr(X=y)$ означает \u0026ldquo;вероятность того, что случайная величина $X$ окажется равна $y$\u0026rdquo;.\nОбычно интересны не единичные события, а то, что происходит на некотором интервале времени. Допустим, на сайт пришло 1000 посетителей из источника с конверсионностью 0.01, или 1% (сейчас предположим, что нам известно точное значение конверсионности). Сколько посетителей сконвертируется? Ровно 10? Совсем не обязательно. Количество успехов из $n$ попыток, с вероятностью успеха каждой попытки, равной $p$, описывается биномиальным распределением: $X \\sim \\mathrm{Bin}(n,p)$. Соответственно, вероятность получить $k$ конверсий описывается функцией вероятности биномиального распределения: $$\\Pr(X=k)={n\\choose k}p^k(1-p)^{n-k}$$ $$\\binom n k =\\frac{n!}{k!(n-k)!}$$ Второе выражение это биномиальный коэффициент, который и дал название распределению. Выглядит страшновато, но ничего сверхъестественного в этих формулах нет. Представим, что каждая попытка это подбрасывание монетки, успех если выпал орёл, неуспех если решка. Если подбрасываем монетку один раз, имеем 50% вероятность успеха. Если подбрасываем два раза, есть четыре варианта развития событий: орёл-орёл, орёл-решка, решка-орёл, решка-решка. То есть 25% вероятность получить два успеха, 50% вероятность получить один успех и 25% вероятность получить ноль успехов. Можно продолжать и дальше, но то, что мы посчитали, это уже биномиальное распределение для $n=2$ и $p=0.5$!\nФормула биномиального распределения просто задаёт обобщенный способ такого расчёта, для любых $n$ и $p$.\nРаспределение Бернулли это частный случай биномиального распределения для единичной попытки ($n=1$), поэтому эти распределения взаимозаменяемы. Обычно распределение Бернулли используют, когда рассматривают единичные события, а биномиальное - когда рассматривают сразу много событий, например все визиты за день.\nВернёмся к конверсиям, и построим график вероятности количества конверсий для наших 1000 визитов, т.е. $n=1000$ и $p=0.01$ (конверсионность 1%)\n  Вероятность получить ровно 10 конверсий - всего 12.6%! Примерно такая же вероятность получить 9 конверсий, а вообще можно получить любое число от 0 до примерно 21.\nВ чём же дело?\nКонверсионность как матожидание На самом деле существует две конверсионности. Первая это параметр $p$ в распределении Бернулли и биномиальном распределении, определяющий вероятность конверсии посетителя. Назовём его истинной конверсионностью. Для распределения Бернулли, описывающего и конверсию и бросание монеты, параметр $p$ равен математическому ожиданию случайной переменной: $$p=\\mathrm{E}(X)$$ Вторая конверсионность - это реализованная (наблюдаемая) конверсионность, т.е. результаты \u0026ldquo;подбрасывания монетки\u0026rdquo;. Та самая цифра, которую мы видим в отчётах Метрики и Google Analytics под названием \u0026ldquo;конверсия\u0026rdquo; и \u0026ldquo;коэффициент конверсии\u0026rdquo;, являющаяся средним количеством конверсий на $n$ визитов. Визуализацию того, как эта конверсионность соотносится с первой, истинной конверсионностью, мы только что сделали (биномиальное распределение).\nНа практике математическое ожидание обычно считают эквивалентным среднему значению, но это не совсем так. Согласно Закону больших чисел среднее значение стремится к матожиданию на бесконечно большой выборке: $$\\frac{1}{n} \\sum_{i=1}^n X_i \\rightarrow \\mathrm{E}[ X ], \\quad n \\rightarrow \\infty$$\nНо закон больших чисел ничего не говорит о том, насколько быстро среднее сойдется к матожиданию. Для многих распределений, встречающихся в жизни, по выборке из 100 значений уже можно вполне точно оценить матожидание. Но интернет-аналитике здесь не повезло, распределение Бернулли с маленьким значением $p$ это как раз пример медленного схождения. Проиллюстрируем это графически.\n  На диаграмме изображена наблюдаемая конверсионность для 10 источников трафика с одинаковой истинной конверсионностью, равной 1%. Каждый источник случайным образом берет (сэмплирует) конверсии из распределения Бернулли с $p=0.01$. Это модель реальных источников и реальных цифр в отчётах. Как видно, до 100 визитов значения конверсионности почти случайны, к 1000 визитов они наконец группируются вокруг истинного значения, но только к 100 000 визитов ($10^5$) наблюдается более-менее полное схождение к точному значению. Повторюсь, сто тысяч визитов. Многие ли дожидаютя 100К визитов, прежде чем оценить конверсионность?\nВ реальности всё ещё хуже, т.к. за время, которое пройдет до набирания 100К визитов, свойства источника скорее всего успеют измениться (например там появится другая аудитория), и истинная конверсионность тоже изменится. Узнать правду (истинную конверсионность) в такой ситуации вообще невозможно.\n  Если конверсионность менее одного процента (на последнем графике смоделирована истинная конверсионность 0.2%), ситуация усугубляется. Стабилизация значений наступает только к 10000 визитов.\nЧтобы нагляднее оценить неточность наблюдаемой конверсионности, построим графики биномиальных распределений при фиксированном $p$ и разных $n$ в сравнимом масштабе по осям X и Y. Для этого разделим значения по оси X (кол-во конверсий) на количество визитов $n$, и получим как раз возможные наблюдаемые значения конверсионности при заданном истинном значении. Значения по оси Y наоборот умножим на кол-во визитов:\n  Все графики построены для истинной конверсионности 1%. Видно, что когда есть только 100 визитов, у нас примерно одинаковые шансы наблюдать конверсионность и 0% и 1%, а также ощутимая возможность увидеть конверсионность 2%, 3% и более. Для 1000 визитов значения уже явно сгруппированы вокруг истинной конверсионности (но пока в диапазоне плюс-минус километр), ну а для 10000 визитов уже есть хороший шанс увидеть относительно точное значение истинной конверсионности.\nИтак, биномиальное распределение говорит нам, какие наблюдаемые значения конверсионности можно получить при заданном истинном значении. Но на практике обычно требуется решение обратной задачи: есть наблюдаемые (неточные) значения конверсионности, на основе которых надо оценить истинную (точную) конверсионность. Эту задача наиболее удобно решается с помощью Байесовского подхода.\nКонверсионность как апостериорное распределение Давайте отложим в сторону теорию вероятностей, и посмотрим, как работают с конверсией обычные люди. Представим, что есть человек, профессионально занимающийся закупками трафика и ведением рекламных кампаний (назовём его Трафик-менеджер), у него есть история предыдущих объявлений в Директе, средняя наблюдаемая конверсия там в районе 2%.\n Подключается новое объявление и набирает два визита и ноль достижений цели. Какая у него конверсия? Трафик менеджер скажет, что пока у нас маловато данных, чтобы делать какие то выводы, но скорее всего конверсия будет в районе 1-5%, потому что у всех других объявлений она была в этом диапазоне. Но точно не 50%, и вероятно не 0%, несмотря на то, что прямо сейчас она равна нулю. Затем объявление набирает 10 визитов из них 2 конверсии. Трафик-менеджер скажет, что это объявление имеет шансы быть хорошим, конверсия точно не ноль, но скорее всего и не 20%, потому что такая конверсия выглядит нереалистично, более реально иметь конверсию 2-6%. Объявление набирает 100 визитов, из них 4 конверсии. Трафик-менеджер скажет, что конверсия объявления в районе 3-5%, но надо подождать ещё, цифра может измениться. И наконец, когда объявление наберет 1000 визитов из них 35 конверсий, трафик-менеджер скажет, что достаточно уверен в конверсии около 3.5%. Но последняя цифра скорее всего ещё изменится, и будет например 3.3% или 3.7%.  Рассуждения нашего трафик-менеджера это не что иное, как байесовский подход к решению проблемы, применяемый на интуитивном уровне! В чём он заключается?\n Есть вероятностная модель, содержащая ненаблюдаемые напрямую параметры (их называют латентными, или скрытыми переменными) и описание, как эти параметры связаны с наблюдаемыми величинами. В нашем случае единственный параметр это $p$, вероятность конверсии. Наблюдаемые величины это количество событий визит и конверсия, они связаны с $p$ биномиальным распределением. Задача - на основе данных о наблюдаемых величинах получить оценку значений ненаблюдаемых (латентных) параметров. Оценка всегда имеет вид распределения вероятностей.\n Есть некоторая информация о диапазоне возможных значений параметров и приблизительной вероятности того или иного значения, основанная на предыдущем опыте. Если мы попросим нашего трафик-менеджера визуализировать его представления о том, какой вообще может быть конверсионность, он нарисует примерно такой график:Эта информация называется априорное распределение. Априорное распределение, как правило, имеет большую дисперсию (график \u0026ldquo;размазан\u0026rdquo; вдоль оси X) и таким образом выражает низкую уверенность в конкретных значениях: конверсионность может быть и 1%, и 2%, и 3%, и больше.\n Через модель прогоняются данные в виде наблюдаемых значений, и априорное распределение модифицируется таким образом, чтобы лучше соответствовать данным (т.е. чтобы повысить правдоподобие модели). Чем больше данных прогоняется через модель, тем сильнее модифицируется априорное распределение. Получившееся распределение, учитывающее и априорное знание, и реальные наблюдения, называется апостериорное распределение (о происхождении терминов априорный и апостериорный можно прочитать в Википедии). Если все данные недоступны сразу (в нашем случае рекламная кампания идет в реальном времени и данные поступают порциями, например раз в день), то апостериорное распределение, полученное после одной порции данных, может использоваться как априорное для следующей порции, и т.д. Это соответствует тому, как наш трафик-менеджер постепенно уточняет свою оценку по мере поступления данных.\n Полученное в итоге апостериорное распределение и является оптимальной оценкой того, какие значения могут быть у параметров модели. Обычно апостериорное распределение имеет более узкую и вытянутую форму по сравнению с априорным (пример - приведённый выше график биномиального распределения для 10000 визитов), отражая рост уверенности в конкретном диапазоне значений параметра. Если данных много, то форма апостериорного распределения практически не зависит от формы априорного. Если наоборот мало, то апостериорное распределение будет близко к априорному.\n  Собственно, это вся суть байесовского подхода к анализу данных. Процесс получения апостериорного распределения называют еще байесовским выводом (bayesian inference). Осталось только выразить всё это в виде формул, чтобы можно было автоматически рассчитывать параметры конверсионности, не прибегая к интуиции трафик-менеджера.\nТеорема Байеса Я намеренно описал байесовский подход на практических примерах, а не на основе теоремы Байеса, т.к. для полного понимания этой теоремы нужно приличное знание тервера, которое есть не у всех потенциальных читателей этой статьи. А если оно есть, то и теорема Байеса таким читателям вероятно знакома. Но на всякий случай кратко изложу:\n$$\\Pr(\\theta|y)=\\frac{\\Pr(y|\\theta)\\Pr(\\theta)}{\\Pr(y)}$$\nЭто теорема Байеса. На самом деле её в таком виде сформулировал не Байес, а Пьер Симон Лаплас. Сам Байес понятия не имел ни о \u0026ldquo;теореме Байеса\u0026rdquo; ни \u0026ldquo;байесовских методах\u0026rdquo;.\nТомас Байес был священником и при своей жизни опубликовал единственную работу по математике в поддержку матанализа, изобретенного Ньютоном. Эссе Байеса по условным вероятностям, которому сам автор видимо не придавал большого значения и не был уверен в правильности своих выводов, обнаружил после смерти Байеса (1761 год) его друг, Ричард Прайс. Прайс, тоже увлечённый математикой священник, настолько вдохновился этим эссе, что объявил его теорией, подтверждающей существование Бога, основательно подредактировал спорные места, исправил ошибки, и представил публике. Тем временем Лаплас, который был настоящим сварщиком математиком (и атеистом), независимо открыл и опубликовал примерно то же, о чём писал Байес, а также то, о чём Байес не писал, включая ту самую теорему, известную нам как \u0026ldquo;теорема Байеса\u0026rdquo;. Но видимо Прайс был хорошим продажником очень убедителен, именно Байеса с его подачи стали считать одним из основателей теории вероятностей, а самого Прайса за популяризацию идей Байеса приняли в Королевское Научное Общество.\nЭтим всё не закончилось. В 1922 году Рональд Фишер (отец современной статистики, который в одно лицо придумал основную её часть, а заодно и современную генетику) заявил, что \u0026ldquo;теория обратной вероятности (так он называл работы Байеса и Лапласа) основана на ошибке и должна быть полностью отвергнута\u0026rdquo;. В кругу Байеса сказали бы, что Фишер объявил эту теорию ересью. Авторитет Фишера был огромен, спорить с ним было трудно, и \u0026ldquo;теория обратной вероятности\u0026rdquo; была надолго отправлена в архив.\nТолько в начале XXI века байесовские методы вновь вошли в моду и приобрели практическое значение в связи с развитием вычислительной техники, позволившей решать задачи байесовского вывода числовыми методами, а не аналитическими (аналитические решения известны только для ограниченного круга задач). Тем не менее мейнстримовая статистика продолжает пользоваться методами Фишера, а байесовские методы популярны в основном у хипстеров data scientist-ов.\nТеперь - что означают все эти буквы в формуле. Символом $\\theta$ принято обозначать параметры модели, которые мы хотим найти (в нашем случае истинную конверсионность, в контексте Байесовских моделей будем называть её также латентной конверсионностью). $y$ это данные, наши наблюдаемые величины визиты и конверсии.  $\\Pr(\\theta|y)$ это вероятность параметров при наблюдаемых данных - апостериорное распределение, которое мы хотим получить $\\Pr(y|\\theta)$ это то, насколько хорошо параметры модели согласуются с наблюдаемыми данными, или правдоподобие (likelihood) $\\Pr(\\theta)$ это вероятность иметь именно такие параметры или \u0026ldquo;на что обычно похожи параметры\u0026rdquo; - априорное распределение $\\Pr(y)$ это фактор, нужный для того, чтобы справа получилась корректная вероятность, т.е. всё просуммировалось в единицу. Обычно все проблемы вызывает именно этот компонент, т.к. для его вычисления требуется взять интеграл: $\\Pr(y)=\\int\\Pr(y|\\theta)\\Pr(\\theta)d\\theta$, который аналитически доступен далеко не всегда. Но в нашем случае проблем не будет.  Бета-биномиальная модель Перейдем наконец к делу. До того, чтобы сделать полноценную модель, остался один шаг: выбрать априорное распределение. Для работы с биномиальным распределением, описывающим генерацию данных, обычно используют бета распределение, которое является сопряженным с биномиальным. Сопряженное означает, что при использовании бета распределения как априорного, апостериорное тоже будет бета распределением, это удобно.\nБета распределение непрерывное (в отличие от дискретного биномиального), определено на интервале $[0,1]$ и задаётся двумя параметрами, $\\alpha$ и $\\beta$. Формула плотности вероятности: $$P(x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}} {\\mathrm{B}(\\alpha,\\beta)}$$ где $\\mathrm{B}(\\cdot,\\cdot)$ это бета-функция.\nПолный вывод бета-биномиальной модели тоже не привожу, его можно найти его можно найти на Википедии и практически в любом курсе по теории вероятностей. В контексте работы с конверсией нам интересен конечный результат, а он очень простой. Если априорное бета распределение было задано параметрами $\\alpha$ и $\\beta$, и в наблюдаемых данных было $s$ успехов и $f$ неуспехов, т.е. всего $s+f$ попыток, то апостериорное распределение будет бета распределением с параметрами $\\alpha\u0026rsquo;=\\alpha+s$ и $\\beta\u0026rsquo;=\\beta+f$. $$Posterior(x;s,f) = \\frac{x^{\\alpha-1+s}(1-x)^{\\beta-1+f}} {\\mathrm{B}(\\alpha+s,\\beta+f)} $$\nТо есть всё, что надо сделать, чтобы получить апостериорное распределение в бета-биномиальной модели, это сложить две пары чисел, априорные параметры и наблюдения. Параметры $\\alpha$ и $\\beta$ можно интерпретировать, как количество успехов и неуспехов, заложенное в модель в качестве априорной информации.\nДавайте посмотрим, как всё это работает на конкретных примерах. Допустим, истинная конверсионность источника равна 2%. В первом примере предположим, что у нас нет никакой априорной информации, например мы измеряем конверсионность на только что открывшемся сайте и до этого никогда не работали с сайтами похожей тематики. В этом случае в качестве параметров априорного распределения можно взять $\\alpha=1, \\, \\beta=1$. Априорное распределение будет просто горизонтальной линией, отражением факта, что мы ничего не знаем о том, какая может быть конверсия. В байесовских терминах это будет называться неинформативное априорное распределение.\nВо втором примере предположим, что мы правильно угадали априорное распределение, соответствующее источнику, и зададим его параметрами $\\alpha=1, \\, \\beta=49$, это означает 1 успех и 49 неуспехов, т.е. конверсию 2%, равную конверсии источника.\n  На графиках показаны апостериорные распределения, образовавшиеся после разного числа наблюдений, т.е. визитов. Пунктирной линией обозначено матожидание каждого распределения (для бета распределения $\\mathrm{E}[X]=\\frac{\\alpha}{\\alpha + \\beta}$).\nВ первом примере видно, что при небольшом количестве визитов матожидание промахивается мимо верного значения, но после примерно 600 визитов большое количество наблюдаемых данных приводит распределение ближе к истине. Во втором примере, естественно, матожидание с самого начала правильное (все пунктирные линии совпали), меняется только форма распределений по мере накопления наблюдений: от размазанности вдоль оси X (большой неопределённости), до острого пика, соответствующего высокой уверенности в узком диапазоне значений конверсионности.\nТакже заметно, что на втором примере распределения для небольшого количества визитов более узкие - так влияет наличие априорной информации, оно снижает степень неопределённости.\nВизуально форма и положение распределений на первом примере кажутся более \u0026ldquo;правильными\u0026rdquo;, чем на втором, но это оптический обман, связанный с несимметричностью распределений: у них длинный хвост справа, поэтому матожидание не совпадает с визуальным максимумом (эта точка называется мода) распределения.\nПостроим еще два примера, на которых смоделируем ситуацию, когда истинная конверсионность отличается от конверсионности, заданной априорным распределением (т.е. нам попался нетипичный источник, конверсионность которого отличается от априорной \u0026ldquo;средней по больнице\u0026rdquo;). Насколько большую ошибку внесёт априорное распределение, и насколько быстро она исправится?\n  На первом графике априорное распределение соответствует конверсии 4%, на втором конверсии 1%. Видно, что ближе к 600-1000 визитов ошибка становится несущественной.\nТеперь оценим, как априорная информация влияет на сходимость. Построим такой же график схождения к истинному значению как раньше (истинная конверсионность 1%), но будем оценивать конверсионность не как отношение конверсий к визитам, а как матожидание апостериорного распределения, при априорных параметрах $\\alpha=1,\\,\\beta=99$\n  Видно, что стало лучше. Разброд и шатание, которые были раньше до 1000 визитов, ощутимо уменьшились. В целом скорость сходимости не изменилась (Природу не обманешь), но вот дисперсия при небольшом количестве визитов теперь находится в пределах разумного.\nВидим невидимое! Посмотрим, чему мы научились. У источника есть латентная конверсионность, которую невозможно измерить напрямую. Можно только косвенным образом оценить её по отношению кол-ва конверсий к кол-ву визитов, но эта оценка при малом количестве данных работает очень грубо. Байесовский подход позволил нам глубже заглянуть \u0026ldquo;внутрь\u0026rdquo; источника и увидеть его латентную конверсионность. Мы видим её немного расплывчато, в виде распределения, а не в виде точного значения, но всё равно это большой прогресс по сравнению с тем, что было раньше.\nБолее точная оценка конверсионности при малом количестве данных это не главное из того, что даёт нам Байесовский подход. Основная ценность в том, что у нас теперь есть апостериорные распределения, отражающие степень уверенности в результатах измерений. Для интернет аналитики на самом деле важно не абсолютное значение конверсионности, а сопоставление источников друг с другом, чтобы оставить лучшие и отключить худшие. И здесь наличие апостериорных распределений не просто улучшает ситуацию, но в корне её меняет.\nСравнение источников друг с другом Давайте посмотрим на источники трафика не со стороны точных значений их конверсионности, а со стороны понимания, какой источник хуже, какой лучше, а какой имеет потенциал, но пока себя не проявил. Насколько вообще различима конверсионность источников при небольшом количестве визитов? В порядке эксперимента, посмотрите на приведенные ниже диаграммы. На них изображены апостериорные распределения конверсионности двух источников, голубого и оранжевого. У какого источника конверсионность больше?\n  Если вы думаете, что на обоих конверсионность больше у оранжевого источника, предлагаю посмотреть ещё на такую диаграмму:\n  А здесь у кого больше?\nПравильный ответ: на первой диаграмме конверсионность источников статистически различима. У оранжевого источника она, очевидно, больше. Для остальных диаграмм правильный ответ \u0026ndash; неизвестно. На второй диаграмме несколько больше вероятность того, что у оранжевого конверсия выше, но дать уверенный ответ невозможно, т.к. диапазоны возможных истинных значений сильно пересекаются. На третьей диаграмме диапазон возможных истинных значений голубого источника настолько велик, что мы вообще не можем ничего сказать.\nА теперь представьте цифры конверсионности в отчётах Метрики или Аналитикса. Конечно все они будут отличаться друг от друга, потому что у этих распределений разные матожидания, и пользователь будет пребывать в полной уверенности, что видит явную разницу между конверсионностями (ведь цифры не могут врать). Увы, могут (мы наглядно в этом убедились на графиках схождения к истинному значению), и для небольших источников врут гораздо чаще, чем показывают истину. Во многих случаях, когда пользователь сравнивает конверсионность источников, он видит всего лишь миражи, созданные случайным шумом, и делает на их основе ложные выводы.\nНо мы теперь вооружены апостериорными распределениями, и сможем отличить мираж от реальности! В Байесовской статистике есть понятие правдоподобный интервал (credible interval), которое похоже на хорошо известный доверительный интервал из традиционной статистики. Проще всего построить его по квантилям апостериорного распределения, например интервал 95% (т.е. содержащий 95% плотности распределения) будет находиться между квантилями 0.025 и 0.975.\n  На диаграмме показан пример таких интервалов. Теперь легко можно определить, можно ли сравнивать конверсионность источников: можно, если их правдоподобные интервалы не пересекаются, и нельзя, если пересекаются. Размером правдоподобного интервала можно регулировать степень нашей уверенности в различии, например интервал 90% даст нам возможность сравнить друг с другом больше источников, чем интервал 95%, но при этом будет больше вероятность ошибки. На практике разумно использовать интервал 90% или 95%.\nКонечно, если потребуется визуально сравнить друг с другом 10 источников, то строить ради этого 10 наложенных друг на друга диаграмм с распределениями будет неудобно. Для массового сравнения хорошо подходит тип диаграммы, называемый forestplot. Пример показан ниже.\n  Кружок в центре это матожидание, т.е. ожидаемая в среднем конверсионность по данным апостериорного распределения, а \u0026ldquo;усы\u0026rdquo; это правдоподобный интервал. Соответственно, можно сравнивать конверсионность источников, усы которых не пересекаются по вертикали. Кружок можно рассматривать как текущий прогноз конверсионности, т.е. значение, к которому будет стремиться конверсионность источника по мере накопления данных.\nРанжирование источников Ранжирование (т.е. сортировка) источников это ещё одна задача, близкая к сравнению. Наверное все пробовали отсортировать отчёты Метрики и Аналитикса по конверсионности и все видели, какая бессмысленная ерунда там всплывает наверх. И здесь тоже приходят на помощь правдоподобные интервалы. Принцип простой: сортировку по убыванию (т.е. когда нас интересует топ лучших) надо делать по значению нижней, пессимистической границы интервала, а сортировку по возрастанию (топ худших) наоборот по верхней оптимистической.\n   На диаграмме показаны ранжированные по убыванию конверсионности источники реального сайта. Источникам, находящимся вверху диаграммы, можно смело добавлять трафик, не боясь израсходовать бюджет на псевдотоповые источники, у которых случайно сконвертировался один визит из нескольких имеющихся.\n   А здесь обратное ранжирование, по худшим источникам. Источники, оказавшиеся вверху, можно смело отключать. Для наглядности серыми кружками обозначена конверсионность, которая отображалась бы в обычном отчёте. Видно, что в топ вышли бы источники с нулем конверсий, которые пока просто не успели набрать статистически значимое количество визитов. Весь отчёт был бы забит такими \u0026ldquo;нулевыми\u0026rdquo; источниками, и найти среди них действительно плохие было бы затруднительно.\n  Практические рекомендации Самая трудоёмкая часть работы при использовании Байесовского подхода это выбор априорного распределения для каждой конкретной ситуации.\nВ простейшем случае можно просто измерить среднюю конверсию источников по заданной цели и взять её за основу. Для распределений конверсионности, встречающихся на сайтах в реальной жизни, неплохо работает такой принцип: принимаем параметр $\\alpha$ (число успехов) равным единице, и задаём параметр $\\beta$, равный числу визитов, необходимому, чтобы получить одну конверсию, минус один успех.\nНапример, средняя конверсия 2%, $\\alpha=1$, тогда $\\beta=50-1=49$. Если параметр $\\beta$ получается меньше чем 30-40, можно принять $\\alpha=2$, и соответственно удвоить значение $\\beta$.\nТакже надо учитывать релевантность источника цели. Представим, что проходит рекламная кампания, под которую на сайте есть отдельный лэндинг, и на этом лендинге определена цель. Понятно, что конверсии по ней будут только у источников, участвующих в рекламной кампании, а у остальных источников будет нулевая конверсионность. Если включить в расчёт априорного распределения для этой цели все источники, а не только участвующие в рекламной кампании, получится некорректный результат.\nБолее продвинутые могут использовать расчет параметров по методу максимального правдоподобия (maximum likelihood estimation, MLE):\nfrom scipy.stats import beta # Конверсионность имеющихся источников (в долях, не в процентах) conversion_rate = [0.011, 0.008, 0.009, 0.012, 0.01, 0.0082, 0.0095] # В a и b запишутся параметры alpha и beta для априорного распределения a, b, _, _ = beta.fit(conversion_rate, floc=0, fscale=1)  Ещё более продвинутый способ это Байесовское моделирование с помощью бета-биномиального распределения. Это очень похоже на нашу модель для источников, только вместо апостериорного распределения конверсионности каждого отдельного источника моделируется общее распределение для всех источников одновременно.\nimport pymc3 as pm with pm.Model() as model: cr = pm.Beta('cr', alpha=1, beta=1) visits = pm.Lognormal('v', mu=np.log(100), sd=1) a = cr * visits b = (1-cr) * visits obs = pm.BetaBinomial('obs', a, b, n_visits, observed=n_conversions) result = pm.find_MAP()  Правда этому методу самому требуется априорное распределение, но здесь можно использовать или совсем приблизительные значения, посчитанные \u0026ldquo;на глаз\u0026rdquo;, как было описано выше, или неинформативный prior.\nСравнение и ранжирование источников это самые простые применения Байесовских методов. Разумеется, этим их возможности не ограничиваются. В следующей статье мы рассмотрим методику оптимального управления источниками трафика, в основе которой лежат апостериорные распределения.\n","date":1545249600,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1545249600,"objectID":"f33d8ef167e51a68f07798d038bef55b","permalink":"https://suilin.ru/post/conversion_numbers/","publishdate":"2018-12-20T00:00:00+04:00","relpermalink":"/post/conversion_numbers/","section":"post","summary":"Введение Этой статьей я начинаю цикл о проблемах Интернет-аналитики (аналитики в широком смысле: аналитика сайтов, аналитика мобильных приложений, игровая аналитика и т.п.) и возможных способах их решения с позиций современного data science. Над этими проблемами я начал размышлять давно, ещё когда делал свои первые проекты по интернет-аналитике, а потом когда создавал Вебвизор и работал в Яндекс.Метрике. Но до практического их решения я добрался только сейчас, когда появилось время на изучение нужной литературы и расширение своего кругозора.","tags":["Conversion"],"title":"Конверсия и data science I. Как увидеть невидимое.","type":"post"},{"authors":null,"categories":["Performance","Programming"],"content":"  th { text-align: right; }  В ходе работы над последним проектом мне понадобилось в промышленных масштабах генерировать сэмплы из бета-распределения. Первое что пришло в голову, это scipy.stat, тем более там есть куча дополнительных возможностей: и CDF, и квантили, и MLE, и всё, что душа пожелает. Но довольно быстро я понял, что scipy нетороплив, и генерация нескольких миллиардов сэмплов затянется на часы, а то и на дни. Стал искать альтернативные варианты и хочу теперь поделиться найденным.\nnumpy.random Сначала выяснилось, что numpy.random умеет генерировать не только равномерные случайные числа, но и сэмплы из распределений. И делает это бодрее, чем scipy.stat, особенно на небольших выборках: по видимому, у scipy.stat большие накладные расходы на каждый вызов.\nmkl_random Совершенно случайно наткнулся на возможность генерации сэмплов в Intel MKL. MKL это вообще такой подводный айсберг, в котором есть много всякого хорошего, но мало кто об этом знает. Почему то Intel его не продвигает, или просто не умеет продвигать?\nРебята крайне серьезно подошли к вопросу генерации сэмплов, для каждого распределения есть замеры производительности при разных значениях параметров распределений и при разных настройках базового генератора случайных чисел. Алгоритмически там тоже всё круто, для бета распределения в зависимости от параметров распределения происходит переключение аж между четырьмя различными алгоритмами генерации сэмплов: у каждого алгоритма есть своя оптимальная область параметров, в которой алгоритм даёт наилучшее быстродействие и качество.\nВ Питоне генератор сэмплов доступен как библиотека mkl_random, причём есть еще такая инкарнация, которая при установке попытается притащить с собой весь дистрибутив Intel Python. Потом выяснилось, что мне вообще ничего не надо устанавливать, mkl_random по видимому идёт в стандартной поставке Conda. Ещё mkl_random прорастает в numpy в виде пакета numpy.random_intel, но происходит это всегда, или при особом положении звёзд, я так и не понял.\nВ плане производительности всё превосходно, по сравнению с numpy.random разница примерно на порядок. Собственно, это лучший вариант для генерации сэмплов на CPU. Ещё одна приятная возможность \u0026ndash; можно менять базовый алгоритм генерации случайных чисел, есть быстрые генераторы, оптимизированные под процессоры с поддержкой SIMD. Переключение генератора совмещено с seed:\nimport mkl_random # SIMD-oriented Fast Mersenne Twister (SFMT) mkl_random.seed(None, 'SFMT19937')  cupy.random Генерация массива сэмплов это по своей природе очень хорошо параллелизуемая операция, поэтому я стал искать реализацию для GPU. Реализация нашлась в лице пакета cupy , имплементирующего подмножество функций numpy на GPU. Подмножество, кстати, весьма впечатляющее, и numpy.random тоже в него входит в виде реинкарнации cupy.random. Хочу выразить респект разработчикам Chainer/Cupy, которые не стали заново имплементировать функциональность numpy в виде собственного велосипеда с блэкджеком, как это сделали Tensorflow и Pytorch, а сохранили совместимость с оригинальным numpy на уровне API.\nСкорость работы просто фантастическая. На GTX1080Ti cupy генерирует более полумиллиарда сэмплов из бета распределения в секунду! Это больше, чем способны переварить остальные компоненты моего проекта. Апгрейдом до cupy вопрос скорости генерации сэмплов был полностью закрыт.\nНе обошлось без ложки дёгтя: в cupy некорректно работает обновление seed после генерации сэмпла. Seed хранится как 64-битный float (размером до 10308), и выставляется на старте обычно в достаточно большое число. После генерации сэмпла к нему добавляется целое, которое этому float-у как слону дробина, и не меняет его ни на единый бит. В итоге cupy каждый раз выдаёт один и тот же сэмпл. Пришлось выставлять seed вручную.\nОстальное Из любопытства прогнал через бенчмарк также высокоуровневые библиотеки, умеющие генерировать сэмплы из распределений: PyMC3, Pytorch и Tensorflow.\nPyMC3 показал стабильно худшие результаты среди всего, что я тестировал. Как это ни печально, библиотека, основная работа которой состоит в сэмплировании, делает это хуже всех остальных. Видимо сказывается возраст Theano, на котором основана вся функциональность PyMC3.\nPyTorch при работе на CPU показал себя средне (на Бета распределении вообще провал), но при работе на GPU вполне конкурировал с cupy, иногда отставал, иногда обгонял.\nTensorflow, как выяснилось, не умеет сэмплировать на GPU. Если запускать всё с дефолтными настройками, то он будет делать вид, что работает на GPU, но на самом деле будет сэмплировать медленнее по сравнению с честным запуском на CPU, потому что будет гонять данные туда-сюда. При сэмплировании на CPU показал себя хорошо, стабильное второе место после mkl_random, но большие накладные расходы на каждый вызов.\nРезультаты тестирования Я тестировал быстродействие сэмплеров на трех распределениях, с параметрами, близким к используемым в моём проекте:\n Бета распределение с параметрами $\\alpha=1$, $\\beta=100$ Распределение Пуассона с параметром $\\lambda=10$ Нормальное распределение со стандартными параметрами $\\mu=0$, $\\sigma=1$, тестировать с другими смысла нет, т.к. такие сэмплы получаются из стандартных простым умножением.\n  Для cupy запускалось два варианта тестов, один с копированием результатов с GPU на хост, второй без копирования (т.к. результаты могут обрабатываться и на GPU). Вариант без копирования называется cupy_nht (No Host Copy). Разница между ними до 2х и более раз. GPU-тесты PyTorch запускались всегда с копированием на хост.\nТесты проводились с переменным размером батча (т.е. сколько сэмплов генерируется за один вызов), варьирующимся от десяти до миллиона. Быстродействие всех библиотек сильно зависит от этого размера.\nРезультаты приводились к количеству сэмплов, генерируемому в секунду. Числа в таблицах измеряются в миллионах сэмплов в секунду.\nБета распределение      batch scipy numpy mkl cupy cupy_nht torch_cpu torch_gpu pymc3 tf_cpu     10 0.2 4.2 6.1 0.1 0.2 0.3 0.1 0.1 0.1   35 0.8 7.6 20.3 0.4 0.6 0.7 0.4 0.4 0.3   129 2.4 10.0 57.6 1.4 2.2 1.3 1.3 1.3 0.9   464 5.6 11.2 137.7 5.1 7.9 1.6 4.6 3.5 2.2   1668 9.0 11.6 218.1 18.1 29.1 1.8 16.3 7.1 5.5   5994 10.8 11.8 252.8 61.1 99.4 1.8 57.4 9.8 16.0   21544 11.4 11.6 267.3 160.7 367.4 1.9 123.1 11.1 30.5   77426 11.6 11.7 254.9 302.9 816.6 1.9 129.4 11.5 35.7   278255 11.4 11.7 258.7 456.3 1005.3 1.9 149.1 11.7 40.9   1000000 11.2 11.7 266.1 551.0 968.3 1.8 159.2 11.8 42.9    Распределение Пуассона      batch scipy numpy mkl cupy cupy_nht torch_cpu torch_gpu pymc3 tf_cpu     10 0.2 3.5 5.2 0.1 0.2 1.3 0.3 0.1 0.1   35 0.7 5.6 17.8 0.4 0.7 3.1 1.0 0.4 0.3   129 2.0 6.8 60.6 1.5 2.5 5.2 3.5 1.3 1.1   464 4.2 7.3 158.4 5.2 8.8 6.6 11.8 2.9 2.8   1668 6.0 7.5 328.9 18.2 31.8 7.0 41.5 4.9 7.5   5994 6.8 7.5 489.0 55.4 113.1 7.2 129.1 6.4 21.2   21544 7.2 7.6 520.4 119.1 227.5 7.3 365.1 7.0 47.6   77426 7.5 7.6 507.3 169.2 344.1 7.3 732.6 7.4 66.4   278255 7.6 7.6 512.9 209.9 294.7 7.3 983.8 7.5 74.7   1000000 7.5 7.6 531.4 229.7 282.9 7.3 1346.7 7.5 84.7    Нормальное распределение      batch scipy numpy mkl cupy cupy_nht torch_cpu torch_gpu pymc3 tf_cpu     10 0.3 4.0 6.3 0.0 0.1 1.2 0.2 0.1 0.1   35 1.1 10.6 21.3 0.2 0.2 4.1 0.8 0.4 0.3   129 3.4 19.7 61.4 0.6 0.7 13.5 2.9 1.5 1.2   464 9.5 26.9 154.6 2.1 2.5 34.8 10.3 4.5 4.1   1668 19.4 30.2 269.3 7.3 8.9 62.9 36.3 11.6 11.6   5994 28.0 31.5 337.7 25.7 31.9 83.4 122.1 21.9 26.5   21544 31.7 31.9 364.5 85.2 114.5 91.5 362.5 28.8 83.3   77426 32.8 32.0 350.2 246.8 409.4 119.4 855.8 31.6 211.0   278255 34.3 32.1 356.4 596.5 1389.0 122.6 1333.1 34.0 327.0   1000000 34.4 32.5 371.9 921.5 3521.7 126.7 2049.5 33.9 366.5    Тестирование проводилось на CPU i7-6850K (Broadwell E, 6 cores, 12 threads) и GPU GTX1080Ti.\nКод, всех тестов доступен в GitHub: https://gist.github.com/Arturus/dd5397e5cba3e4c05745811371406d83\n","date":1544385600,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1544385600,"objectID":"833f72f0007d06a33705ba4506eeb13d","permalink":"https://suilin.ru/post/sampling_performance/","publishdate":"2018-12-10T00:00:00+04:00","relpermalink":"/post/sampling_performance/","section":"post","summary":"th { text-align: right; }  В ходе работы над последним проектом мне понадобилось в промышленных масштабах генерировать сэмплы из бета-распределения. Первое что пришло в голову, это scipy.stat, тем более там есть куча дополнительных возможностей: и CDF, и квантили, и MLE, и всё, что душа пожелает. Но довольно быстро я понял, что scipy нетороплив, и генерация нескольких миллиардов сэмплов затянется на часы, а то и на дни. Стал искать альтернативные варианты и хочу теперь поделиться найденным.","tags":["sampling","GPU","distribution","numpy","MKL","scipy","Pytorch","Tensorflow","PyMC3"],"title":"Эффективное сэмплирование распределений на Python","type":"post"},{"authors":null,"categories":null,"content":" Аудиторию блогера в Инстаграм можно разделить на активную (те, кто лайкают посты) и пассивную (те, кто подписан, но не ставит лайки). Для рекламодателей активная аудитория особенно интересна, поэтому её свойства (соцдем, интересы и т.п.) лучше определять отдельно. Чтобы выделить активную аудиторию, очевидно, необходимо получить лайки к постам. Проблема в том, что у крупных блогеров могут быть миллионы лайков, а Инстаграм за один запрос отдаёт весьма ограниченное количество последних лайков, поэтому получение всех лайков от постов всех блогеров технически затруднительно.\nРазумная альтернатива это сэмплирование лайков. Если взять, например сэмпл в 10000 лайков, то свойства аудитории, рассчитанные по этому сэмплу, будут не сильно отличаться от свойств аудитории, посчитанных по миллиону лайков. Но для этого сэмплирование должно быть равномерным, то есть у всех \u0026ldquo;лайкеров\u0026rdquo; должны быть примерно одинаковые шансы попасть в сэмпл. Равномерный сэмпл можно получить, если забирать последние лайки поста через некоторые интервалы времени. Но здесь возникает другая проблема: пост собирает основную часть лайков в первые несколько часов своей жизни. Поэтому если не успеть вовремя обнаружить новый пост, основная масса лайков уже не будет доступна, можно будет делать сэмпл только из \u0026ldquo;хвоста\u0026rdquo; лайкеров, что приведёт к искажению статистики. Опоздав всего на час, мы пропустим 20-30% всех лайков.\nСамый простой вариант борьбы с опозданиями, это проверять, не появились ли новые посты, почаще, например раз в 10 минут. Но этот же вариант и самый неэкономичный по кол-ву запросов в Инстаграм. Блогер вряд ли будет делать посты, когда спит, возможно у него есть любимое время для постинга, любимые дни недели, и т.п. Если понять индивидуальное \u0026ldquo;расписание\u0026rdquo; каждого блоггера и подстроить под него наши проверки, можно будет получить хорошую экономию запросов.\nКак постят блоггеры Чтобы понять, есть ли у нас шансы выявить поведенческие паттерны блоггеров, визуализируем некоторые имеющиеся данные.   Распределение интервалов между постами, I  \nХорошо видны пики 24-48-72 часа, соответствующие постингу раз в сутки/двое/трое в одно и то же время. То есть у многих блогеров есть \u0026ldquo;любимое\u0026rdquo; время постинга, и интервал между их постами получается кратным 24 часам.\n  Распределение интервалов между постами, II   На меньшем масштабе заметны пики, соответствующие постингу раз в час или в несколько часов. Возможно, это следы работы сервисов отложенного постинга.\n  Количество постов по времени суток, по всему Инстаграму.   Хорошо заметны внутрисуточные подъемы и спады активности, связанные с естественными суточными ритмами \u0026ldquo;сон-бодрствование\u0026rdquo; и разной активностью в рабочее и нерабочее время.\nАудитория Инстаграма интернациональна и распределена по всему миру по разным часовым поясам. Чтобы лучше понять природу подъемов и спадов, визуализируем аудиторию из разных стран по отдельности:\n  Количество постов по времени суток, раздельно по странам.   На графиках отдельных стран суточная цикличность выражена еще более явно. Также в некоторых странах заметно нарастание активности до начала рабочего дня и после его окончания.\nДетализированные паттерны постинга Визуальный анализ общей активности показывает, что посты распределяются внутри суток неравномерно, есть определенные закономерности. А сейчас от общего анализа перейдем к более подробной детализации, и посмотрим, как делают посты индивидуальные блоггеры.\nСтабильные блогеры Сначала посмотрим на \u0026ldquo;стабильных\u0026rdquo; блогеров, у которых минимальна дисперсия интервалов между постами. Видно, что такие блогеры делают посты в основном строго раз в 24 часа, с небольшими отклонениями:  \nНестабильные блогеры На следующем графике \u0026ndash; \u0026ldquo;нестабильные\u0026rdquo; блогеры, у которых высокая вариативность интервалов между постами. В основном это те, кто делает постинг пакетами по 2-3 поста, с большой паузой между пакетами. Видно, как интервал между постами циклически изменяется от секунд до нескольких суток:  \nБыстрые блоггеры На этом графике блоггеры с минимальным интервалом между постами, т.е. создающие много постов в час. В основном это аккаунты магазинов, выкладывающие в Instagram свой товарный каталог. Видны регулярные паузы между постингами (единичные всплески вверх), по видимому они постят только в рабочее время, а в нерабочее и по выходным - отдыхают.  \nМедленные блоггеры На этом графике блоггеры с максимальным интервалом между постами, т.е. с редкими постами. Каких либо явных закономерностей здесь не прослеживается:  \nИтак, мы увидели, что с одной стороны в posting patterns есть множество явных закономерностей, с другой стороны у разных блогеров эти закономерности разные, а у многих вообще отсутствуют. Создать вручную набор правил, который одинаково хорошо подходил бы любому блогеру, не представляется возможным. Решить эту задачу за разумное время можно только с помощью машинного обучения.\nПробуем machine learning Немного теории Первый вопрос, который надо задать при разработке machine learning модели \u0026ndash; что мы, собственно, хотим предсказать? С первого взгляда кажется, что надо предсказывать время следующего поста. То есть наша модель должна обучиться функции: $$t_{i+1} = f(t_0, \\dots, t_{i})$$\nгде $t_0, \\dots, t_{i}$ это история времени предыдущих постов, а $t_{i+1}$ это время следующего поста, которое мы хотим предсказать.\nНо на самом деле это плохая идея. Предсказывать точное время следующего поста это все равно, что предсказывать итог бросания монетки. Если монета честная, то мы знаем, что в среднем итог бросков будет 50/50. Но предсказать итог каждого конкретного броска невозможно. Та же ситуация с блогером: допустим, мы знаем, что он предпочитает делать посты по вечерам. Но в конкретный день он может не сделать пост, потому что находится в поездке, или сделать пост позже обычного, потому что у него были другие дела, или наоборот сделать сразу два поста. Чтобы предсказывать точное время, нам потребовалось бы знать множество фактов из жизни блогера, которых у нас нет.\nПоэтому лучше предсказывать вероятность того, что блогер сделает пост в определённом интервале времени. Вероятность единичных событий, происходящих на фиксированном отрезке времени, в простейшем случае описывается распределением Пуассона: $$\\Pr(k)=\\frac{\\lambda^k}{k!}e^{\\lambda}$$\n $k$ \u0026ndash; наблюдаемое количество событий за единицу времени (в нашем случае события это посты, а за единицу времени можно взять, например, сутки); $\\lambda$ \u0026ndash; математическое ожидание количества событий за единицу времени, т.е. среднее количество событий. Этот параметр еще называют интенсивностью.  Допустим, блогер делает в среднем 3 поста в сутки ($\\lambda=3$). Тогда, согласно распределению Пуассона, у нас получаются следующие вероятности увидеть $k$ постов за один конкретный день:\n вероятность увидеть 0 постов (т.е. ни одного): $$\\Pr(k=0)=\\frac{2^0}{0!}e^{-3}=\\frac{1}{1}e^{-3}\\approx0.05$$ вероятность увидеть 1 пост: $$\\Pr(k=1)=\\frac{3^1}{1!}e^{-3}=\\frac{3}{1}e^{-3}\\approx0.15$$  остальные значения выведены на график:  \nЕсли бы блоггеры всегда делали посты с одинаковой интенсивностью, т.е. соблюдалось бы условие $\\lambda=const$, то на этом можно было бы закончить наш анализ, и даже не понадобилось бы машинное обучение. Но в реальной жизни интенсивность все время изменяется. Блогер может открыть для себя новую тематику, вдохновиться ею, и начать делать посты в несколько раз чаще, чем обычно. Или наоборот, блогер может забросить свой аккаунт, переключившись на что-то другое, или уехать в отпуск, и вообще перестать делать посты. В этом случае интенсивность будет стремиться к нулю. Таким образом, в реальной жизни это не константа, а функция от времени:\n$$\\lambda=f(t)$$\nНаша задача \u0026ndash; найти эту функцию, тогда мы сможем оценить вероятность появления нового поста в произвольный момент времени, и смоделировать поведение блогеров.\nОт теории к практике Построим модель, которая обучится целевой функции: $$\\lambda=f(t, h)$$ где $h$ \u0026ndash; история предыдущих постов блогера, $t$ \u0026ndash; относительное время, прошедшее с момента последнего поста.\nБудем предсказывать интенсивность для следующих 24 часов после момента времени $t$. Для обучения необходимо задать loss function, показывающую, насколько хорошо работают наши предсказания. Будем использовать negative log likelihood:\n$$loss=-\\log(Pr_{\\lambda}(X=x\\mid t))$$ это вероятность наблюдения в течение 24 следующих часов после момента времени $t$ количества постов, равного $x$, для распределения Пуассона, характеризуемого параметром $\\lambda$. Количество постов берется из реальных данных. Чем точнее мы предсказали значение $\\lambda$, тем выше будет вычисленная по реальному количеству постов вероятность, и тем меньше будет loss.\nДля обучения будем использовать deep learning model, состоящую из Recurrent Neural Network и нескольких fully connected layers. На входы RNN подаётся история предыдущих постов блогера, на вход fully connected layers блока \u0026ndash; состояния на выходе из RNN и время $t$.\nПосмотрим, что получается в результате обучения, на примерах постов отдельных блоггеров. Голубым обозначена предсказанная интенсивность, оранжевыми треугольниками и вертикальными линиями - моменты времени, когда происходили посты. В идеале в те моменты, когда происходил постинг, предсказанная интенсивность должна быть высокой, а в моменты, когда постов не было \u0026ndash; низкой:\n  Видно, что при повышении частоты постов предсказанное значение параметра $\\lambda$, как и ожидалось, растет, а при отсутствии постов падает. Когда блогер перестал делать новые посты, значение $\\lambda$ упало практически до нуля. В момент, когда происходит новый пост после долгой паузы, значение $\\lambda$ скачкообразно повышается, т.к. модель видит, что блогер still alive, и начинает ожидать от него потока новых постов.\nФактически, мы смоделировали самовозбуждающийся Hawkes process, не используя при этом никакой сложной параметризованной математики \u0026ndash; deep learning модель сама обучилась всем закономерностям!\n  На втором примере модель, пронаблюдав историю постов в первые три месяца, выявила \u0026ldquo;любимые дни\u0026rdquo; блогера, когда он делает посты. Соответственно, предсказанная интенсивность начинает повышаться заранее, в ожидании того, что блогер скоро сделает пост. Даже когда постов нет, ожидаемая интенсивность все равно циклически повышается и понижается.\n   На третьем примере моделью тоже выявлена недельная сезонность, но немного другого вида: блогер обычно не делает посты по выходным. В течение рабочей недели примерно одинаковая высокая интенсивность, по выходным интенсивность падает.\nКак видим, наша модель вполне способна выявить поведенческие паттерны блогера, и предсказывать вероятность постов во времени в соответствии с этими паттернами.\nМодель для реального применения Модель, предсказывающая интенсивность (т.е. параметр $\\lambda$ для распределения Пуассона), хороша с теоретической точки зрения. Но она не дает прямого ответа на вопрос, который нас интересует на практике: когда надо проверять, не появился ли новый пост? Чтобы получить ответ на этот вопрос, необходимо проинтегрировать функцию интенсивности:\n$$\\Lambda=\\int_{t_{0}}^{t\u0026rsquo;} \\mathrm\\lambda(t)\\,\\mathrm{d}t$$\nгде $t_0$ \u0026ndash; текущее время, $t\u0026rsquo;$ \u0026ndash; предполагаемое время проверки.\nСначала надо положить $t\u0026rsquo;=t_0$, и потом постепенно увеличивать $t\u0026rsquo;$, пока вычисленное значение интеграла не станет больше некоторого заранее выбранного порога. Понятно, что вычисление интеграла числовой аппроксимацией по отдельным точкам, в которых модель рассчитала предсказания для $\\lambda$, это неточная, неудобная и ресурсоемкая процедура. Поэтому для реального использования лучше сделать другую модель, которая сразу будет предлагать время проверки.\nКак и с предыдущей моделью, нам надо определить, какая будет loss function. Необходимо соблюсти одновременно два условия, противоречащие друг другу: с одной стороны, надо делать проверки как можно реже, чтобы не генерировать большое кол-во запросов к Instagram. С другой стороны, надо минимизировать опоздания, т.е. обнаруживать пост в тот момент, когда он еще не накопил много лайков. Для минимизации опозданий надо наоборот проверять как можно чаще. Наша loss function должна выражать баланс между этими двумя условиями.\nДля начала разберемся с тем, как оценивать опоздания. Рост количества лайков в посте происходит нелинейно, вначале он очень быстрый, потом затухает, и к истечению двух дней с момента публикации поста рост практически прекращается. Прирост лайков в разных постах можно визуализировать таким суммарным графиком:  \nПриблизительно смоделировать зависимость кол-ва лайков от времени можно формулой: $$likes=1-e^{-\\left(\\frac{t}{\\alpha}\\right)^{\\beta}}$$ где $t$ \u0026ndash; время в часах; $\\alpha, \\beta$ \u0026ndash; подбираемые эмпирические коэффициенты. Для нашего случая $\\alpha=4.2, \\beta=0.7$. Смоделированные значения показаны голубой линией на графике. Используя эту формулу, мы можем оценить опоздание, как долю от общего кол-ва лайков, которую мы пропустили. Таким образом, опоздание будет величиной в интервале $[0,1]$. Это первая часть нашей loss function.\nВторой частью будет оценка частоты проверок \u0026ndash; инвертированная длина предсказанного интервала между текущим временем и временем следующей проверки. Чем длинее эти интервалы, тем меньше будет проверок. Итоговый loss складывается из потерь по лайкам $loss_{l}$ и потерь по опозданиям $loss_{f}$:\n$$loss = loss_{l} + k \\cdot loss_{f}$$\nгде $k$ это коэффициент, регулирующий баланс между частотой проверок и размером опозданий. Выставляется вручную, исходя из бизнес соображений: какой есть бюджет на проверки и насколько критичны опоздания. При увеличении коэффициента частота проверок снижается и опоздания растут, при уменьшении наоборот.\n$$loss_{f} = \\hat{t}^{-1}$$ $$loss_{l} = \\sum_{i=1}^{n} loss_{p_i}$$ $$loss_{p_i} = \\begin{cases} 1-\\exp\\left(-\\left(\\frac{\\hat{t}-t^{post}_i}{\\alpha}\\right)^{\\beta}\\right) \u0026amp; \\quad \\text{if } \\hat{t}-t^{post}_i \u0026gt; 0\\\\\n0 \u0026amp; \\quad \\text{otherwise} \\end{cases} $$\n $\\hat{t}$ \u0026ndash; предсказанный временной интервал от текущего времени до следующей проверки. $n$ \u0026ndash; количество будущих постов, т.е. постов, будут сделаны после текущего времени. $t^{post}_i$ \u0026ndash; временной интервал от текущего времени до $i$-го будущего постоа. $loss_{p_i}$ \u0026ndash; потери по лайкам для каждого будущего поста. Потери учитываются, только если произошло опоздание, т.е. для поста выполнятся условие $\\hat{t}-t^{post}_i \u0026gt; 0$, в противном случае потери равны нулю. $\\alpha, \\beta$ \u0026ndash; коэффициенты для моделирования динамики лайков, о которых говорилось выше.  Во время обучения будем каждый раз выбирать случайным образом момент текущего времени внутри истории постов аккаунта, и предсказывать время следующей проверки относительно этого текущего времени. Таким образом, после достаточно длительного обучения, мы попадем почти в каждый интервал между постами, и сделаем предсказания для множества разных мест в истории.\nРезультаты Посмотрим, какие результаты дает обученная модель. Будем визуализировать на timeline одновременно и реальные посты, и точки проверок, предсказанные нашей моделью. В идеале, если бы нам было известно, когда блогер сделает пост, каждая проверка была бы сразу после появления нового поста, а в интервалах между постами проверок не было бы вообще, или они были бы очень редкими. Но этот идеал, как уже обсуждалось в начале этой статьи, недостижим. С другой стороны, можно вообще не применять никаких моделей, и просто проверять, например раз в час, не появились ли у блогера новые посты. Тогда проверки будут равномерно распределены в интервалах между постами, но будет много лишних проверок.\nПроверки, полученные с помощью нашей модели, должны быть где-то между этими двумя крайними случаями. Т.е. на временных интервалах, где вероятность нового поста мала, проверки должны быть редкими, а на интервалах, где велика вероятность обнаружить новый пост, проверки должны быть более частыми.\nБудем отображать проверки в виде светло-зеленых маленьких точек, а посты в виде более крупных точек, окрашенных в цветовой шкале от синего до желтого, в зависимости от того, какое количество лайков мы пропустили из-за опоздания:  \nТакже надо помнить, что лайки нарастают очень быстро, при опоздании всего на 10 минут мы пропускаем 10% лайков, при опоздании на полчаса – 20% лайков, при опоздании на час – 30% лайков.\nПосмотрим на предсказания нашей модели для блогеров из тестовой выборки (т.е. блоггеров, которых модель не видела во время обучения):  \nНа первой диаграмме показана вся история одного блогера. По оси Y - интервалы между проверками, т.е. чем выше зеленая точка, тем больше интервал. Видно, что в начале истории модель адаптируется к поведению блогера, наблюдаются довольно сильные опоздания, и короткие интервалы между проверками. Потом, по мере накопления информации о привычках блогера, проверки становятся более редкими и более точными.\n   На второй диаграмме показан участок истории того же блогера, когда модель вышла на стабильный режим работы. Видна явная суточная цикличность, проверки ночью делаются намного реже, проверки днем - чаще. Время постов совпадает с периодами наиболее частых проверок, т.е. наша модель успешно обучилась.\nРассмотрим ещё один аккаунт. На первом графике видно, как модель постепенно увеличивает интервал между проверками (зеленые точки уходят выше), если блогер не делает новых постов. Действительно, зачем часто проверять аккаунт, если он ничего не постит?     На втором графике видно, как модель адаптируется к изменяющемуся поведению блоггера. Сначала блогер делал один пост в день, и на диаграмме частоты проверок видно явное увеличение частоты в середине дня, когда вероятность поста максимальна. Потом блоггер начинает делать два поста в день, и хорошо видно, как проверки быстро адаптируются к новому поведению: вместо одной впадины, соответствующей уменьшению интервала между проверками в середине дня, мы начинаем наблюдать в правой половине графика \u0026ldquo;площадки\u0026rdquo;, соответствующие равномерно уменьшенному интервалу в течение всего дня.\nУспешная работа модели подтверждается не только визуально, но и цифрами. Если интервалы между проверками задаются нашей моделью, то для получения того же среднего процента пропущенных лайков (~15%) требуется сделать в 2-4 раза меньше проверок по сравнению с baseline. За baseline принимаются равномерные проверки раз в N минут. Если же наоборот зафиксировать кол-во проверок и сравнивать процент пропущенных лайков, то у модели он будет в 1.5-2 раза меньше, чем у baseline.\n","date":1543262400,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1543262400,"objectID":"aa2052af661083f2fac80a8d7a63b446","permalink":"https://suilin.ru/project/post_time/","publishdate":"2018-11-27T00:00:00+04:00","relpermalink":"/project/post_time/","section":"project","summary":"Аудиторию блогера в Инстаграм можно разделить на активную (те, кто лайкают посты) и пассивную (те, кто подписан, но не ставит лайки). Для рекламодателей активная аудитория особенно интересна, поэтому её свойства (соцдем, интересы и т.п.) лучше определять отдельно. Чтобы выделить активную аудиторию, очевидно, необходимо получить лайки к постам. Проблема в том, что у крупных блогеров могут быть миллионы лайков, а Инстаграм за один запрос отдаёт весьма ограниченное количество последних лайков, поэтому получение всех лайков от постов всех блогеров технически затруднительно.","tags":["Instagram","Deep.Social"],"title":"Предсказание времени поста в Instagram","type":"project"},{"authors":null,"categories":["Performance","Programming"],"content":" Введение В Pandas существует по меньшей мере три официальных способа добавить колонку, не включая экзотических:\nСпособ №1\nimport pandas as pd df = pd.DataFrame(...) df['column'] = value  У этого способа самый простой и очевидный синтаксис, поэтому по умолчанию обычно используют именно его. Но наверняка каждый, кто работал с Pandas, получал хотя бы раз в жизни такой неприятный warning при добавлении колонки:\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  Этот warning говорит нам, что существует второй способ.\nСпособ №2\ndf.loc[:, 'column'] = value  Откуда же берется warning в первом способе? Он возникает, когда выполняется несколько выборок идущих друг за другом, причем на вход следующей выборки подаются результаты предыдущей выборки. В терминологии Pandas это называется chained indexing и выглядит например так:\n# Выборка по строкам, потом по колонкам df[df['a'] \u0026gt; 5][['b', 'c']] # Выборка по колонкам, потом по строкам df[['b', 'c']][df['a'] \u0026gt; 5]  Если попытаться модифицировать результаты chained indexing (добавление колонки это тоже модификация), то Pandas не поймет, что мы хотим - добавить колонку в результаты выборки, или добавить колонку в исходный фрейм? Оба примера, приведенные ниже, эквивалентны с точки зрения Pandas:\n# Добавить колонку 'b' к исходному фрейму? df[df['a'] \u0026gt; 5]['b'] = 42 # Или к результатам выборки? df1 = df[df['a'] \u0026gt; 5] df1['b'] = 42  Чтобы выдать SettingWithCopyWarning, Pandas запоминает источник данных для каждого фрейма, \u0026lsquo;родительский\u0026rsquo; фрейм. Если такой источник существует, т.е. фрейм является подмножеством данных родительского фрейма, то в момент модификации выдается warning.\nВторой способ позволяет нам более явным образом сообщить о своих намерениях, т.к. даёт совместить выборку и присваивание в одном выражении.\nБолее подробно о премудростях chained indexing можно прочитать в документации Pandas или в отличной статье на Medium.\nСпособ №3\nresult = df.assign(column=value)  Третий способ не модифицирует исходный фрейм, что в зависимости от ситуации может быть как плюсом (например при повторном выполнении ячейки в Ipython Notebook), так и минусом, загромождая код присваиваниями. Кроме того, при выполнении assign() всегда происходит создание нового фрейма, что теоретически должно быть немного медленнее, чем предыдущие in-place способы.\nНаличие нескольких способов сделать одну и ту же простую задачу противоречит известному принципу Zen of Python :\n There should be one—and preferably only one—obvious way to do it.\n И как оказалось, проблема здесь не только в нарушении философского принципа.\nПроблема Я давно замечал, что при активном добавлении колонок во фреймы код начинает работать подозрительно медленно. Под активным я имею в виду сотни и тысячи добавлений - такие задачи встречаются, когда данные надо разбить на много мелких групп и работать с каждой отдельно. Использование третьего способа, через assign() обычно ускоряло такой код, хотя теоретически он должен работать медленнее двух первых - я списывал это на то, что мне просто показалось, и никогда не делал точных замеров.\nНо на последней задаче эта проблема проявилась особенно остро. Скрипт, который должен был пропустить через себя примерно 100Gb данных, и довольно бодро стартовавший с прогнозом времени выполнения 3 часа, был оставлен на ночь. К утру скрипт не выполнил и 20% работы и почти завис, потребляя при этом 100% CPU. В чём же дело?\nЗапуск скрипта под cProfile выявил занятную картину: основную часть времени процесс находится внутри метода gc.collect(), при том, что я нигде не вызываю сборщик мусора. Такое поведение было бы объяснимым для виртуальной машины Java, работающей в условиях нехватки памяти, тогда бы сборщик мусора активировался на каждый чих. Но Python?\nПришлось поглубже залезть в трассировку вызовов\u0026hellip; и следы привели к коду, добавляющему колонки в dataframe! Вот фрагмент кода метода DataFrame._check_setitem_copy(), занимающегося проверкой при добавлении колонки, и выдающего тот самый SettingWithCopyWarning, о котором говорилось выше :\nif force or self._is_copy: value = config.get_option('mode.chained_assignment') if value is None: return # see if the copy is not actually referred; if so, then dissolve # the copy weakref try: gc.collect(2) if not gc.get_referents(self._is_copy()): self._is_copy = None return except Exception: pass  В поле self._is_copy хранится weak reference на объект, являющийся \u0026lsquo;родителем\u0026rsquo; текущего фрейма. Чтобы проверить, жив ли еще родитель, авторы Pandas не нашли лучшего способа, чем просто запустить сборку мусора во всей виртуальной машине 😟\nНа тестах, когда в памяти не очень много объектов, сборка мусора отрабатывает практически мгновенно и код не вызывает никаких нареканий. В моём же случае в памяти было закешировано около 10Gb данных, и сборщику мусора приходилось изрядно потрудиться, обходя все эти объекты при каждом добавлении колонки во фрейм.\nРешение Решение было простым - раз блок кода со сборкой мусора исполняется только при наличии \u0026lsquo;родителя\u0026rsquo;, надо сделать так, чтобы родителя не было. Я просто добавил вызов copy() перед тем местом, где добавляется колонка. После copy() фрейм считается \u0026lsquo;заново рождённым\u0026rsquo;, и не содержит ссылок на источник данных:\ndf = df.copy() df['column'] = value  Скрипт сразу заработал намного быстрее, и завершился всего за час 🎉\nОтмечу, что тормоза были одинаковыми при использовании и первого и второго способа добавления колонки, что неудивительно, т.к. оба они вызывают эту проверку. А что же третий способ, assign()? Посмотрим на его код, он очень простой (привожу только ветку для Python 3.6):\ndef assign(self, **kwargs): data = self.copy() for k, v in kwargs.items(): data[k] = com._apply_if_callable(v, data) return data  Как видно, этот код делает ровно то, что я сделал вручную, ускоряя свой скрипт: сначала копирует фрейм, а потом добавляет в него колонки дедовским способом. Именно поэтому использование assign(), вопреки логике, всегда ускоряло работу.\nВыводы Для пользователей Pandas вывод простой: надёжнее всего использовать assign(), и со стороны performance, и со стороны того, что он ограждает пользователя от side effects, связанных с необратимым изменением фрейма. Автор статьи, которую я рекомендовал выше, приходит к тем же выводам. Всегда, когда надо присвоить что-то фрейму, перед присваиванием лучше вызвать df.copy(), чтобы избежать неоднозначностей. И, как показывает мой пример, еще и получить прибавку к скорости!\nА разработчикам Pandas хорошо бы или найти способ отказаться от такой brute-force проверки, или хотя бы отразить её наличие в документации.\n","date":1539979200,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1539979200,"objectID":"a918495836ef210fbb6290744c812154","permalink":"https://suilin.ru/post/pandas_column/","publishdate":"2018-10-20T00:00:00+04:00","relpermalink":"/post/pandas_column/","section":"post","summary":"...и остаться в живых? В Pandas существует по меньшей мере три официальных способа добавить колонку, не включая экзотических. Способ №1:","tags":["Pandas","Python"],"title":"Как добавить колонку к pd.DataFrame","type":"post"},{"authors":null,"categories":null,"content":" Введение Для рекламодателей, использующих influencer marketing, важно понимать, у какого блогера аудитория наиболее соответствует рекламируемым товарам и услугам. Довольно бессмысленно рекламировать деловые костюмы девочкам-подросткам, так же как и продвигать женскую косметику среди аудитории мужчин за 30.\nНо сам Instagram не предоставляет никакой соцдем информации по аудитории блогера, поэтому рекламодателям приходится работать с блогерами исключительно на основе своих предположений о составе их аудитории. Единственный способ подтвердить эти предположения \u0026ndash; просмотреть выборку из фолловеров интересующего блогера и оценить на глаз их возраст и пол. Это долгая, неинтересная, и немасштабируемая работа, к тому же не совсем объективная, т.к. разные люди оценят возраст по разному.\nНо почему бы не поручить эту работу машине? Современные технологии Computer Vision уже достаточно развиты, чтобы справиться с этой задачей без участия человека.\nПостановка задачи На входе есть аватары Instagram пользователей. Необходимо понять, есть ли на аватаре люди, и сколько их. Если изображён один человек, предсказать его возраст и пол. При этом масштаб изображения может быть разным: на аватаре может быть портрет или даже часть лица, может быть человек в полный рост, может быть что-то промежуточное. Фото может быть цветным, чёрно-белым, тонированным, пропущенным через искажающие фильтры, с дорисованными частями и т.п.\nЕстественным образом задача разделяется на две основные части:\n Обнаружение на фотографии людей Определение пола и возраста.  Обнаружение людей Пол и возраст определяется прежде всего по лицу, поэтому обнаружением человека считается наличие лица в кадре. Нога, рука или спина, хотя и говорят о наличии человека на фото, для решаемой задачи не подходят. Безусловно, можно обучить computer vision модель, которая будет отличать мужскую ногу от женской, но на покрытие всех таких ситуаций ушло бы слишком много ресурсов, при сравнительно небольшой отдаче \u0026ndash; всё таки лица изображены на аватарах гораздо чаще, чем ноги.\nОбнаружение лиц на фото это известная и хорошо проработанная задача computer vision, поэтому здесь работа свелась к поиску подходящей модели и адаптации её под требования проекта. Основными требованиями были приемлемая скорость работы (аудитория блогеров это сотни миллионов пользователей) и наличие уже обученного и готового к использованию варианта (чтобы не тратить время на разметку данных и обучение).\nОдним из дополнительных пожеланий было совмещение моделью двух функций: собственно нахождения лиц на фото, и определения опорных точек (face landmarks). Опорные точки это обычно центры глаз, кончик носа, углы губ и другие топологические точки, положение которых на лице может быть однозначно определено. Зачем они нужны?\nСвёрточные сети (convolutional networks), используемые в моделях компьютерного зрения, обладают свойством трансляционной инвариантности (translational invariance), но не обладают (или обладают в ограниченном объеме) свойствами масштабной инвариантности (scale invariance) и инвариантности к повороту (rotation invariance). Это означает, что если изображение одного и того же лица смещается на фото в разные положения, то с точки зрения нейросети это будет то же самое лицо (трансляционная инвариантность). Но если лицо поворачивается или изменяется его масштаб, для нейросети это будут разные лица. Поэтому, чтобы облегчить задачу для нейросети, лучше приводить все лица к единому масштабу и единому вертикальному положению, а для этого нужна привязка к надёжным опорным точкам (например, считать стандартным размером лица расстояние в 100 пикселей между горизонталью глаз и горизонталью губ, и приводить все лица к этому масштабу)   Инвариантность для свёрточных сетей  \nС учетом перечисленных требований, задача свелась к выбору из двух моделей: MTCNN[1] и детектора из библиотеки dlib: CNN Face detector + 5-point landmark detector.\nMTCNN vs dlib Обе модели продемонстрировали одинаково высокое качество обнаружения лиц, ошибаясь крайне редко. С определением опорных точек ошибок больше, особенно на лицах, сильно наклонённых, или повёрнутых ближе к положению \u0026ldquo;профиль\u0026rdquo;, чем к положению \u0026ldquo;анфас\u0026rdquo;. Но и здесь нет явного лидера. MTCNN более корректно определяет границы лица (bounding box), лучше распараллеливает обработку, плюс имеет хороший запас в коде для будущего ускорения, поэтому для дальнейшей работы была выбрана именно эта библиотека. Рассматривалась также библиотека face_alignment[2], она даёт даже избыточное дял данной задачи качество, но при этом медленно работает, и требует готовых bounding boxes.\n  Примеры определения границ лица и опорных точек библиотеками MTCNN (зелёный прямоугольник, зелёные точки), dlib (красный прямоугольник, фиолетовые точки), face-alignment (белые миниатюрные точки)     Примеры некорректной работы детектора. Первое фото - захват искусственного изображения вместо лица. Второе - некорректная работа dlib (фиолетовые точки) на лице, частично повернутом в анфас   Архитектура MTCNN MTCNN состоит из трех CNN-\u0026ldquo;стадий\u0026rdquo; P-Net, R-Net и O-Net, каждая из которых уточняет результаты предыдущей ступени, и предварительной стадии построения \u0026ldquo;пирамиды изображений\u0026rdquo;. Пирамида представляет из себя просто набор уменьшенных копий входного изображения. MTCNN заранее не знает, в каком масштабе будут лица на фото, а свёрточные сети, как уже говорилось выше, не инвариантны к масштабированию. Поэтому приходится готовить несколько версий входного изображения в разных масштабах, и искать лица на каждой версии.\nЗадача стадий - определить границы (bounding box) всех лиц на изображении. Первая стадий имеет самую простую архитектуру и работает очень быстро, но при этом генерирует много ошибок первого рода (false positives). Задача следующих стадий, более сложных и мощных \u0026ndash; выбрать из предложенных bounding boxes наиболее похожие на правду, уточнить их координаты, и передать дальше. Последняя ступень также определяет координаты опорных точек внутри результирующих bounding boxes. В конце каждой стадии сильно пересекающиеся друг с другом bounding boxes сливаются в один bounding box с помощью алгоритма NMS (non-max suppression):\n  Стадии работы MTCNN   Трёхстадийная архитектура позволяет MTCNN работать быстро, т.к. всю черновую работу делает простая первая ступень, а следующие занимаются только уточнением результатов. Как показали замеры, основное время уходит на построение пирамиды изображений, а не на собственно работу свёрточных сетей. Переход на более быстрые алгоритмы уменьшения изображений позволяет ещё в разы поднять производительность.\nНормализация лиц Перед тем, как отдавать найденные лица в детектор пола/возраста, их необходимо нормализовать, т.е. привести лица разного масштаба, по разному повёрнутые и наклонённые, к одному стандартному виду \u0026ldquo;как на паспорт\u0026rdquo;.\nСуществует довольно много методик нормализации, от продвинутых, натягивающих лицо как скин на 3D модель и манипулирующих этой моделью в пространстве, чтобы она смотрела прямо в объектив и заполняла весь кадр, находясь в его центре, до простых, ограничивающихся приведением лиц к близкому масштабу. Какую из них выбрать?\nЯ исходил из принципа минимального вмешательства: нормализация должна выдавать только естественную форму лица, которая встречается в природе. Если нормализация изменяет пропорции раздельно по осям X и Y, изменяет параллельность линий, или тем более натягивает лицо на 3D сетку, то она может принести больше вреда, чем пользы. Это подтверждают исследования[3] результатами которых я активно пользовался при работе над этим проектом.\nПринципу сохранения естественных пропорций лица соответствует преобразование подобия (similarity transform) , т.е. набор действий ограничивается сочетанием сдвига, вращения и масштабирования. Это преобразование является частным случаем аффинного преобразования и описывается c помощью следующей матрицы перехода : $$ \\mathbf{A} = \\begin{bmatrix} a_0 \u0026amp; b_0 \u0026amp; a_1 \\\\\nb_0 \u0026amp; a_0 \u0026amp; b_1 \\\\\n0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\n$$\\begin{bmatrix}x\u0026rsquo;\\\\y\u0026rsquo;\\\\1\\end{bmatrix}=\\mathbf{A}\\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix}$$\nили в скалярной форме: $$ x\u0026rsquo; = a_0x - b_0y + a_1 = sx\\cos(\\theta) - sy\\sin(\\theta) + a_1 $$ $$ y\u0026rsquo; = b_0x + a_0y + b1 = sx\\sin(\\theta) + sy\\cos(\\theta) + b_1 $$\nгде $(x,y)$ и ($x\u0026rsquo;,y\u0026rsquo;)$ \u0026ndash; координаты исходной и результирующей точек изображения, $\\theta$ \u0026ndash; угол поворота, $s$ \u0026ndash; коэффициент масштабирования.\nЧтобы найти коэффициенты для матрицы перехода, используются опорные точки. Есть координаты пяти опорных точек $(\\mathbf{s}_1 \\dots \\mathbf{s}_5),\\: \\mathbf{s}_i = [x_i, y_i, 1]^\\top$ \u0026ldquo;эталонного лица\u0026rdquo;, имеющего правильный масштаб, вертикальную ориентацию и расположенного в центре кадра. И есть координаты опорных точек на фото $(\\mathbf{a_1} \\dots \\mathbf{a_5})$, которые нашёл MTCNN. Задача \u0026ndash; найти такие коэффициенты для $\\mathbf{A}$, чтобы после преобразования точки c фото по возможности совпали с эталонными точками, т.е. минимизировать расстояние между эталоном и преобразованными точками с фото: $$\\operatorname*{arg\\,min}_\\mathbf{A} \\frac{1}{n}\\sum_{i=1}^n \\|\\mathbf{s}_i - \\mathbf{Aa}_i\\|$$ Эта задача решается с помощью метода Umeyama[4]\n  Примеры нормализации лиц, слева исходное фото, справа результат.   Определение пола и возраста Выбор модели Модель для определения пола и возраста подбиралась из примерно тех же критериев, что и модель для обнаружения лиц:\n Приемлемая скорость работы (и для обучения, и для предсказаний). Наличие pretrained модели, чтобы не обучать всё с нуля.  Рассматривались три архитектуры: ResNet[5], NASNet[6], MobileNet[7]. За baseline был взят хорошо известный ResNet-50.\nNASNet-A-Mobile-224, сконструированный с помощью \u0026ldquo;искусственного интеллекта\u0026rdquo;, содержит примерно в 5 раз параметров, чем ResNet-50. Но на практике он обучался не в 5 раз быстрее, а даже медленнее, чем ResNet-50.\nАналогично mobilenet-1.0-224, несмотря на то, что содержит в разы меньше параметров, чем ResNet-50, на практике обучался со скоростью, сопоставимой с ResNet-50, показывая при этом худшие результаты предсказаний. Видимо, эта архитектура имеет смысл именно для мобильных устройств, а не для стационарных GPU.\nВ итоге победил ResNet-50, как архитектура, оптимальным образом использующая стационарный GPU (обучение шло на видеокартах GTX 1080TI).\nАрхитектура На выходе надо получить два предсказания, пол и возраст. Можно обучить две отдельных модели, но эффективнее использовать multitask learning и получить одну модель, выдающую одновременно два прогноза. Тем более пол и возраст с точки зрения здравого смысла не являются независимыми переменными, женщины и мужчины взрослеют и стареют по разному: $$P_{gender}(D) \\neq P_{gender}(D|age)$$ $$P_{age}(D) \\neq P_{age}(D|gender)$$ таким образом знание моделью пола будет помогать предсказывать возраст и наоборот.\nПредсказание возраста это регрессия, для регрессионных задач в качестве целевой функции обычно используют MSE (среднеквадратичную ошибку от предсказываемой переменной). Но в данном случае прямое применение MSE не оправдано. Во-первых возраст не может быть отрицательным. Во-вторых, MSE предполагает линейную шкалу. В реальности разница в 5 лет между 2-x и 7-летними детьми это гораздо больше, чем разница в 5 лет между 60-летним и 65-летним человеком, т.е. человеческий возраст имеет скорее логарифмическую шкалу. Поэтому на входе в модель возраст трансформировался из исходного возраста $age$: $$a=\\log(age + \\gamma)$$ где $\\gamma$ это эмпирическая сглаживающая константа, чтобы разница между новорожденным и взрослым не устремилась в бесконечность. Модель внутри себя везде использует логарифмический возраст $a$, при выдаче результатов пользователю он переводится обратно в линейную шкалу: $$\\widehat{age}=\\exp(\\hat{a}) - \\gamma$$\nТак как определение возраста по фото это сложная задача, с которой даже люди далеко не всегда справляются, хотелось получить на выходе также оценку неопределённости (uncertainty estimation). Для этого можно предсказывать не точечную оценку возраста, а распределение вероятностей, каким он мог бы быть. Для простоты было принято, что ошибка определения возраста имеет Гауссовское распределение, т.е. $$\\hat{a} \\sim \\mathcal{N}(a, \\sigma^2)$$ где $\\hat{a}$ это предсказанный возраст, $a$ - истинный возраст, $\\sigma^2$ \u0026ndash; дисперсия, отражающая степень неуверенности в прогнозе.\nТогда за целевую функцию можно принять то, насколько хорошо предсказанное распределение соответствует истинному возрасту, т.е. функцию правдоподобия (likelihood): $$\\mathcal{L}(\\hat{a}, \\hat{\\sigma} | a) = \\prod_{i=1}^{n}\\frac{1}{\\hat{\\sigma}_i\\sqrt{2\\pi}}\\exp\\left(-\\frac{(a_i-\\hat{a}_i)^2}{2\\hat{\\sigma}_i^2}\\right)$$ Использование произведения по всем примерам из батча может привести к проблемам с точностью вычислений с плавающей точкой, поэтому на практике используют негативную логарифмическую функцию правдоподобия (negative log-likelihood): $$\\ell=\\log(\\mathcal{L})=-\\frac{n}{2}\\log(2\\pi)-\\sum_{i=1}^{n}\\log\\hat{\\sigma}_i-\\sum_{i=1}^{n}\\frac{(a_i-\\hat{a}_i)^2}{2\\hat{\\sigma}_i^2}$$ $$L_{age} = -\\ell$$\nОпределение пола является бинарной классификацией, в качестве целевой функции в модели используется стандартная для таких задач перекрестная энтропия: $$L_{gender} = -{(y\\log(p) + (1 - y)\\log(1 - p))}$$ где $y$ это бинарная метка класса, например 1 соответствует мужчине, 0 женщине, а $p$ это предсказанная вероятность принадлежать классу с меткой 1, в данном случае вероятность быть мужчиной.\nРезультирующая целевая функция является просто суммой функций по полу и по возрасту: $$L=L_{age} + kL_{gender}$$ Абсолютные значения целевых функций находятся в разных шкалах, например значение $L_{age}$ зависит от того, в каких единицах измеряется возраст. Чтобы привести их к более-менее одному масштабу, нужен выравнивающий коэффициент $k$, значение которого подбирается эмпирически.\nОбучающая выборка В идеале обучающая выборка должна быть из того же распределения, что и данные, на которых потом модель будет делать предсказания. Это означает ручную разметку фотографий из Instagram, т.к. достоверную информацию о возрасте владельцев аккаунтов взять негде.\nРучная разметка пола это в принципе посильная задача, а вот с возрастом всё не так просто. Люди определяют возраст на глаз крайне субъективно, это означает большую дисперсию и затруднительность контроля результатов. Для того, чтобы получить от работников надёжный результат, обычно одна и та же задача даётся трём-пяти людям, и за верный результат принимается большинство голосов. Работник, часто дающий ошибочные результаты, заменяется. Для возраста такая схема работать не будет, т.к. чтобы получить надёжную картину максимума распределения возрастов для каждого фото и отсеять выбросы, пришлось бы давать оценить каждое фото 10-20 людям, что было бы слишком затратно.\n  Распределение разницы между средним оценочным возрастом (по \u0026gt; 10 оценкам от разных людей) и реальным возрастом. Источник: AgeGuess database, J. A. Barthold Jones et al, 2018, arXiv:1803.10063   Оценка возраста людьми может сильно расходиться с реальным возрастом, см. приведённый рисунок. Кроме того, оценка возраста зависит еще и от национальности и культурного контекста оценщика, т.е. пришлось бы набирать распределённую по разным точкам мира команду.\nПоэтому были рассмотрены другие источники. Стандартный dataset, используемый в академических кругах для подобных задач, это IMDB-WIKI[8]\nОднако, качество разметки этого dataset-а, особенно в данных из IMDB, крайне низкое, и неприемлемо для проекта, который будет использоваться в production.\nОстальные доступные datasets: (Adience[9], UTKFace[10]) слишком малы для полноценного обучения.\nВ результате самым продуктивным оказался самостоятельный автоматизированный сбор данных из социальных сетей и Интернет-сайтов, с последующей модерацией. Но найти хорошие источники размеченных фотографий для возрастов \u0026lt;17 так и не удалось, поэтому для этой возрастной категории были выкачаны фото из Инстаграм, содержащие тэги, указывающие на возраст (обычно такие тэги бывают в фото с дней рождения). Пол в этих фото размечался вручную, релевантность содержимого фото (что на ней изображены именно дети) и разметки возраста (возраст с тэга совпадает с визуальным возрастом) контролировались командой модераторов.\nРабота по получению и разметке обучающих данных была самым долгим этапом проекта и заняла около 4-х месяцев.\nПодготовка данных В обучающей выборке выравнивалось количество мужчин и женщин из каждой страны (например в арабских странах и в Индии женщины представлены в онлайне слабо, и без выравнивания модель могла просто не научиться с ними работать).\nРаспределение количества фото по странам по возможности приводилось в соответствие с распределением кол-ва аккаунтов по странам в Instagram. При этом странам с преобладающим азиатским или негроидным населением давался больший вес, чтобы в обучающей выборке не доминировала европеоидная раса и у модели было достаточно данных, чтобы научиться работать с азиатскими и негроидными типами лиц.\nТакже убирались перекосы по возрастам, чтобы распределение возрастов было похожим на распределение, заявленное для Instagram в открытых источниках.\nВ итоговую выборку попало около 3 млн. фото взрослых и около 300 тыс. фото детей и подростков.\nАугментация данных Аугментация проводилась по тому же принципу, что и нормализация лиц: на выходе должны получаться только такие фото, которые могут встретиться в естественных условиях. Зашумленных, нерезких, тонированных и частично обрезанных фото было и так достаточно в обучающих данных, при этом качество фото на аватарах в Instagram в среднем достаточно высокое. Поэтому разновидности аугментации, \u0026ldquo;ухудшающие\u0026rdquo; фото, не применялись.\nАугментация свелась к вертикальному зеркалированию (flip), случайному повороту на небольшой угол и случайному кропу. MTCNN не абсолютно точно и одинаково определяет опорные точки на всех фото, от лица к лицу возможны вариации, два последних вида аугментации как раз учат нейросеть справляться с такими отклонениями.\nОбучение Использовалась модель ResNet-50, предварительно обученная на данных Imagenet. У неё убирался последний слой, отвечающий за классификацию ImageNet, и вместо него добавлялся полносвязный (fully connected, FC) слой, генерирующий предсказания для модели (3 выходных значения: пол, матожидание и дисперсия возраста).\nОбучение проводилось в два этапа, сначала обучался только добавленный FC слой, затем, когда ошибка переставала уменьшаться, в обучение включалась вся модель. Transfer learning, т.е. обучение только последнего слоя, сам по себе давал посредственные результаты: точность определения пола была не выше 85%. Это объяснимо, т.к. человеческие лица представляют собой довольно узкий и специфический домен, сильно отличающийся от данных ImageNet, к тому же в ImageNet не существует классов \u0026ldquo;человек\u0026rdquo; или \u0026ldquo;лицо\u0026rdquo;.\nИспользовался оптимизатор Nesterov Momentum и cosine learning rate decay с рестартами[11] Максимальный и минимальный learning rate подбирался с помощью техники LR range test [12]\n  Один из результатов LR range test, ось Y - loss, ось X - learning rate, логарифмическая шкала   Один полный прогон обучения занимал около пяти дней.\nРезультаты Результаты работы обученной модели сравнивались с результатами 4-х крупных коммерческих систем Computer Vision: AWS Rekognition, Microsoft Azure, Face++, Clarifai. Чтобы сравнение было объективным, замеры проводились не только на собственной тестовой выборке, но и на дополнительных datasets: Adience[9] и IMDB-WIKI[8].\nДля возраста точность оценивалась по ошибке MAPE (Mean absolute percentage error), показывающей отклонение в процентах определенного моделью возраста от реального.\n$$MAPE=\\frac{100\\%}{n}\\sum_{i=1}^{n}\\left|\\frac{age_i - \\widehat{age}_i}{age_i}\\right|$$\nгде $age$ \u0026ndash; истинный возраст, $\\widehat{age}$ - предсказанный возраст\nСобственный dataset Распределение возрастов соответствует естественному распределению в социальных сетях, откуда были собраны данные.\n   Service Gender accuracy, % Age MAPE, %     Ours 99.3 14.4   Face++ 92.2 59.3   Clarifai 84.8 47.7   Azure 96.9 34.2   AWS Rekognition 91.1 38.1    IMDB-Wiki Из IMDB-WIKI были отброшены данные IMDB, как содержащие огромное количество неточностей, в данных WIKI был дополнительно вручную исправлен пол там, где были явные ошибки. Для тестов были взяты фото людей в диапазоне возрастов 13-44 года (актуальный диапазон для Instagram). Также были отброшены фотографии, сделанные до 2005 года, т.к. стилистика этих фото (косметика, причёски) отличается от современной, и фото такой давности редко встречаются в соцсетях.\n   Service Gender accuracy, % Age MAPE, %     Ours 98.7 14.9   Face++ 92.6 36.6   Clarifai 91.9 35.9   Azure 89.4 24.6   AWS Rekognition 94.0 43.8    Adience Возраст в Adience указан в виде диапазона а не точного значения, поэтому для него использовались другие метрики точности:\n Age accuracy \u0026ndash; процент попаданий предсказанного возраста в правильный диапазон. Age accuracy one-off \u0026ndash; процент попаданий или в правильный диапазон или в два соседних с ним.     Service Gender accuracy, % Age acc., % Age acc. one-off, %     Ours 97.7 19.8 83.8   Face++ 86.3 14.8 65.1   Clarifai 84.6 25.7 78.4   Azure 94.8 36.9 89.8   AWS Rekognition 88.9 21.3 82.9    В этом тесте точность определения возраста у модели уступает некоторым коммерческим сервисам (на первом месте - Azure). Объясняется это тем, что Adience это академический dataset, в котором все возраста от 0 до 80 лет присутствуют в примерно равной пропорции. Модель же обучалась под распределение возрастов, наблюдающееся в реальной жизни в Instagram и социальных сетях, которое весьма далеко от равномерного (доминирует возраст 18-30). Соответственно, на равномерном распределении точность модели хуже, т.к. при прочих равных предпочтение отдаётся возрастам в диапазоне 18-30.\nЕсли бы целью было показать хороший результат именно на Adience, надо было бы обучить модель на выборке с равномерным сэмплированием по всем возрастам.\nНа что смотрит модель? Было бы интересно понять, какие области лица играют главную роль при определении пола/возраста. Большинство традиционных методы выявления областей внимания для этой модели, к сожалению, не выдают наглядных результатов, т.к. в результате нормализации лицо занимает практически весь кадр, и вся его площадь является активной областью. Самые наглядная визуализация получилась при использовании библиотеки SHAP[13] (метод DeepExplainer)   Активные зоны, влияющие на определение пола на мужском и женском лицах   Видно, что модель прежде всего обращает внимание на зоны, где возможна растительность на лице: область над губой, щёки. Для щек также вероятно важна структура кожи, более грубая у мужчин. Для женщин важны разрез глаз (и по видимому наличие там косметики) и форма подбородка. Для мужчин - форма и \u0026ldquo;кустистость\u0026rdquo; бровей.\nИз собственного практического опыта работы с этой моделью \u0026ndash; она смотрит примерно на те же признаки, что и человек, никакого сверхзнания у неё нет. Если показать модели фото мальчика, хорошо загримированного под девочку, модель выдаст ответ \u0026ldquo;девочка\u0026rdquo;, и наоборот. Трансгендеры и лица, не до конца определившиеся с выбором визуального пола, вызовут у модели затруднения при определении биологическго пола, такие же, как и у людей.\nЭволюция в результате обучения Ещё один вопрос, на который было интересно ответить \u0026ndash; насколько модель далеко ушла в своей эволюции от изначальной модели, обученной на изображениях ImageNet? Поскольку выразить \u0026ldquo;далеко\u0026rdquo; или \u0026ldquo;не очень\u0026rdquo; в виде числовой оценки затруднительно, лучше получить ответ в виде визуализации. Я использовал визуализацию каналов ResNet с помощью библиотеки Lucid. Суть этой визуализации в том, что с помощью оптимизации подбирается такое входное изображение, которое максимизирует ответ от канала. Содержимое этого изображения будет указывать, на какие паттерны во входном изображении реагирует данный канал.\nЕсли сравнить визуализации одних и тех же каналов в исходной сети и в сети после обучения на лицах, будет видно, как поменялось \u0026ldquo;восприятие\u0026rdquo; сетью изображений. Будем визуализировать избранные каналы в блоках ResNet от первых блоков, самых примитивных, обрабатывающих контуры и границы до последних, обрабатывающих целые визуальные объекты. Верхний ряд каждой визуализции это каналы модели, обученной распознаванию пола и возраста, нижний ряд - те же самые каналы исходной модели, обученной на ImageNet.\n   Визуализация паттернов в первом блоке ResNet, 6-й канал     Визуализация паттернов в первом блоке ResNet, 7-й канал   В первом блоке обрабатываются самые простые паттерны. Видно что паттерны для исходной и нашей модели не сильно отличаются. Тем не менее уже заметно, что паттерны для ImageNet (нижние ряды) имеют более сложную структуру.\n   Визуализация паттернов во втором блоке ResNet, 5-й канал   Во втором блоке паттерны усложнились, но всё еще похожи друг на друга. Заметно, что в паттернах ImageNet больше цветовое разнообразие, а паттерны нашей модели окрашены в цвета, близкие к телесным.\n   Визуализация паттернов во третьем блоке ResNet, 4-й канал     Визуализация паттернов во третьем блоке ResNet, 8-й канал   В третьем блоке паттерны продолжают усложняться, и схожесть между ними остаётся только на самом общем уровне. Паттерны ImageNet имеют гораздо более проработанную структуру, которая начинает соответствовать объектам из реального мира.\n   Визуализация паттернов в четвёртом блоке ResNet, 4-й канал   В последнем блоке паттерны окончательно перестали был похожими друг на друга. В паттернах нашей модели видна структура, соответствующая человеческим губам. в паттерах Imagenet \u0026ndash; что то растительное.\nВидно, что эволюция в верхних слоях зашла довольно далеко, при этом паттерны нашей модели в целом проще исходных, т.е. произошла некоторая деградация. Возможно, что ResNet-50 избыточен для данной задачи, и можно было использовать более простую сеть.\nИнтерактивное демо В статье не публикуются образцы предсказаний модели, т.к. любые предсказания можно сгенерировать самостоятельно, с помощью интерактивного демо, находящегося по адресу https://ag-demo.suilin.ru/.\nВ демо можно загружать любые фото, где есть лицо одного человека. Поддерживается работа как с компьютеров, так и со смартфонов (можно определять пол и возраст для селфи).\nРезюме Задача распознавания пола и возраста в промышленных масштабах оказалась вполне решаемой. При этом модель угадывает возраст примерно на уровне человека, часто даже точнее.\nВозможные улучшения  Сделать более выравненную по возрастам обучающую выборку, чтобы не страдало качество вне основного диапазона возрастов Попробовать ускорить обучение с помощью Super-Convergence [14]. Если ускорить обучение (рекорд скорости обучения ResNet-50 \u0026ndash; 18 минут), появится возможность обучаться на большем количестве данных. В результате можно увеличить размер обучающей выборки и активнее использовать аугментацию, например применить алгоритмы AutoAugment[15], Mixup[16].\n Попробовать более современные, чем ResNet, архитектуры: AmoebaNet[17], DenseNet[18], WideResNet[19].\n Работа с лицами это достаточно узкая и специфичная задача, возможно оптимальнее будет не использовать архитектуру общего назначения, а создать custom архитектуру, нацеленную именно на обработку лиц. Можно использовать методики автоматического создания и оптимизации архитектур: ENAS[20], DARTS[21], Auto-Keras[22], AdaNet[23].\n  Источники  Joint face detection and alignment using multitask cascaded convolutional networks [link] ^ K. Zhang, Z. Zhang, Z. Li, Y. Qiao, 2016. IEEE Signal Processing Letters. 23.10 (pp. 1499-1503) DOI:10.1109/lsp.2016.2603342   How far are we from solving the 2D \u0026amp; 3D face alignment problem? (And a dataset of 230,000 3D facial landmarks) ^ A. Bulat, G. Tzimiropoulos, 2017. International conference on computer vision.   Demystifying face recognition [link] ^ B. Ludwiczuk, 2017.   Least-squares estimation of transformation parameters between two point patterns [PDF] ^ S. Umeyama, 1991. IEEE Transactions on Pattern Analysis and Machine Intelligence. 13.4 (pp. 376-380) DOI:10.1109/34.88573   Deep residual learning for image recognition [link] ^ K. He, X. Zhang, S. Ren, J. Sun, 2016. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). DOI:10.1109/cvpr.2016.90   Learning transferable architectures for scalable image recognition [arXiv] ^ B. Zoph, V. Vasudevan, J. Shlens, Q. Le, 2017.   MobileNets: Efficient convolutional neural networks for mobile vision applications [arXiv] ^ A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, H. Adam, 2017.   Deep expectation of real and apparent age from a single image without facial landmarks [PDF] ^ R. Rothe, R. Timofte, L. Gool, 2016. International Journal of Computer Vision (IJCV).   Age and gender estimation of unfiltered faces [PDF] ^ E. Eidinger, R. Enbar, T. Hassner, 2014. IEEE Transactions on Information Forensics and Security. 9. (pp. 2170-2179) DOI:10.1109/TIFS.2014.2359646   Age progression/regression by conditional adversarial autoencoder [arXiv] ^ S. Zhang, H. Qi, 2017. IEEE conference on computer vision and pattern recognition (cvpr).   SGDR: Stochastic gradient descent with warm restarts [arXiv] ^ I. Loshchilov, F. Hutter, 2016.   Cyclical learning rates for training neural networks [link] ^ L. Smith, 2017. IEEE Winter Conference on Applications of Computer Vision (WACV). DOI:10.1109/wacv.2017.58   A unified approach to interpreting model predictions [PDF] ^ S. Lundberg, S. Lee, 2017. Advances in neural information processing systems 30. (pp. 4765-4774)   Super-convergence: Very fast training of neural networks using large learning rates [arXiv] ^ L. Smith, N. Topin, 2017.   AutoAugment: Learning augmentation policies from data [arXiv] ^ E. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. Le, 2018.   Mixup: Beyond empirical risk minimization [arXiv] ^ H. Zhang, M. Cisse, Y. Dauphin, D. Lopez-Paz, 2017.   Regularized evolution for image classifier architecture search [arXiv] ^ E. Real, A. Aggarwal, Y. Huang, Q. Le, 2018.   Densely connected convolutional networks [arXiv] ^ G. Huang, Z. Liu, L. Maaten, K. Weinberger, 2017. CVPR. (pp. 2261-2269)   Wide residual networks [arXiv] ^ S. Zagoruyko, N. Komodakis, 2016.   Efficient neural architecture search via parameter sharing [arXiv] ^ H. Pham, M. Guan, B. Zoph, Q. Le, J. Dean, 2018. abs/1802.03268.   Darts: Differentiable architecture search [arXiv] ^ H. Liu, K. Simonyan, Y. Yang, 2018.   Auto-keras: Efficient neural architecture search with network morphism [arXiv] ^ H. Jin, Q. Song, X. Hu, 2018.   AdaNet: Adaptive structural learning of artificial neural networks [arXiv] ^ C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, S. Yang, 2017.    ","date":1535313600,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1535313600,"objectID":"a89ced8364c2ab71b9f0c1337e64f993","permalink":"https://suilin.ru/project/gender_age/","publishdate":"2018-08-27T00:00:00+04:00","relpermalink":"/project/gender_age/","section":"project","summary":"Принципы работы модели, архитектура, подготовка данных, обучение. Результаты работы модели превосходят результаты всех коммерческих поставщиков аналогичной услуги по состоянию на июль 2018 года (Microsoft, Amazon, etc)","tags":["Instagram","Deep.Social"],"title":"Определение пола и возраста по фото","type":"project"},{"authors":null,"categories":null,"content":" Проблема подбора блогеров У рекламодателей, использующих продвижение товаров и услуг через посты инфлюэнсеров, есть большая проблема, которая называется таргетирование. Если рекламодатель даёт рекламу, например, через AdWords, он может сфокусироваться по ключевым словам на сколь угодно узкий тематический сегмент аудитории, точно соответствующий тем товарам и услугами, которые он продвигает на рынок. В социальных сетях такой возможности нет: в Instagram миллионы потенциальных инфлюэнсеров, у которых можно разместить рекламу. Каким образом выбрать из них тех, тематика которых подходит рекламодателю? Или рекламодатель должен сам активно пользоваться Инстаграмом и априори знать тех блогеров, которые ему подойдут (т.е. только крупных и только тех, на кого он сам подписан). Или просто давать рекламу наугад, надеясь что она дойдет до хотя бы небольшого процента целевой аудитории. Понятно, что оба варианта не очень хороши, поэтому для нишевых средних и мелких рекламодателей Инстаграм малоэффективен.\nЕсли решить эту проблему, реклама в Инстаграме доступной и удобной для всех, а не только для крупных брендов.\nТематический рубрикатор? Спасибо, не надо. Самый очевидный путь разделения блогеров на тематические сегменты это тематический рубрикатор. Для этого сначала вручную создаётся дерево тематик. Затем каждый блогер вручную или с помощью машинного обучения соотносится с рубриками этого дерева.\nЭтим путём идёт большинство рекламных систем, включая такие крупные, как Facebook Ads. Но на самом деле это неэффективное решение. Количество потенциально интересных рекламодателям тематик очень велико. Не будет ошибкой сказать, что оно потенциально бесконечно: чем более в узкой нише работает рекламодатель, тем более мелкое дробление на тематики ему требуется.\n Например, есть тематика Еда. Очевидно, что ресторану нужна не просто Еда, а Еда : Рестораны. Если это ресторан тайской кухни, то нужна еще более узкая тематика Еда : Рестораны : Тайская кухня. Если этот ресторан находится в Лондоне, неплохо бы добавить и тематику Еда : Рестораны : Лондон, и так далее.  Но дерево тематик не может расти бесконечно:\n C ним станет банально неудобно работать рекламодателю. Размер рубрикатора, который без усилий может воспринять и удержать в голове обычный человек \u0026ndash; не больше 100 пунктов. Чем больше рубрик, тем сложнее отнести блогера к конкретным рубрикам и тем больший объем работы для этого нужен. Сделать эту работу вручную (блогеров \u0026ndash; несколько миллионов) вообще невозможно. Можно использовать машинное обучение, но всё равно сначала надо составить обучающую выборку, т.е. образцы правильной разметки блогеров тематиками. Её составляют люди, люди делают ошибки, люди субъективны в своих взглядах и оценках (разные асессоры отнесут одного и того же блогера к разным рубрикам). Если учесть, что многие блогеры работают одновременно в нескольких тематиках, проблема еще больше усложняется. Придётся тратить много усилий на поддержание рубрикатора в актуальном состоянии (постоянно появляются новые тренды, открываются новые тематики и отмирают старые), и на обновление обучающей выборки.  Получается, что маленькое дерево тематик будет слишком негибким и давать слишком грубое деление, а с большим деревом будет неудобно работать и рекламодателю, и его создателям. Необходимо другое решение.\nТематику задаёт сам рекламодатель Что, если тематику можно будет задавать набором ключевых слов, как в AdWords? Тогда рекламодатель сам сможет настроить для себя любую, сколь угодно узкую тематику, и ему при этом не надо будет ползать по огромному рубрикатору. Но в AdWords ключевые слова соответствуют тому, что явным образом ищут потенциальные клиенты, а чему должны соответствовать ключевые слова в Instagram? Хэштэгам? Но здесь всё не так просто.\n Блогер, пишущий про автомобили, совсем не обязательно будет употреблять хэштэг #car, он может использовать #auto, #fastcars, #wheels, #drive или названия брендов #bmw, #audi и т.п. Как рекламодатель должен догадаться, какие конкретно тэги используют блогеры? В принципе эта же проблема есть и в AdWords: рекламодателю надо приложить немалые усилия, чтобы охватить все возможные ключевые слова в его тематике. Блогер может случайно употребить хэштэг, например #car, если он купил новый авто, или просто увидел и сфотографировал интересный автомобиль на улице. Это совсем не значит, что он пишет на автомобильную тематику. Блогеры часто используют популярные тэги, не имеющие никакого отношения к тематике поста, просто чтобы попасть в поисковую выдачу по тэгу (hashtag spam). Например тэгом #cat может быть помечено и фото бородатого хипстера, и пейзаж с закатом, и селфи себя любимой в новом наряде и в окружении подруг.\n  Поэтому подбор блогеров просто по наличию у них тэгов, заданных рекламодателем, будет работать плохо. Нужны более интеллектуальные способы решения этой задачи.\nТематическое моделирование, теория В современных методиках обработки естественного языка есть область, называемая тематическое моделирование (topic modeling). Проще всего объяснить применение тематического моделирования к нашей проблеме на простом примере.\nПредставим очень примитивную социальную сеть, в которой у людей есть всего два основных интереса - интерес к еде (Food), и интерес к Японии (Japan). Если “силу” интереса сопоставить с числом от 0 до 1, то любые хэштэги, используемые блогерами, можно поместить на 2D диаграмму.\n  Пример задания тематик на двумерном пространстве   Видно, что любой хэштэг можно описать парой чисел, соответствующих координатам X и Y в тематическом пространстве. Тэги, относящиеся к одной тематике, группируются в кластера, т.е. у них близкие координаты. Пользуясь этой диаграммой, можно вычислить релевантность поста тематикам, рассчитав по всем тэгам, входящим в пост, его “усредненные” координаты в тематическом пространстве (т.е. получив центроид). Координаты центроида по осям X и Y будут соответствовать релевантности поста тематикам Food и Japan, чем координата ближе к 1, тем выше релевантность. Таким же образом рассчитав центроид всех постов блогера, можно понять, каким тематикам в целом релевантен контент у блогера.\nВ реальном тематическом моделировании конечно используется не две тематики, а десятки и сотни, соответственно тэги существуют в high-dimensional пространстве. Более математическое определение:\n Существует множество документов $D$ (в нашем случае это посты), множество слов $W$ (в нашем случае тэги) и множество тематик $T$, размер которого задан заранее. Содержимое документов можно представить как набор пар документ-слово: $(d, w), d \\in D, w \\in W_d$ Каждая тематика $t \\in T$ описывается неизвестным распределением $p(w|t)$ на множестве слов $w \\in W$ Каждый документ $d\\in D$ описывается неизвестным распределением $p(t|d)$ на множестве тем $t\\in T$ Предполагается, что распределение слов в документах зависит только от тематики: $p(w|t,d)=p(w|t)$ В процессе построения тематической модели алгоритм находит матрицу \u0026ldquo;слово\u0026ndash;тематика\u0026rdquo; $\\mathbf{\\Phi} =||p(w|t)||$ и матрицу \u0026ldquo;тематика\u0026ndash;документ\u0026rdquo; $\\mathbf{\\Theta} =||p(t|d)||$ по содержимому коллекции $D$. Нас интересует первая матрица.  Тематическое моделирование эквивалентно неотрицательному матричному разложению (NMF, Non-negative matrix factorization). На входе есть разреженная матрица \u0026ldquo;cлово-документ\u0026rdquo; $\\mathbf{S} \\in \\mathbb{R}^{W \\times D}$, описывающая вероятность встретить слово $w$ в документе $d$. Вычисляется её аппроксимация в виде произведения низкоранговых матриц $\\mathbf{\\Phi} \\in \\mathbb{R}^{W \\times T}$ и $\\mathbf{\\Theta} \\in \\mathbb{R}^{T \\times D}$. $$\\mathbf{S} \\approx \\mathbf{\\Phi}\\mathbf{\\Theta}$$ Более подробная информация по тематическому моделированию и его алгоритмам есть в Википедии.\nТематическое моделирование, практика На практике тематическое моделирование показало посредственные результаты. В таблице представлены результаты моделирования по 15 тематикам с помощью библиотеки BigARTM:\n   Тематика Топ тэги     0 sky, clouds, sea, spring, baby, ocean, nyc, flower, landscape, drinks   1 beer, vintage, chill, school, rainbow, yoga, rock, evening, chicago, relaxing   2 sweet, chocolate, dance, rain, nike, natural, anime, old, wcw, reflection   3 foodporn, breakfast, delicious, foodie, handmade, gold, instafood, garden, healthy, vegan   4 architecture, california, lights, portrait, newyork, wine, blonde, familytime, losangeles, thanksgiving   5 nature, travel, autumn, london, fall, trees, tree, photoshoot, city, cake   6 flowers, design, inspiration, artist, goals, illustration, pizza, ink, glasses, money   7 winter, snow, catsofinstagram, sexy, cats, cold, quote, fire, disney, festival   8 work, mountains, paris, football, nails, video, florida, diy, free, japan   9 dog, puppy, wedding, dogsofinstagram, dogs, roadtrip, painting, trip, thankful, pet   10 coffee, quotes, river, yum, moon, streetart, sleepy, music, adidas, positivevibes   11 style, fashion, party, home, model, music, dress, goodvibes, couple, tired   12 fitness, motivation, gym, workout, drawing, dinner, fit, sketch, health, fresh   13 beach, lake, usa, shopping, hiking, fashion, kids, park, freedom, sand   14 makeup, cat, yummy, eyes, snapchat, homemade, tattoo, kitty, lips, mom    Видно, что какая то разумная структура прослеживается, но тематики далеки от совершенства. Увеличение количества тематик до 150 даёт сравнительно небольшое улучшение.\nВозможно, причина в том, что тематическое моделирование рассчитано на работу с документами, содержащими сотни и тысячи слов. В нашем случае у большинства постов есть всего 2-3 тэга.\nУ BigARTM есть большое количество гиперпараметров и возможных способов их применения (в начале обучения, в конце, ко всем тематикам, к отдельным тематикам и т.п.). Возможно, при некоторых настройках результат был бы лучше, но TopicTensor это коммерческий проект, подразумевающий лимиты времени на реализацию. С тематическим моделированием был риск потратить всё проектное время на подбор гиперпараметров, и так и не получить удовлетворительный результат на выходе. Другие библиотеки ( Gensim, Mallet) тоже показали весьма скромные результаты.\nПоэтому был выбран другой, более простой и в то же время более мощный способ моделирования. $ \\newcommand{\\sim}[2]{\\operatorname{sim}(#1,#2)} $\nМодель TopicTensor Основной плюс тематического моделирования это интерпретируемость полученных результатов. Для любого слова/тэга на выходе получается набор весов, показывающих, насколько близко это слово к каждой тематике из всего набора.\nНо этот же плюс накладывает серьезные ограничения на модель, вынуждая её укладываться строго в фиксированное количество тематик, не больше и не меньше. В реальной жизни количество тематик большой социальной сети практически бесконечно. Поэтому, если убрать требование по интерпретируемости тематик (и их фиксированному количеству), обучение станет более эффективным.\nВ результате получается модель, близкая по смыслу к хорошо известной модели Word2Vec. Каждый тэг представлен в виде вектора в $N$-мерном пространстве: $w \\in \\mathbb{R}^N$. Степень схожести (т.е. насколько близки тематики) между тэгами $w$ и $w\u0026rsquo;$ может вычисляться как скалярное произведение (dot product): $$\\sim{w}{w\u0026rsquo;}=w \\cdot w\u0026rsquo;$$ как Евклидово расстояние: $$\\sim{w}{w\u0026rsquo;}=\\|w-w\u0026rsquo;\\|$$ как cosine similarity: $$\\sim{w}{w\u0026rsquo;}=\\cos(\\theta )=\\frac{w \\cdot w\u0026rsquo;}{\\|w \\|\\|w\u0026rsquo; \\|}$$\nЗадача модели в ходе обучения - найти такие представления тэгов, которые будут полезны для одного из предсказаний:\n На основе одного тэга предсказать, какие ещё тэги будут включены в пост (архитектура Skip-gram) На основе всех тэгов поста, кроме одного, предсказать недостающий тэг (архитектура CBOW, \u0026ldquo;мешок слов\u0026rdquo;) Взять два случайных тэга из поста, и на основе первого предсказать второй  Все эти предсказания сводятся к тому, что есть целевой тэг $w_t$ , который надо предсказать, и контекст $c$, представленный одним или несколькими тэгами, входящими в пост. Модель должна максимизировать вероятность тэга в зависимости от контекста, это можно представить в виде softmax критерия:\n$$P(w_t|c) = \\operatorname{softmax}(\\sim{w_t}{c})$$ $$P(w_t|c) = \\frac{\\exp(\\sim{w_t}{c})}{\\sum_{w\u0026rsquo; \\in W}\\exp(\\sim{w\u0026rsquo;}{c})}$$\nНо вычислять softmax по всему множеству тэгов $W$ дорого (в обучении может участвовать миллион тэгов и более), поэтому вместо него используются альтернативные способы. Они сводятся к тому, что есть позитивный пример $(w_t,c)$, который надо предсказать, и случайно выбранные негативные примеры $(w_1^{-}, c), (w_2^{-}, c),\\dots,(w_n^{-}, c)$ являющиеся образцом того, как не надо предсказывать. Негативные примеры должны сэмплироваться из того же распределения частот тэгов, что и в обучающих данных.\nLoss функция по набору примеров может иметь вид бинарной классификации (Negative sampling в классическом Word2Vec) $$L = \\log(\\sigma(\\sim{w_t}{c})) + \\sum_i\\log(\\sigma(-\\sim{w_i^-}{c}))$$ $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$ или работать как ranking loss, попарно сравнивая \u0026ldquo;совместимость\u0026rdquo; с контекстом позитивного и негативного примеров: $$L = \\sum_{i} l(\\sim{w_t}{c}, \\sim{w_i^-}{c})$$ гдe $l(\\cdot, \\cdot)$ это ranking функция, в качестве которой часто используют max margin loss: $$l=\\max(0,\\mu+\\sim{w_i^-}{c}−\\sim{w_t}{c})$$\nМодель TopicTensor также эквивалентна матричной факторизации, только вместо матрицы \u0026ldquo;документ-слово\u0026rdquo; (как в тематическом моделировании) здесь факторизуется матрица \u0026ldquo;контекст-тэг\u0026rdquo;, которая в при некоторых типах предсказаний превращается в матрицу взаимной встречаемости тэгов \u0026ldquo;тэг-тэг\u0026rdquo;.\nПрактическая реализация TopicTensor Были рассмотрены несколько возможных способов реализации модели: код на Tensorflow , код на PyTorch, библиотека Gensim, библиотека StarSpace. Выбран последний вариант, как требующий минимальных усилий на доработку (вся необходимая функциональность уже есть), дающий высокое качество, и практически линейно распараллеливающийся на любое количестве ядер (для ускорения обучения использовалась 32 и 64-ядерные машины).\nStarSpace по умолчанию использует loss функцию max margin ranking loss и cosine distance как метрику близости векторов. Последующие эксперименты с гиперпараметрами показали, что эти установки по умолчанию являются оптимальными.\nПодбор гиперпараметров Перед финальным обучением проводился подбор гиперпараметров, с целью найти баланс между качеством и приемлемым временем обучения. Качество замерялось так: бралась выборка постов, которые модель не видела в ходе обучения. Для каждого тэга из поста (всего в посте $n$ тэгов) находились наиболее близкие ему тэги-кандидаты по критерию cosine similarity из множества всех тэгов $W$: $$candidates_i=\\operatorname{top_n}(\\sim{w_t}{w\u0026rsquo;}, \\forall w\u0026rsquo; \\in W)$$ $$i \\in 1 \\dots n$$ Подсчитывалось, сколько из этих кандидатов совпало с реальными тэгами в посте (число совпадений $n^{+}$). $$quality=\\frac{\\sum n^+}{\\sum n}$$ Качество \u0026ndash; процент правильно угаданных тэгов по всем постам из выборки. Такая оценка качества наиболее близка к использованию модели в реальной жизни, когда пользователь будет задавать в большинстве случаев один стартовый тэг и подбирать по нему остальные тэги, блогеров, и т.д.\nЭта оценка также подразумевает, что наиболее оптимально обучать модель по skip-gram критерию (по одному тэгу предсказывать остальные). Это подтвердилось на практике: skip-gram обучение показало наилучшее качество, хотя оказалось и самым медленным.\nПодбирались следующие гиперпараметры:\n Размерность векторов Кол-во эпох для обучения Кол-во негативных примеров Learning rate Undersampling и oversampling  Последний гиперпараметр связан с тем, что на постах с малым количеством тэгов обучение происходит быстрее, чем на постах с большим количеством тэгов. StarSpace за один проход случайным образом выбирает из поста только один целевой тэг. Таким образом за 20 эпох каждый тэг из поста, содержащего 2 целевых тэга, побудет целевым в среднем 10 раз, а каждый тэг из поста, содержащего 20 тэгов \u0026ndash; в среднем только один раз. На коротких списках тэгов модель переобучится, а на длинных наоборот недообучится. Чтобы избежать этого, \u0026ldquo;коротким\u0026rdquo; постам надо делать undersampling, а \u0026ldquo;длинным\u0026rdquo; \u0026ndash; oversampling.\nПодготовка данных Тэги были нормализованы: приведены к lowercase, убраны диакритические знаки (за исключением случаев, где знак влияет на смысл слова)\nДля обучения отобраны тэги, встречающиеся в обучающей выборке не менее N раз у разных блогеров, чтобы обеспечить разнообразие контекстов их использования (в зависимости от языка N варьировалось от 20 до 500).\nДля каждого языка была сделана выборка из топ 1000 самых распространенных тэгов, и в этой выборке внесены в blacklist общеупотребительные слова, не несущие тематической нагрузки (me, you, together, etc), числительные, названия цветов (красный, желтый, и т.п.), и некоторые тэги, особенно любимые спамерами.\nТэги каждого блогера перевзвешены в соответствии с частотой их употребления этим блогером. У большинства блогеров есть \u0026ldquo;любимые\u0026rdquo; тэги и сочетания, используемые почти в каждом посте, и если не понизить их вес, активно пишущий блогер может перекосить глобальную статистику по употреблению любимых им тэгов и модель обучится под вкусы этого блогера.\nВ финальную обучающую выборку попало около 8 млрд тэгов из 1 млрд постов. Обучение шло более трех недель на 32-ядерном сервере.\nРезультаты Полученные embeddings показали отличное разделение тематик, хорошую способность к генерализации и устойчивость к спам-тэгам.\nДемо-выборка top 10К тэгов (только английский язык) доступна для просмотра в Embedding Projector. Перейдя по ссылке, надо переключиться в режим t-SNE (tab в левой нижней части) и подождать примерно 500 итераций, пока не построится проекция в 3D. Просматривать лучше в режиме Color By = logcnt. Если не хочется ждать, в правом нижнем углу есть раздел Bookmarks, в нём выбрать Default, тогда сразу загрузится уже рассчитанная проекция.\nПримеры формирования тематик Начнём с самого простого. Зададим тематику одним тэгом и найдем топ 50 релевантных тэгов.   Тематика, заданная тэгом #bmw   Тэги окрашены в соответствии с релевантностью. Размер тэга пропорционален его популярности.\nКак видно, TopicTensor прекрасно справился с формированием тематики ‘BMW’ и нашел много релевантных тэгов, о существовании которых большинство даже не подозревает.\nУсложним задачу, и сформируем тематику из нескольких немецких автобрендов (найдем тэги, наиболее близкие к сумме векторов входных тэгов):   Тематика, заданная тэгами #bmw, #audi, #mercedes, #vw  \nНа этом примере видна способность TopicTesnor к обобщению (generalisation): TopicTensor понял, что мы имеем в виду автомобили в целом (тэги #car, #cars). Также понял, что в тематике надо отдать предпочтение немецким автомобилям (тэги, обведенные красным), и сам добавил “недостающие” тэги: #porsche (тоже немецкий автобренд), и варианты написания тэгов, которых не было на входе: #mercedesbenz, #benz и #volkswagen\n  Тематика, заданная тэгом #apple   Усложним задачу еще больше, и создадим тематику на основе неоднозначного тэга #apple, который может обозначать как бренд, так и просто фрукт. Видно, что тематика бренда доминирует, тем не менее фруктовая тема тоже присутствует в виде тэгов #fruit, #apples и #pear.\nПопробуем выделить чистую “фруктовую” тематику, для этого добавим несколько тэгов, относящихся к бренду apple, с отрицательным веcом. Соответственно, будем искать тэги, наиболее близкие к взвешенной сумме векторов входных тэгов (по умолчанию вес равен единице): $$target = \\sum_i w_i \\cdot tag_i $$\n  Тематика, заданная тэгами #apple, #iphone:-1, #macbook:-0.05 #macintosh:-0.005   Видно, что отрицательные веса убрали тематику бренда, и осталась только тематика фрукта.\n   Тематика, заданная тэгом #mirror   TopicTensor в курсе, что одно и то же понятие может быть выражено различными словами на разных языках, как это видно на примере с #mirror. К английским mirror и reflection подобрались: зеркало и отражение на русском, espejo и reflejo на испанском, espelho и reflexo на португальском, specchio и riflesso на итальянском, spiegel и spiegelung на немецком.\n   Тематика, заданная тэгом #boobs   На последнем примере видно, что casual тематики работают так же хорошо, как и брендовые.\nПодбор блоггеров Для каждого блогера анализируются его посты и суммируются вектора всех входящих в них тэгов. $$\\beta=\\sum_i^{|posts|}\\sum_j^{|tags_i|} w_{ij}$$ где $|posts|$ это кол-во постов, $|tags_i|$ это кол-во тэгов в $i$-том посте. Результирующий вектор $\\beta$ это и есть тематика блогера. Затем находятся блогеры, тематический вектор которых наиболее близок к тематическому вектору, заданному пользователем. Список сортируется по релевантности и выдается пользователю.\nДополнительно учитывается популярность блогера и количество тэгов в его постах, т.к. в противном случае в топ вышли бы блогеры, у которых есть один пост с одним тэгом, заданным пользователем на входе. Финальный score, по которому сортируются блогеры, рассчитывается так: $$score_i = {\\sim{input}{\\beta_i} \\over \\log(likes)^\\lambda \\cdot \\log(followers)^\\phi \\cdot \\log(tags)^\\tau}$$ где $\\lambda, \\phi, \\tau$ - эмпирически подобранные коэффициенты, лежащие в интервале $0\\dots1$\nРасчет cosine distance по всему массиву блогеров (в подборе участвует несколько миллионов аккаунтов) занимает значительное время. Для ускорения подбора была использована библиотека NMSLIB (Non-Metric Space Library), позволившая сократить время поиска на порядок. NMSLIB заранее строит индексы по координатам векторов в пространстве, что позволяет намного быстрее вычислять топ близких векторов, рассчитывая cosine distance только для тех кандидатов, для которых это имеет смысл.\nДемо сайт Демонстрационный сайт с ограниченным количеством тэгов и блогеров доступен по адресу http://tt-demo.suilin.ru/. На сайте можно самостоятельно поэкспериментировать с формированием тематик и подбором блогеров по сформированной тематике.\nТематические lookalikes Вектора $\\beta$, рассчитанные для подбора блогеров, можно использовать и для сопоставления блогеров друг с другом. Фактически lookalikes это тот же подбор блогеров, но вместо вектора тэгов на вход подается тематический вектор $\\beta$ блогера, заданного пользователем. На выходе получается список блогеров, тематика которых близка тематике заданного блогера, в порядке релевантности.\nФиксированные тематики В TopicTensor, как уже говорилось, нет явным образом заданных тематик. Тем не соотнесение постов и блогеров с фиксированным набором тематики бывает необходимо, для упрощения поиска, или для ранжирования блогеров внутри отдельных тематик. Возникает задача по экстрагированию фиксированных тематик из векторного пространства тэгов.\nДля решения этой задачи было выбрано unsupervised обучение, чтобы избежать субъективности при определении возможных тематик, и чтобы сэкономить ресурсы, т.к. просмотр сотен тысяч тегов (даже 10% из них) и присвоение им тематик \u0026ndash; это большая ручная работа.\nСамый очевидный способ экстракции тематик это кластеризация векторного представления тэгов, один кластер = одна тематика. Кластеризация проводилась в два этапа, т.к. алгоритмов, способных эффективно искать кластера в 200D пространстве, пока не существует.\nНа первом этапе проводилось уменьшение размерности с помощью технологии UMAP. Эта технология является в некотором смысле улучшенным t-SNE (хотя основана на совершенно других принципах), быстрее работает, и лучше сохраняет исходную топологию данных. Размерность уменьшалась до 5D, в качестве метрики расстояния использовался cosine distance, остальные гиперпараметры подбирались по результатам кластеризации (второго этапа).\n  Пример кластеризации тэгов в 3D пространстве. Разные кластера помечены разными цветами (цвета не уникальны и могут повторяться для разных кластеров).   На втором этапе проводилась кластеризация алгоритмом HDBSCAN . Результаты кластеризации (только по английскому языку) можно увидеть в GitHub. Кластеризация выделила около 500 тематик (параметрами UMAP и кластеризации можно регулировать кол-во тематик в широких пределах), при этом в кластера попало 70%-80% тэгов. Визуальная проверка показала хорошую когерентность тематик и отсутствие заметной корреляции между кластерами. Тем не менее, для практического применения кластера нуждаются в доработке: собрать из них дерево, убрать бесполезные (например кластер личных имён, кластер негативных эмоций, кластер общеупотребительных слов), объединить некоторые кластера в одну тематику.\nВозможные улучшения Основной недостаток TopicTensor \u0026ndash; покрытие, далёкое от 100%. Далеко не все блогеры используют тэги, и далеко не все, кто их используют, пишут в них что-то осмысленное. Есть два основных способа расширить покрытие:\n Анализ содержимого фото. Тематика блога чётко определяется по фотографиям (собственно, они её и задают), поэтому computer vision модель, обученная выдавать по фотографии её тематический вектор, могла бы частично заменить тэги. Если считать, что у блогеров со схожей аудиторией должна быть схожая тематика, можно выводить тематику блогеров, не использующих тэги, через audience lookalikes, если существуют блогеры с похожей аудиторией и с тэгами.  ","date":1530043200,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1530043200,"objectID":"b71679196fd7b0d7efd2c380a7afc6a7","permalink":"https://suilin.ru/project/topic_tensor/","publishdate":"2018-06-27T00:00:00+04:00","relpermalink":"/project/topic_tensor/","section":"project","summary":"Создание embeddings для формирования и определения тематик Instagram постов и тематик блогеров. Подбор блогеров по тематикам, тематические lookalikes и другие применения.","tags":["Instagram","Deep.Social"],"title":"TopicTensor: тематики в Instagram","type":"project"},{"authors":null,"categories":null,"content":" Для рекламодателей одним из самых важных параметров блогера и его аудитории является географическое положение (geolocation), соответствующее месту жительства. Чтобы рекламная кампания была эффективной, она обычно должна быть таргетирована на конкретную страну или даже город.\nГеотэги Посты в Instagram могут содержать информацию о месте, в котором был сделан пост, называемую geotags. Это самый простой и очевидный способ определения geolocation. В заголовке страницы приведена карта, окраска которой соответствует количеству постов, сделанных в каждой точке мира (построено по данным геотэгов из более чем 4 млрд. постов). Видно, что наибольшая интенсивность постов соответствует местам с большой плотностью населения \u0026ndash; крупные города с пригородами, зоны вдоль крупных трасс.\nНо геотэги, несмотря на свою очевидную полезность, не являются надёжным источником информации о месте проживания владельца аккаунта:\n Часто люди используют geotagging только когда едут в командировку/отпуск/путешествие или находятся в интересных с их точки зрения местах. То есть у блогера, живущего в Лондоне, геотэги могут быть из каких угодно мест мира, кроме самого Лондона. Далеко не все аккаунты включают geotagging постов. У большинства постов (более 85%) геотэги не проставлены.  Поэтому, если определять geolocation только по геотэгам, у результатов будет маленькое покрытие и низкая точность. Чтобы улучшить качество определения, необходимо использовать дополнительную информацию.\nДополнительные источники информации Cуществует много дополнительных, не столь очевидных и надёжных, как геотэги, источников информации о местоположении блогера.\n В первую очередь это текст из раздела ‘bio’ аккаунта: в нем могут быть указаны город блогера или локальные географические названия, сайт или email в национальных доменах, emoji, соответствующие флагам стран, и т.п. Такая же информация + текстовые тэги может содержаться в тексте постов и комментариев. Кроме этого, можно использовать информацию о языке и audience geolocation \u0026ndash; очевидно, что например японская аудитория скорее будет у блогера из Японии, чем у блогера из Мексики. Также маловероятно, что мексиканский блогер будет делать посты на японском языке. Также, приблизительную геоинформацию можно извлекать из фотографий в постах (когда нет геотэгов).  Вся эта информация извлекается с помощью набора правил, использующих как и сложные закодированные вручную эвристики, так и machine learning модели для named entity recognition. Рассказ о методиках извлечения может потянуть на отдельный проект, поэтому здесь я сфокусируюсь только на использовании уже извлеченной геоинформации.\nEnsemble learning Итак, есть несколько источников геоинформации. Каждый отдельно взятый источник не очень точен и достоверен, но совместное их использование может дать намного более точный результат. Классический подход, используемый для работы с такими источниками, это ensemble learning.\n Ensemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem. In contrast to ordinary machine learning approaches which try to learn one hypothesis from training data, ensemble methods try to construct a set of hypotheses and combine them to use.1\n В нашем конкретном случае используется разновидность ensemble learning, называемая stacking. Смысл этого метода в том, что есть два уровня моделей:\n First level \u0026ndash; это модели, извлекающие геоинформацию из данных Instagram, о которых мы говорили в предыдущем разделе (base classifiers). Second level (meta-classifier) использует предсказания, сгенерированные моделями на первом уровне, для принятия окончательного решения.  Weighted majority voting Простейший способ создания meta classifier \u0026ndash; это использование weighted majority voting. Есть $N$ базовых классификаторов, каждому из них присваивается вес $w_i$, соответствующий вероятности правильной классификации $\\hat{p}_i$ (в данном случае правильному определению страны блогера). Вероятности рассчитываются по обучающей выборке. $$w_i=\\log\\left(\\frac{\\hat{p}_i}{1−\\hat{p}_i}\\right),\\quad i=1,\\dotsc,N$$\nКлассификаторы, которые часто ошибаются, получают меньший вес, классификаторы, у которых высокая точность \u0026ndash; больший вес. На этом \u0026ldquo;обучение\u0026rdquo; заканчивается.\nИспользование: для интересующего нас аккаунта каждым из базовых классификаторов назначаются метки классов (в нашем случае это предполагаемые страны), получаем набор меток $l_1,\\dotsc,l_N$ . Значения, которые могут принимать метки, это множество всех стран размером $M$: $(c_1,\\dotsc,c_M)$. Подсчитывается рейтинг каждого класса суммированием весов всех отданных за него голосов.\n$$R(k)=\\sum_{l_i=c_k}w_i,\\quad k=1,\\dotsc,M$$\nРезультирующий класс (страна) \u0026ndash; тот, который набрал наибольший рейтинг, т.е. тот за которого было отдано больше голосов с большим весом: $$k^*=\\operatorname*{arg} \\operatorname*{max}_{k=1}^M R(k)$$\nПервая модель для определения geolocation использовала именно этот способ, как самый простой в реализации. Но у него есть недостатки:\n Предсказания base classifiers могут быть неверными, например из bio может быть извлечена страна Япония, из геотэгов \u0026ndash; Бразилия и Германия, при этом блогер на самом деле живет в Англии. Поэтому meta classifier должен не просто предсказывать страну, но еще и выдавать вероятность правильного ответа. Если полученная вероятность ниже некоторого порога, надо считать, что для определения страны у нас недостаточно информации. Но majority voting не выдает значение вероятности, можно только примерно оценить верхнюю и нижнюю границы. Majority voting не умеет работать с дополнительной информацией, не являющейся меткой класса, например с языком аккаунта. Majority voting исходит из предположений, что предсказания base classifiers не коррелируют друг с другом, и что вероятность корректной классификации одинакова для любой страны. В реальной жизни ни то ни другое не выполняется: надо учитывать и возможные взаимодействия между классификаторами и индивидуальные особенности стран.  Gradient boosting Учитывая все недостатки majority voting, была разработана более совершенная модель, использующая градиентный бустинг. В данном проекте использовалась библиотека Yandex CatBoost.\nГрадиентный бустинг это один из самых лучших известных на сегодняшний день алгоритмов машинного обучения для табличных данных. Поэтому качество полученной модели превзошло все ожидания.\nФормирование обучающих данных Чтобы обучить модель, надо сначала каким то образом получить разметку Instagram аккаунтов в виде истинного места проживания владельца. Получить эти данные напрямую от владельцев затруднительно, поэтому был использован косвенный источник: Twitter.\nВладелец Twitter аккаунта может указать свое место жительства (location), эта информация общедоступна. Если у этого же человека есть Instagram account, то можно использовать геоинформацию из Twitter, в качестве ground truth. Конечно геоданные из Twitter не являются абсолютно точными, т.к. люди не всегда указывают настоящее место жительства, или переезжают и забывают обновить location. По приблизительным оценкам, примерно у 1-3% Twitter аккаунтов информация о location является неверной или устаревшей. Но, как показала практика, это не помешало обучению.\nМесто жительства из Твиттера представляет собой строку произвольного формата. Иногда там указана страна, иногда город, иногда штат, иногда просто афоризм или шутка, не имеющие отношения к географии. Для обучения необходимо преобразовать эти данные к коду страны. Задача получения географических координат из адреса произвольного формата называется геокодирование. В этом проекте для геокодирования была использована система Nominatim, созданная сообществом Open Street Maps.\nЗадача геокодирования часто не имеет однозначного решения. Например, строка Moscow может обозначать столицу России, а может город в USA. Georgia может оказаться как страной Грузией, так и штатом в USA.\nЧтобы убрать неоднозначности и повысить точность, дополнительно использовался анализ поисковой выдачи Google. В выдаче по строке с адресом проверялось наличие сниппета \u0026ldquo;географическое место\u0026rdquo;, и если сниппет найден, в нём анализировалась ссылка на информацию в WikiData. Если страна из WikiData совпадала со страной, которую выдал Nominatim, результат геокодирования принимался, в противном случае назначалась дополнительная ручная проверка. Идея здесь в том, что Google, на основе собственного анализа популярности поисковых запросов, знает, какой город люди обычно имеют в виду, когда пишут \u0026ldquo;Moscow\u0026rdquo;. Как показали результаты, такая перепроверка оказалась полезной, и значительно повысила точность геокодирования по сравнению с использованием чистого Nominatim.\nМодель Самый очевидный способ моделирования это multiclass classification, где каждый класс соответствует отдельной стране. Но такая модель не очень хорошо работает на практике. Проблема в том, что распределение стран по количеству аккаунтов в них очень неравномерное. Есть несколько доминирующих стран, в которых находится основное количество аккаунтов, и длинный хвост из стран небольшого размера, или в которых Instagram непопулярен. В результате для стран из \u0026ldquo;хвоста\u0026rdquo; качество определения страны будет посредственным. Модели просто не хватит данных для обучения в этих странах.\n  Зависимость точности (ось Y) от кол-ва аккаунтов страны в обучающей выборке (ось X, логарифмическая шкала), для одной из ранних моделей.   На рис. 1 видно, что с уменьшением количества аккаунтов на страну точность понижается. Чтобы избежать этого эффекта, был выбран другой способ моделирования классов, близкий по смыслу к majority voting, где модель вообще не отличает страны друг от друга, а принимает во внимание только \u0026ldquo;силу\u0026rdquo; голосов за каждую страну.\nВ данных каждого аккаунта базовые классификаторы обычно распознают не более 3-4 разных стран. Из этих стран и надо выбрать правильный ответ (или отсутствие ответа). Поэтому для работы модели достаточно всего 4-x классов, каждый из которых соответствует одной уникальной стране во входных данных, плюс один негативный класс \u0026ldquo;Другое\u0026rdquo;, чтобы модель могла просигнализировать, что ни одна страна из предложенных не подходит. Страны ранжируются в порядке популярности, т.е. самая часто встречающаяся страна из предложенных принимается за класс №1, вторая по популярности за класс №2 и т.п. В то же время, чтобы учесть индивидуальные особенности отдельных стран, идентификаторы предложенных стран (country codes) тоже подаются на вход модели.\nТаким образом модель совмещает в себе лучшее и от majority voting (способность обучаться на любых странах, даже если эта страна встретилась в обучающей выборке всего единожды) и от multiclass c классами-странами (способность учитывать особенности отдельной страны). Эксперименты показали, что точность такой модели для мелких стран заметно выросла, без ухудшения точности для крупных.\nКорректировка весов обучающих примеров   Распределение долей top 18 стран в Twitter (по данным из обучающей выборки) и Instagram (по данным из геотэгов)   Распределение стран в Twitter значительно отличается от распределения стран в Instagram, это видно на рис. 2. Например, популярность Instagram в России и в Иране значительно превосходит популярность Twitter. Если использовать обучающие данные as is, модель запомнит распределение стран из Twitter и будет подгонять свои ответы под это распределение. Вероятность того, модель выдаст в ответе Иран, будет так же мала, как доля Ирана в Twitter.\nЧтобы избежать этого, при обучении дополнительно задавались веса для каждого обучающего примера, равные соотношению долей соответствующей примеру страны в Instagram и Twitter.\nРезультаты обучения Была получена точность определения страны 97% при покрытии 86% (т.е. для 14% аккаунтов имеющейся информации оказалось недостаточно для надежного принятия решения и выданная вероятность была ниже заданного порога). Если вообще не использовать пороговую вероятность, и учитывать предсказания для любых аккаунтов, где есть хотя бы какая нибудь геоинформация, получится точность 93.4%. Цифры точности измерялись на тестовой выборке, т.е. на аккаунтах, которые модель ни разу не видела в процессе обучения и настройки гиперпараметров.\n  Взаимозависимость между точностью и покрытием при разных значениях порога вероятности.     Важность входных features для модели.   Как видно из рис. 4, наиболее важны country_id (идентификатор страны, позволяет модели учитывать индивидуальные особенности отдельных стран), геотэги из постов, язык блогера и геоданные аудитории. Но эта диаграмма не совсем корректно отражает реальную важность, потому что далеко не все features присутствуют в каждом аккаунте.\n  Важность входных features для модели. Features нормализованы по частоте.   Если нормализовать важность features по частоте их появления в аккаунтах, то будут лидировать features c самой точной геоинформацией: телефонные номера (код страны), geotags, emoji (флаги стран).\nМожно заглянуть еще глубже, и проанализировать модель с использованием SHAP framework2.   Feature importancе c использованием SHAP.  \nЧем дальше точки на рис. 6 отклонены от середины, тем больше влияет на результат данная фича. Голубые точки это нулевые значения, обычно соответствующие отсутствию feature во входных данных, красные \u0026ndash; ненулевые, т.е. в данном контексте непустые.\n  Детализация Feature importancе (SHAP) для отдельных аккаунтов.   На рис. 7 также видно, что наиболее “активными” features являются country_id, геотэги из постов, язык и emoji.\nОбразец работы модели Чтобы можно было посмотреть на реальные результаты работы модели, сформирована демо-выборка из 2000 аккаунтов (по тестовым данным, которые модель не видела в процессе обучения). Выборка представляет собой CSV файл, состоящий из 5 колонок:\n instagram \u0026ndash; Instagram аккаунт, для которого определялась страна twitter \u0026ndash; соответствующий Twitter account, источник \u0026ldquo;правды\u0026rdquo; prediction \u0026ndash; предсказанная страна truth \u0026ndash; страна из Twitter probability \u0026ndash; вероятность, интерпретируемая как уверенность в предсказании  Файл можно загрузить здесь.\nВ выборку включены результаты, где вероятность \u0026gt; 90%. Как видно по приведенным данным, качество связки Instagram \u0026lt;\u0026ndash;\u0026gt; Twitter не идеальное, попадаются ошибки, которые ухудшают замеренную результирующую точность. Поэтому реальная точность модели возможно еще выше, чем замеренные 97%\nВизуализации геоданных В завершение \u0026ndash; пара дополнительных визуализаций географических данных Instagram, полученных с помощью описанной выше модели.\nВизуализация 1 Подсчитаем распределение количества аккаунтов Instagram по странам мира. Абсолютное количество, конечно, невозможно подсчитать, потому что для этого нужны были бы данные по всем аккаунтам, существующим в Instagram. Вместо этого можно взять достаточно большой сэмпл и посчитать, какой процент от всех аккаунтов принадлежит каждой стране.\n  Тройка лидеров по количеству аккаунтов это USA, Brazil, Indonesia. Меньше всего аккаунтов в странах Африки, North Korea, и Greenland.   Визуализация 2 Предыдущая визуализация интересна, но очевидно, что чем больше население страны, тем больше в ней будет Instagram accounts. Чтобы убрать зависимость от населенности, можно подсчитать, какой процент от общего количества Instagram аккаунтов приходится на одного жителя страны. Таким образом мы получим популярность Instagram среди населения. Видна совершенно другая картина. Пятерка стран, где жители больше всего любят Instagram: Cyprus, United Arab Emirates, Iceland, Qatar, Malaysia. Эти страны окрашены светло-желтым, поэтому их не очень хорошо заметно на карте. Среди крупных стран Instagram более всего популярен в Brazil, Australia, USA. Менее всего Instagram популярен в Африке и Азии.\nЗаключение Проект передан в production, и успешно использовался в компании Deep.Social, радикально улучшив качество определения места жительства блогеров и их аудиторий, и убрав больше количество претензий клиентов к неточным данным. В настоящий момент проект также используется в компаниях, купивших у Deep.Social права на её продукт.\nВозможные улучшения:  Сейчас учитываемые геотэги просто суммируются, порядок их следования никак не учитывается (bag of geotags). В то же время, в геотэгах часто прослеживается явная темпоральная структура, соответствующая географическим перемещениям владельца аккаунта. Если учитывать эту структуру, то есть работать с геотэгами, как с временным рядом, возможно будет повысить точность. Например модель сможет различать ситуации, когда владелец аккаунта поехал в командировку и когда он переехал на новое место жительства. Информация из фотографий сейчас не используется, т.к. обучение computer vision классификатора занимает много времени, и проект не уложился бы в плановые сроки. Если начать использовать эту информацию, будет дополнительный источник данных, особенно актуальный для аккаунтов, которые не используют геотэги и не указывают дополнительной информации в bio. Вырастет покрытие.   Zhi-Hua Zhou, Ensemble Learning. ^ Scott Lundberg, Su-In Lee (2017). A Unified Approach to Interpreting Model Predictions. arXiv:1705.07874 [cs.AI] ^   ","date":1530043200,"expirydate":-62135596800,"kind":"page","lang":"ru","lastmod":1530043200,"objectID":"f7eb2a82e7c69144fd200dd0bfa2505b","permalink":"https://suilin.ru/project/geo/","publishdate":"2018-06-27T00:00:00+04:00","relpermalink":"/project/geo/","section":"project","summary":"Решение проблемы определения места жительства владельца Instagram аккаунта с помощью моделей машинного обучения. Подготовка данных, архитектура моделей, методика обучения, результаты.","tags":["Instagram","Deep.Social"],"title":"Определение места жительства Instagram пользователей","type":"project"}]